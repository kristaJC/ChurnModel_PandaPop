{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99588b18-d4f4-4e5b-913e-c76f2ac1c2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow xgboost\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8dfb97-a7b6-4ba6-a5b8-575ccdcb8587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "import mlflow.spark\n",
    "from mlflow.artifacts import download_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb74358-93d4-4617-823d-26c86da509c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "from sampling import *\n",
    "from tracking import *\n",
    "from tuning import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15a0f9e-5b1a-4a39-a08a-f39f1c0b8e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn7\"\n",
    "DATE_FILTER = \"2025-10-26\"\n",
    "DATE_INTERVAL = 30\n",
    "\n",
    "# Payer split: None --> no split, \"0\" --> non-payer, \"1,2\" --> payer\n",
    "#payer_split = \"1,2\"\n",
    "\n",
    "\n",
    "#payer_split = None\n",
    "#payer_split = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a801104-8be3-4b60-9959-412e0e9b9cb8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"data_type\":130},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763668760345}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\" describe table {FEATURES_TABLE_NAME}\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f34f45-7b37-4ac4-91df-b0cc3f0822fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5795866552327339,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca11be40-bdf4-451e-97bd-4bba833b1a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078fba85-9530-4f81-9e8f-fc938c12420b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(col('payer_type_cd')).agg(count('payer_type_cd')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766a5ca1-b759-48fe-825b-e11e250513e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, NumericType\n",
    "\n",
    "#.withColumn(\"payer_type_cd\", when((col('payer_type_cd') == 'P')| (col('payer_type_cd')=='S'), 1).otherwise(0))\n",
    "labels = ['churn3','churn5','churn7','churn14']\n",
    "for label in labels:\n",
    "    df = df.withColumn(label, when(col(label)==True,1).otherwise(0))\n",
    "\n",
    "string_features = []\n",
    "numerical_features = []\n",
    "\n",
    "drop_cols = ['judi','date','ts_last_updated','processed_date','churn3','churn5','churn7','churn14']\n",
    "\n",
    "for field in df.schema.fields:\n",
    "    if isinstance(field.dataType, StringType) and field not in drop_cols:\n",
    "        string_features.append(field.name)\n",
    "    elif isinstance(field.dataType, NumericType) and field not in drop_cols:\n",
    "        numerical_features.append(field.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98e2442-a5b5-43fc-97dc-5928ae326f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b124df15-be1d-4010-8de0-9583ec9cbddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payer_split = \"0,1\"\n",
    "\n",
    "if payer_split == None:\n",
    "    numerical_features.remove('payer_type_cd')\n",
    "\n",
    "\n",
    "churn_features = df.withColumn('label', col(LABEL_COL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c37a823-c247-4a53-802a-563e0eb15f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Splitting by payer and non-payer?\n",
    "\n",
    "\n",
    "\n",
    "### Stratified Sampling\n",
    "\n",
    "strat_train, strat_val, strat_test = stratified_sampling(churn_features, P_TEST=0.2, P_VAL=0.2)\n",
    "\n",
    "## Upsampling\n",
    "strat_train_up, train_up_info = upsample_minority(churn_features)\n",
    "\n",
    "## Undersampling\n",
    "strat_train_under, train_under_info = undersample_majority(churn_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd0e948-6ec6-4fd7-8eaa-22344d933c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07b0a6d-4e09-4a90-8689-79a7e211d69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if num_workers > available slots, fitting fails\n",
    "# determine number of workers and repartition the training data\n",
    "strat_train, safe_workers = get_safe_works_repartition(strat_train)\n",
    "strat_train_up, _ = get_safe_works_repartition(strat_train_up)\n",
    "strat_train_under, _ = get_safe_works_repartition(strat_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afec6ca2-ba0b-4e10-9d91-e8c3f947150b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For XGBoost we don't need to standarize any features\n",
    "indexers = [StringIndexer(inputCol=x, \n",
    "                          outputCol=x+\"_index\", \n",
    "                          handleInvalid=\"keep\") for x in string_features]\n",
    "indexed_cols = [ x+\"_index\" for x in string_features]\n",
    "\n",
    "inputs = numerical_features + indexed_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=inputs, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "\n",
    "# Now add the xgb model to the pipeline\n",
    "#eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "eval_metrics = [\"aucpr\"]\n",
    "\n",
    "\n",
    "#safe_workers=1\n",
    "\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col = \"features\",\n",
    "  label_col = \"label\",\n",
    "  num_workers = safe_workers,\n",
    "  eval_metric = eval_metrics,\n",
    ")\n",
    "\n",
    "# Set the pipeline stages for the entire process\n",
    "pipeline = Pipeline().setStages(indexers+[vec_assembler]+ [xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0d1129-6ae9-489b-a2a8-8901b57f00c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f74c3682-cb2d-4e8c-8493-1ccdabc86f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "spec = {\n",
    "    # \"n_estimators\": (\"int_uniform\", 50, 1000),\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "    #\"gamma\": (\"uniform\", 0.0, 0.2),\n",
    "    #\"learning_rate\": (\"uniform\", 0.01,0.5),\n",
    "    # \"subsample\": (\"uniform\", 0.7, 0.9),\n",
    "    #\"colsample_bytree\": (\"uniform\", 0.7, 0.9),\n",
    "    # \"min_child_weight\": (\"int_uniform\", 1, 5),\n",
    "    #\"reg_alpha\": (\"uniform\", 0.0, 0.1),\n",
    "    #\"reg_lambda\": (\"int_uniform\", 1, 10),\n",
    "    #\"colsample_bylevel\": (\"uniform\", 0, 0.6),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03254e5d-f737-4388-b107-27fd946a1356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spec = {\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41021f21-aa61-4370-ac01-7171c9d79264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the MLflow logging level to INFO\n",
    "logger = logging.getLogger(\"mlflow\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd5d79c-6115-47f9-af44-e467d9e7ba07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "extra_tags = {'label': LABEL_COL,\n",
    "              'payer_split':payer_split, \n",
    "              'date_filter':DATE_FILTER, \n",
    "              'date_interval':DATE_INTERVAL,\n",
    "              'sampling':'upsample',\n",
    "              'safe_workers':safe_workers,\n",
    "              #date....?\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ca6138-19ab-4feb-a6d7-a5f92ce78ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "# Modify and test the tracking functions (log all to the mlflow experiment, vs. the notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4ccddc-677e-40cd-98c6-5cc70edace9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train on upsampled data \n",
    "\n",
    "cv_xgb.setEvaluator(BinaryClassificationEvaluator(metricName=\"areaUnderPR\"))\n",
    "cv_xgb.fit(strat_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b729418-56b3-4e2f-9e62-acefb6b71848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Clean XGB- New Table - 11-20-25",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdca6ef-7d36-4766-8233-ce038931c504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow xgboost\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c9c289-852d-4992-ab5b-ce691f8eb6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "import mlflow.spark\n",
    "from mlflow.artifacts import download_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8bdcf9-aa2f-4543-a603-6f171c63ec5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "from sampling import *\n",
    "from tracking import *\n",
    "from tuning import * \n",
    "#from Deprecated.deprecated_tracking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dcf9360-36ee-44ca-8f36-66cce9d4390e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FEATURES_TABLE_NAME = 'teams.data_science.pp_churn_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b12dab-2878-4b54-b886-4904141854a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\" describe table {FEATURES_TABLE_NAME}\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c16dbe7-14ba-47a7-8a0d-307810b64ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe63046-e771-4f7c-961e-4c58f9eb682c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Principal categories \n",
    "## Cluster models\n",
    "### SHAP\n",
    "\n",
    "\n",
    "\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8294bcf4-2aec-43f0-9e8b-57a255534a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# String into a number - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b46195d-5f85-4450-aa6d-e88b8c97965c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Can do this programmatically later \n",
    "\n",
    "drop_cols = ['judi','date','ts_last_updated','processed_date','churn3','churn5','churn7','churn14']\n",
    "\n",
    "all_features = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "string_cols = ['market','attribution_source_cd','country_cd','payer_type_cd']\n",
    "\n",
    "\n",
    "numerical_cols = [item for item in all_features if item not in string_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0478405-3ad8-4fc9-a282-2edfbee3faed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b455a9-de4b-4ebc-90a4-1f84a7758beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Análisis of proportion of churn for different targets\n",
    "\n",
    "display(spark.sql(f\"\"\"select count(case when churn3 = 1 then 1 end) / count(case when churn3 is not null then 1 end) as churn3_rt,\n",
    "       count(case when churn5 = 1 then 1 end) / count(case when churn5 is not null then 1 end) as churn5_rt,\n",
    "       count(case when churn7 = 1 then 1 end) / count(case when churn7 is not null then 1 end) as churn7_rt\n",
    "        from {FEATURES_TABLE_NAME}\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da27a5a4-5ee9-4724-80a4-accc6f0983be",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763065919443}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"cSA9IGYiIiJzZWxlY3QgZGF5b2Z3ZWVrLAogICAgICAgIGNvdW50KGNhc2Ugd2hlbiBjaHVybjMgPSAxIHRoZW4gMSBlbmQpIC8gY291bnQoY2FzZSB3aGVuIGNodXJuMyBpcyBub3QgbnVsbCB0aGVuIDEgZW5kKSBhcyBjaHVybjNfcnQsCiAgICAgICBjb3VudChjYXNlIHdoZW4gY2h1cm41ID0gMSB0aGVuIDEgZW5kKSAvIGNvdW50KGNhc2Ugd2hlbiBjaHVybjUgaXMgbm90IG51bGwgdGhlbiAxIGVuZCkgYXMgY2h1cm41X3J0LAogICAgICAgY291bnQoY2FzZSB3aGVuIGNodXJuNyA9IDEgdGhlbiAxIGVuZCkgLyBjb3VudChjYXNlIHdoZW4gY2h1cm43IGlzIG5vdCBudWxsIHRoZW4gMSBlbmQpIGFzIGNodXJuN19ydAogICAgICAgIGZyb20ge0ZFQVRVUkVTX1RBQkxFX05BTUV9IGdyb3VwIGJ5IGRheW9md2VlayAiIiIKCnNwYXJrLnNxbChxKS5kaXNwbGF5KCk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1763065941603,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         7
        ],
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "8cee6381-6caf-40da-90f9-5dacfa794f21",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.65625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1763065939473,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1763065939383,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = f\"\"\"select dayofweek,\n",
    "        count(case when churn3 = 1 then 1 end) / count(case when churn3 is not null then 1 end) as churn3_rt,\n",
    "       count(case when churn5 = 1 then 1 end) / count(case when churn5 is not null then 1 end) as churn5_rt,\n",
    "       count(case when churn7 = 1 then 1 end) / count(case when churn7 is not null then 1 end) as churn7_rt\n",
    "        from {FEATURES_TABLE_NAME} group by dayofweek \"\"\"\n",
    "\n",
    "spark.sql(q).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b0bc15-5deb-47ec-9f28-1b4c0a2c1e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### TODO:\n",
    "\n",
    "### Explore some top models trained - see if reducing num. vars improves performance\n",
    "### payer/non-payer churn day 7 - trained during Nov 6 ish\n",
    "### payer/non-payer churn day 7 -  also extend day back to n days - include month; percentage of month; \n",
    "\n",
    "### Later add to dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff5fc1e-7004-490c-95b9-d3d7f8e52572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "\n",
    "\n",
    "### Explore feature importances to see if dayofweek is important... or day of month?  \n",
    "### See if there are event drops or something on certain days of month - from game team (1/31)--> percentage; 1/28 --> percentage; to track progress of elapsed time in the month\n",
    "\n",
    "\n",
    "### Monitor auc-PR also daily feature importances?\n",
    "\n",
    "### look at feature importances - to see if day of week is significant \n",
    "### track the feature importance for different targets and re-training \n",
    "### log all the figures/plots/metrics/etc. \n",
    "\n",
    "### scoring metric: auc-PR\n",
    "\n",
    "\n",
    "### some threshold for retraining - if the change is significant - retrain\n",
    "\n",
    "### need some info from game team about any changes in features/holidays/seasonal whatever/events etc. \n",
    "\n",
    "\n",
    "### Defining schema for features table, cluster results, predictions/outputs - semi done \n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3398c93-722b-4cd9-89ee-a7f3f5709eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Análisis of proportion of churn for different targets\n",
    "\n",
    "display(spark.sql(f\"\"\"select count(case when churn3 = 1 then 1 end) / count(case when churn3 is not null then 1 end) as churn3_rt,\n",
    "       count(case when churn5 = 1 then 1 end) / count(case when churn5 is not null then 1 end) as churn5_rt,\n",
    "       count(case when churn7 = 1 then 1 end) / count(case when churn7 is not null then 1 end) as churn7_rt\n",
    "        from {FEATURES_TABLE_NAME}\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22db0c6-79bf-431e-9036-75491b97620f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn7\"\n",
    "#FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features_v3_small\"\n",
    "\n",
    "\n",
    "DATE_FILTER = \"2025-10-26\"\n",
    "DATE_INTERVAL = 30\n",
    "\n",
    "# Payer split: None --> no split, \"0\" --> non-payer, \"1,2\" --> payer\n",
    "payer_split = \"1,2\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cluster vars: None --> no extra cluster vars\n",
    "\n",
    "\n",
    "# These are loaded in config already\n",
    "#EXPERIMENT_NAME = \"/Users/krista@jamcity.com/PP-Churn-Model\"\n",
    "#FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d3811b-9360-450c-bdd0-30917755b7fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#string_features = []\n",
    "\"\"\" other_features = ['unique_levels_played', 'market_idx','dayofweek','rounds_played', 'avg_attempts', 'total_attempts', 'avg_moves', 'win_rate', 'assist_success_rate', 'unassist_success_rate', 'assist_rate', 'total_boosters_used', 'total_boosters_spent', 'used_boosters_rate', 'spend_boosters_rate', 'avg_difficulty_score', 'rate_hard_levels', 'rate_superhard_levels', 'min_room_id_int', 'max_room_id_int', 'daily_win_rate_ref', 'daily_avg_boosters_used_ref', 'daily_avg_boosters_spent_ref', 'attribution_source_cd_idx', 'country_cd_idx', 'payer_type_cd_idx', 'iap_lifetime_amt', 'days_since_install', 'days_since_last_purchase', 'ad_revenue_amt', 'iap_revenue_amt', 'session_qty', 'total_session_length_qty', 'avg_session_length', 'sessions_per_round', 'avg_population_wr_on_levels_played_today', 'avg_population_assisted_rate_today', 'avg_population_attempts_today', 'wr_diff_vs_population', 'attempts_diff_vs_population', 'assist_rate_diff_vs_population', 'active_days_l7d', 'total_rounds_l7d', 'avg_rounds_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d', 'boosters_used_l7d', 'avg_used_boosters_rate_l7d', 'active_days_l14d', 'avg_rounds_l14d', 'avg_win_rate_l14d', 'std_rounds_l14d', 'std_win_rate_l14d', 'active_days_l30d', 'avg_rounds_l30d', 'rounds_trend_weekly', 'win_rate_trend_weekly', 'boosters_usage_trend_weekly', 'rounds_ratio_7d_vs_14_7d', 'frequency_ratio_7d_vs_14d', 'levels_progressed_l7d', 'levels_progressed_l14d', 'levels_progressed_l30d', 'days_on_current_max_level', 'level_diversity_ratio',]\n",
    "\n",
    "if cluster_vars is not None:\n",
    "    other_features = other_features + cluster_vars\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91381bbe-21b5-408c-b38f-7ada1328326c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FEATURES_TABLE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0403195-7554-4b70-9244-d1b040b951ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payer_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc5dab7-81e8-4861-af5c-71f052197a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get data from table\n",
    "\n",
    "# If there is payer split\n",
    "if payer_split is None:\n",
    "\n",
    "    churn_features = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\")\\\n",
    "        .withColumn(\"label\",col(LABEL_COL))\n",
    "\n",
    "else:\n",
    "\n",
    "    churn_features = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                --and payer_type_cd_idx in ({payer_split})\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\")\\\n",
    "        .withColumn(\"label\",col(LABEL_COL))\n",
    "    if payer_split == \"0\":\n",
    "        other_features.remove(\"payer_type_cd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcc7084-35db-4210-9297-a909cafb9afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, NumericType\n",
    "\n",
    "#df = spark.sql(f\"\"\" select * from {FEATURES_TABLE_NAME}\"\"\")\n",
    "\n",
    "\n",
    "string_features = []\n",
    "numerical_features = []\n",
    "\n",
    "drop_cols = ['judi','date','ts_last_updated','processed_date','churn3','churn5','churn7','churn14']\n",
    "\n",
    "#df = df.withColumn('label', col(LABEL_COL))\n",
    "\n",
    "churn_features = churn_features.withColumn(\"payer_type_cd\", when(col('payer_type_cd') == 'P', 1).otherwise(0))\n",
    "\n",
    "\n",
    "for field in churn_features.schema.fields:\n",
    "    if isinstance(field.dataType, StringType) and field not in drop_cols:\n",
    "        string_features.append(field.name)\n",
    "    elif isinstance(field.dataType, NumericType) and field not in drop_cols:\n",
    "        numerical_features.append(field.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac80c96d-10dc-4a3a-8fb8-1acd295c7e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_features =  spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                --and payer_type_cd_idx in ({payer_split})\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\")\n",
    "\n",
    "if payer_split == '0':\n",
    "    numerical_features.remove(\"payer_type_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecf9857-8566-4a80-8662-b99ced78bdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get stratified train, validation, test set\n",
    "strat_train, strat_val, strat_test = stratified_sampling(df, P_TEST=0.2, P_VAL=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34d7df4-7caa-4e87-a146-09fae4078088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Undersample majority class\n",
    "strat_train_under, train_under_info = undersample_majority(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca51b17d-7a0b-4726-8188-c756d7289a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Upsample minority class\n",
    "strat_train_up, train_up_info = upsample_minority(churn_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea40cf1-2728-4955-a288-7b119bdf8832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build Pipeline for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1efbe14-c00a-4084-9edd-0afe94c20337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc4c5fa-394d-4046-a7af-308a36b9c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: would love to have a function that automatically sorts the columns by type\n",
    "#drop_for_features = {\"judi\",\"date\",\"churn3\"} \n",
    "#feature_cols = [c for c in df.columns if c not in drop_for_features and c not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14131b91-a056-49b5-bee1-22e2fabd4525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92016ccf-d515-4c32-a1d0-11f33a24e36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if num_workers > available slots, fitting fails\n",
    "# determine number of workers and repartition the training data\n",
    "strat_train, safe_workers = get_safe_works_repartition(strat_train)\n",
    "strat_train_up, _ = get_safe_works_repartition(strat_train_up)\n",
    "strat_train_under, _ = get_safe_works_repartition(strat_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a33407f-a44c-4c0e-a114-d5c60ac7a992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(safe_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7d8a2b-fac3-4a72-b816-1344b5333a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c262725c-a991-4e20-9f05-a0fd10727267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For XGBoost we don't need to standarize any features\n",
    "indexers = [StringIndexer(inputCol=x, \n",
    "                          outputCol=x+\"_index\", \n",
    "                          handleInvalid=\"keep\") for x in string_features]\n",
    "indexed_cols = [ x+\"_index\" for x in string_features]\n",
    "\n",
    "inputs = other_features + indexed_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=inputs, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "\n",
    "# Now add the xgb model to the pipeline\n",
    "#eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "eval_metrics = [\"aucpr\"]\n",
    "\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col = \"features\",\n",
    "  label_col = \"label\",\n",
    "  num_workers = safe_workers,\n",
    "  eval_metric = eval_metrics,\n",
    ")\n",
    "\n",
    "# Set the pipeline stages for the entire process\n",
    "pipeline = Pipeline().setStages(indexers+[vec_assembler]+ [xgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d294e78b-58d7-4df7-96b1-1e42a3e5a0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can fit your pipeline model here with MLFlow tracking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ad8669-0bc4-4438-9aaa-578fb47a8600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Param specs for random grid builder\n",
    "spec = {\n",
    "    # \"n_estimators\": (\"int_uniform\", 50, 1000),\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "    #\"gamma\": (\"uniform\", 0.0, 0.2),\n",
    "    #\"learning_rate\": (\"uniform\", 0.01,0.5),\n",
    "    # \"subsample\": (\"uniform\", 0.7, 0.9),\n",
    "    #\"colsample_bytree\": (\"uniform\", 0.7, 0.9),\n",
    "    # \"min_child_weight\": (\"int_uniform\", 1, 5),\n",
    "    #\"reg_alpha\": (\"uniform\", 0.0, 0.1),\n",
    "    #\"reg_lambda\": (\"int_uniform\", 1, 10),\n",
    "    #\"colsample_bylevel\": (\"uniform\", 0, 0.6),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ac9be2-0637-42fd-82d1-5dc6fa8723a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the MLflow logging level to INFO\n",
    "logger = logging.getLogger(\"mlflow\")\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51141cb1-661c-4225-b696-884d0a843786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_xgb.setEvaluator(BinaryClassificationEvaluator(metricName=\"areaUnderPR\"))\n",
    "cv_xgb.fit(strat_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7d0fa5-2bbb-4ab6-867f-512d703be5c5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"params\":216},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762448456270}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "experiment_lst = [{\"run_id\":\"a679118511bd46c3b13b76bb22e24972\",\"name\":\"XGB_30_churn7\"},\n",
    "                  {\"run_id\":\"d68b7ac6064c4ce7989a5716fe4b2322\",\"name\":\"XGB_30_churn3\"},\n",
    "                  {\"run_id\":\"3fdd4de3b3bb42b9a765a50d1e2254e3\",\"name\":\"XGB_30_churn5\"},\n",
    "                  {\"run_id\":\"0ae51ea70f3b48cab60c0dc9ea8049fc\",\"name\":\"XGB_30_churn7_non_payer\"},\n",
    "                  {\"run_id\":\"954ff7ad42d04d0eaa29eee31d0de69b\",\"name\":\"XGB_30_churn7_payer\"},\n",
    "                  {\"run_id\":\"a7c6fab2d77846a8918e49368f9c7e1c\",\"name\":\"XGB_30_churn7_non_payer_cluster\"},\n",
    "                  {\"run_id\":\"d92501999472499d85e1e794716fb5b5\",\"name\":\"XGB_30_churn7_payer_cluster\"}]\n",
    "\n",
    "df_lst = []\n",
    "\n",
    "for experiment in experiment_lst:\n",
    "\n",
    "    run_id = experiment[\"run_id\"]\n",
    "\n",
    "    artifact_path = download_artifacts(artifact_uri=f\"runs:/{run_id}/search_results.csv\")\n",
    "    df_tmp = pd.read_csv(artifact_path)\n",
    "    df_tmp[\"model\"] = experiment[\"name\"]\n",
    "    df_lst.append(df_tmp)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.concat(df_lst, axis=0)\n",
    "\n",
    "display(df.sort_values([\"params\",\"model\"]))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8406786470410038,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "KP- XGB Training - Different Targets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

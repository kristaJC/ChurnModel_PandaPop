{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b851a7f-d066-4be5-8b93-b88773436372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"teams.data_science.pp_churn_features_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011c58b8-a71e-43a9-ad73-34243f996e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "cols_to_drop = ['judi', 'date', 'churn7', 'churn14', 'subs_lifetime_amt', \n",
    "                'subs_revenue_amt', 'daily_avg_boosters_used_ref']\n",
    "df_clean = (\n",
    "    df.drop(*cols_to_drop)\n",
    "      .withColumn(\"churn3\", F.col(\"churn3\").cast(\"int\"))\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Registros totales: {df_clean.count():,}\")\n",
    "print(f\"Columnas: {len(df_clean.columns)}\")\n",
    "print(f\"\\nDistribución de la clase target:\")\n",
    "df_clean.groupBy(\"churn3\").count().show()\n",
    "\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef22f17f-6a6b-4d08-80cb-6a06220d7aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Muestreo estratificado manteniendo proporción de churn3\n",
    "sample_size = 10000\n",
    "total_count = df_clean.count()\n",
    "sample_fraction = min(1.0, sample_size / total_count)\n",
    "\n",
    "df_sample = df_clean.sampleBy(\"churn3\", fractions={0: sample_fraction, 1: sample_fraction}, seed=42)\n",
    "\n",
    "# Verifica las proporciones\n",
    "df_sample.groupBy(\"churn3\").count().show()\n",
    "\n",
    "df_clean = df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b78991-b1af-48ae-9a5f-73a6c955770b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# Split estratificado 80/10/10 (train/val/test) \n",
    "# Usamos una columna aleatoria para el split\n",
    "df_split = df_clean.withColumn(\"rand\", F.rand(seed=42))\n",
    "\n",
    "# Estratificación manual por clase\n",
    "train_df = df_split.filter(\n",
    "    ((F.col(\"churn3\") == 0) & (F.col(\"rand\") <= 0.8)) |\n",
    "    ((F.col(\"churn3\") == 1) & (F.col(\"rand\") <= 0.8))\n",
    ").drop(\"rand\")\n",
    "\n",
    "val_df = df_split.filter(\n",
    "    ((F.col(\"churn3\") == 0) & (F.col(\"rand\") > 0.8) & (F.col(\"rand\") <= 0.9)) |\n",
    "    ((F.col(\"churn3\") == 1) & (F.col(\"rand\") > 0.8) & (F.col(\"rand\") <= 0.9))\n",
    ").drop(\"rand\")\n",
    "\n",
    "test_df = df_split.filter(\n",
    "    ((F.col(\"churn3\") == 0) & (F.col(\"rand\") > 0.9)) |\n",
    "    ((F.col(\"churn3\") == 1) & (F.col(\"rand\") > 0.9))\n",
    ").drop(\"rand\")\n",
    "\n",
    "# Verificar distribución \n",
    "print(\"Distribución aproximada por split:\")\n",
    "print(\"Train:\")\n",
    "train_df.groupBy(\"churn3\").count().show()\n",
    "print(\"Validation:\")\n",
    "val_df.groupBy(\"churn3\").count().show()\n",
    "print(\"Test:\")\n",
    "test_df.groupBy(\"churn3\").count().show()\n",
    "\n",
    "# Guardar en Delta \n",
    "train_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_train_churn3\")\n",
    "val_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_val_churn3\")\n",
    "test_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_test_churn3\")\n",
    "\n",
    "print(\"\\nSplits guardados en Delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a555e9-f8d4-4d27-8c78-bed1c7235378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Leer train desde Delta\n",
    "train_df = spark.table(\"teams.data_science.gp_pp_train_churn3\")\n",
    "\n",
    "# Separar features de target\n",
    "feature_cols = [col for col in train_df.columns if col != 'churn3']\n",
    "print(f\"Features a normalizar: {len(feature_cols)}\")\n",
    "\n",
    "# Pipeline de normalización\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\", \n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit SOLO en train (calcula mean/std)\n",
    "scaler_model = pipeline.fit(train_df)\n",
    "\n",
    "# Transformar todos los splits\n",
    "train_scaled = scaler_model.transform(train_df).select(\"features_scaled\", \"churn3\")\n",
    "val_scaled = scaler_model.transform(spark.table(\"teams.data_science.gp_pp_val_churn3\")).select(\"features_scaled\", \"churn3\")\n",
    "test_scaled = scaler_model.transform(spark.table(\"teams.data_science.gp_pp_test_churn3\")).select(\"features_scaled\", \"churn3\")\n",
    "\n",
    "# Guardar datos normalizados\n",
    "train_scaled.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_train_churn3_scaled\")\n",
    "val_scaled.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_val_churn3_scaled\")\n",
    "test_scaled.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_test_churn3_scaled\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3bd7e8-bfc8-4c51-9785-dc74948a1883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# limpio las versiones viejas de las tablas\n",
    "spark.sql(\"set spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "\n",
    "vacuum_lst = [\"teams.data_science.gp_pp_train_churn3_scaled\",\n",
    "\"teams.data_science.gp_pp_val_churn3_scaled\",\n",
    "\"teams.data_science.gp_pp_test_churn3_scaled\",\n",
    "\"teams.data_science.gp_pp_train_churn3\",\n",
    "\"teams.data_science.gp_pp_val_churn3\",\n",
    "\"teams.data_science.gp_pp_test_churn3\"]\n",
    "\n",
    "for table in vacuum_lst:\n",
    "    spark.sql(f\"VACUUM {table} RETAIN 0 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7aa41d-2348-4ceb-a602-b82480067de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardo el scaler\n",
    "scaler_model.write().overwrite().save(\"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_scaler_model\")\n",
    "\n",
    "print(\" Normalización completada y guardada\")\n",
    "print(\"\\nEjemplo de dato transformado:\")\n",
    "train_scaled.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780c893a-6b2a-4745-b26c-41297e4ca941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql.functions import col, udf\n",
    "import math\n",
    "\n",
    "vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "def write_tfrecords_dynamic(table_name, output_path, target_rows_per_file=500_000, min_files=4, max_files=200):\n",
    "    \"\"\"\n",
    "    Convierte tabla Delta a Parquet con número de archivos dinámico según tamaño del dataset.\n",
    "    \"\"\"\n",
    "    df = spark.table(table_name)\n",
    "    \n",
    "    # Contar filas\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    # Calcular número de archivos ideal\n",
    "    num_files = max(min_files, min(max_files, math.ceil(total_rows / target_rows_per_file)))\n",
    "    \n",
    "    # Convertir Vector a Array y seleccionar columnas relevantes\n",
    "    df = df.withColumn(\"features\", vector_to_array(col(\"features_scaled\"))) \\\n",
    "           .select(\"features\", \"churn3\")\n",
    "    \n",
    "    # Repartir según tamaño estimado\n",
    "    df = df.repartition(num_files)\n",
    "    \n",
    "    # Guardar como Parquet\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    print(f\"Guardado: {output_path}\")\n",
    "    print(f\"Total filas: {total_rows:,}\")\n",
    "    print(f\"Archivos generados: {num_files}\")\n",
    "    return output_path\n",
    "\n",
    "train_path = write_tfrecords_dynamic(\n",
    "    \"teams.data_science.gp_pp_train_churn3_scaled\",\n",
    "    \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/train\"\n",
    ")\n",
    "val_path = write_tfrecords_dynamic(\n",
    "    \"teams.data_science.gp_pp_val_churn3_scaled\",\n",
    "    \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/val\"\n",
    ")\n",
    "test_path = write_tfrecords_dynamic(\n",
    "    \"teams.data_science.gp_pp_test_churn3_scaled\",\n",
    "    \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/test\"\n",
    ")\n",
    "\n",
    "print(\"\\nTodos los datasets convertidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dab8446-8a85-45d5-9980-3718d438d5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.fs import LocalFileSystem\n",
    "import os\n",
    "\n",
    "\n",
    "def create_tf_dataset_from_parquet(parquet_path, batch_size=2048, shuffle=True):\n",
    "    \"\"\"\n",
    "    Lee Parquet con TensorFlow de forma eficiente\n",
    "    \"\"\"\n",
    "    parquet_path_dbfs = parquet_path.replace('/dbfs', 'dbfs:')\n",
    "    files = dbutils.fs.ls(parquet_path_dbfs)\n",
    "    parquet_files = [f.path for f in files if f.name.endswith('.parquet')]\n",
    "\n",
    "    print(f\"Encontrados {len(parquet_files)} archivos parquet\")\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Lee Parquet files en batches\"\"\"\n",
    "        for file_path in parquet_files:\n",
    "            local_path = file_path.replace('dbfs:', '/dbfs')\n",
    "            table = pq.read_table(local_path)\n",
    "            features = np.array([np.array(x, dtype=np.float32) for x in table['features'].to_pylist()])\n",
    "            labels = table['churn3'].to_numpy().astype(np.int32)\n",
    "            for i in range(len(features)):\n",
    "                yield features[i], labels[i]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(65,), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Crear datasets \n",
    "# --------------------------------------------------------\n",
    "print(\"Creando TF Datasets desde Parquet...\")\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "train_path = \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/train\"\n",
    "val_path = \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/val\"\n",
    "\n",
    "# Contar filas \n",
    "train_size = spark.table(\"teams.data_science.gp_pp_train_churn3_scaled\").count()\n",
    "val_size = spark.table(\"teams.data_science.gp_pp_val_churn3_scaled\").count()\n",
    "\n",
    "steps_per_epoch = max(1, train_size // BATCH_SIZE)\n",
    "validation_steps = max(1, val_size // BATCH_SIZE)\n",
    "\n",
    "print(f\"\\nFilas train: {train_size:,}\")\n",
    "print(f\"Filas val: {val_size:,}\")\n",
    "print(f\"Steps por epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = create_tf_dataset_from_parquet(train_path, BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_tf_dataset_from_parquet(val_path, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# TEST crítico\n",
    "print(\"\\n Probando lectura de 1 batch...\")\n",
    "for x_batch, y_batch in train_dataset.take(1):\n",
    "    print(f\"Features shape: {x_batch.shape}\")\n",
    "    print(f\"Labels shape: {y_batch.shape}\")\n",
    "    print(f\"Distribución labels: {np.bincount(y_batch.numpy())}\")\n",
    "    print(f\"Rango features: [{x_batch.numpy().min():.2f}, {x_batch.numpy().max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e33fb1-9aa2-4989-8c4c-1b765df6960d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Calcular class weights dinámicamente desde train\n",
    "train_class_counts = spark.table(\"teams.data_science.gp_pp_train_churn3_scaled\") \\\n",
    "    .groupBy(\"churn3\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"churn3\") \\\n",
    "    .collect()\n",
    "\n",
    "# Extraer counts\n",
    "count_class_0 = train_class_counts[0]['count']\n",
    "count_class_1 = train_class_counts[1]['count']\n",
    "total = count_class_0 + count_class_1\n",
    "\n",
    "# Método 1: Inverse frequency \n",
    "class_weight = {\n",
    "    0: 1.0,\n",
    "    1: count_class_0 / count_class_1\n",
    "}\n",
    "\n",
    "print(f\"Distribución train:\")\n",
    "print(f\"  Clase 0: {count_class_0:,} ({count_class_0/total*100:.2f}%)\")\n",
    "print(f\"  Clase 1: {count_class_1:,} ({count_class_1/total*100:.2f}%)\")\n",
    "print(f\"\\nClass weights: {class_weight}\")\n",
    "print(f\"  Ratio: 1:{class_weight[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7529ee-8c8d-4ffa-b764-3fa4840957c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "\n",
    "def create_model(input_dim=65, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    NN simple con regularizacion para evitar overfitting en desbalance\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Layer 1\n",
    "        layers.Dense(128, activation='relu', \n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Layer 2\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Layer 3\n",
    "        layers.Dense(32, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Output (sigmoid para binaria)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# Compilar con metricas apropiadas para desbalance\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.AUC(name='pr_auc', curve='PR')  # Precision-Recall AUC\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nModelo creado y compilado\")\n",
    "print(f\"Total parámetros: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2257531-a89d-4ed5-883c-e8e80c4ba85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Directorio para guardar modelo\n",
    "model_path = \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_models\"\n",
    "dbutils.fs.mkdirs(f\"dbfs:{model_path}\")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Early stopping en validation AUC (mejor métrica para desbalance)\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=5,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Guardar mejor modelo\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{model_path}/best_model.keras\",\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reducir learning rate si no mejora\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_auc',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        mode='max',\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard (opcional)\n",
    "    TensorBoard(\n",
    "        log_dir=f\"{model_path}/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        histogram_freq=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configurados\")\n",
    "\n",
    "# Entrenar\n",
    "print(\"\\nIniciando entrenamiento...\")\n",
    "print(f\"Epochs: 20 (con early stopping)\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a2530c-a5b0-4794-a83d-434a57125b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "# Cargar mejor modelo\n",
    "best_model = keras.models.load_model(f\"{model_path}/best_model.keras\")\n",
    "\n",
    "# Leer test set\n",
    "test_dataset = create_tf_dataset_from_parquet(\"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/test\", BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af631cc-26d2-4bce-adcc-0308e887086c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_size = spark.table(\"teams.data_science.gp_pp_test_churn3_scaled\").count()\n",
    "test_steps = test_size // BATCH_SIZE\n",
    "\n",
    "print(\"Evaluando en test set...\")\n",
    "test_results = best_model.evaluate(test_dataset, steps=test_steps, verbose=1)\n",
    "\n",
    "print(\"\\n Métricas en Test:\")\n",
    "for name, value in zip(best_model.metrics_names, test_results):\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# Obtener predicciones (probabilidades)\n",
    "print(\"\\n Generando predicciones...\")\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    preds = best_model.predict(x_batch, verbose=0)\n",
    "    y_pred_proba.extend(preds.flatten())\n",
    "    y_true.extend(y_batch.numpy())\n",
    "    \n",
    "    if len(y_true) >= test_size:\n",
    "        break\n",
    "\n",
    "y_pred_proba = np.array(y_pred_proba[:test_size])\n",
    "y_true = np.array(y_true[:test_size])\n",
    "\n",
    "print(f\"Predicciones obtenidas: {len(y_pred_proba):,}\")\n",
    "\n",
    "# Evaluar con threshold default (0.5)\n",
    "y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n Resultados con threshold=0.5:\")\n",
    "print(classification_report(y_true, y_pred_default, target_names=['No Churn', 'Churn']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e08b07-58f6-419a-b121-afee703f9f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view vo_test as\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  delta.transaction\n",
    "WHERE\n",
    "  date = '2025-10-21'\n",
    "  AND appId = 2151\n",
    "  AND storeTransactionId like 'c092f703-2893-3be2-ac61-4ac22f460fd8';\n",
    "\n",
    "create or replace temp view vo_real as\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  delta.transaction\n",
    "WHERE\n",
    "  date = '2025-10-22'\n",
    "  AND appId = 2151 \n",
    "  and isocurrencycode='USD'\n",
    "  limit 1\n",
    "  ;\n",
    "\n",
    "select * from vo_test\n",
    "union \n",
    "select * from vo_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9b28a9-7730-4874-8cbf-dbec91b79fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  delta.transaction\n",
    "WHERE\n",
    "  date = '2025-10-21' \n",
    "  AND appId = 2151 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d01a47-07ff-4653-bf41-fe15a4c11de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Métricas en Test:\n",
    "# loss: 0.6047\n",
    "# compile_metrics: 0.8058\n",
    "\n",
    "#  Generando predicciones...\n",
    "#  Predicciones obtenidas: 10,033\n",
    "\n",
    "#  Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.81      0.89      9701\n",
    "#        Churn       0.13      0.82      0.22       332\n",
    "\n",
    "#     accuracy                           0.81     10033\n",
    "#    macro avg       0.56      0.81      0.55     10033\n",
    "# weighted avg       0.96      0.81      0.87     10033\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "# [[7828 1873]\n",
    "#  [  60  272]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c24cdb7-3e77-4414-badc-edf87b95d12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10k\n",
    "# Predicciones obtenidas: 1,057\n",
    "\n",
    "#  Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.84      0.91      1013\n",
    "#        Churn       0.18      0.84      0.30        44\n",
    "\n",
    "#     accuracy                           0.84      1057\n",
    "#    macro avg       0.59      0.84      0.60      1057\n",
    "# weighted avg       0.96      0.84      0.88      1057\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "# [[846 167]\n",
    "#  [  7  37]]\n",
    "\n",
    "\n",
    "\n",
    "# 100k\n",
    "#  Métricas en Test:\n",
    "# loss: 0.5824\n",
    "# compile_metrics: 0.8002\n",
    "\n",
    "#  Generando predicciones...\n",
    "#  Predicciones obtenidas: 10,073\n",
    "\n",
    "#  Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.80      0.88      9730\n",
    "#        Churn       0.12      0.81      0.22       343\n",
    "\n",
    "#     accuracy                           0.80     10073\n",
    "#    macro avg       0.56      0.81      0.55     10073\n",
    "# weighted avg       0.96      0.80      0.86     10073\n",
    "\n",
    "\n",
    "# onfusion Matrix:\n",
    "# [[7764 1966]\n",
    "#  [  64  279]]\n",
    "\n",
    "# 100k -- red mas simple\n",
    "# Métricas en Test:\n",
    "# loss: 0.5279\n",
    "# compile_metrics: 0.8391\n",
    "\n",
    "#  Generando predicciones...\n",
    "#  Predicciones obtenidas: 10,073\n",
    "\n",
    "#  Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.84      0.91      9730\n",
    "#        Churn       0.14      0.77      0.24       343\n",
    "\n",
    "#     accuracy                           0.84     10073\n",
    "#    macro avg       0.57      0.80      0.58     10073\n",
    "# weighted avg       0.96      0.84      0.89     10073\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "# [[8173 1557]\n",
    "#  [  80  263]]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6833368668327262,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nn-test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b851a7f-d066-4be5-8b93-b88773436372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"teams.data_science.pp_churn_features_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011c58b8-a71e-43a9-ad73-34243f996e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Tu preprocessing (ya lo tienes)\n",
    "cols_to_drop = ['judi', 'date', 'churn7', 'churn14', 'subs_lifetime_amt', \n",
    "                'subs_revenue_amt', 'daily_avg_boosters_used_ref']\n",
    "df_clean = (\n",
    "    df.drop(*cols_to_drop)\n",
    "      .withColumn(\"churn3\", F.col(\"churn3\").cast(\"int\"))\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# Validaci√≥n b√°sica\n",
    "print(f\"Registros totales: {df_clean.count():,}\")\n",
    "print(f\"Columnas: {len(df_clean.columns)}\")\n",
    "print(f\"\\nDistribuci√≥n de la clase target:\")\n",
    "df_clean.groupBy(\"churn3\").count().show()\n",
    "\n",
    "# Verificar tipos de datos\n",
    "print(\"\\nTipos de datos:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef22f17f-6a6b-4d08-80cb-6a06220d7aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Muestreo estratificado manteniendo proporci√≥n de churn3\n",
    "sample_size = 1000000\n",
    "total_count = df_clean.count()\n",
    "sample_fraction = min(1.0, sample_size / total_count)\n",
    "\n",
    "df_sample = df_clean.sampleBy(\"churn3\", fractions={0: sample_fraction, 1: sample_fraction}, seed=42)\n",
    "\n",
    "# Verifica las proporciones\n",
    "df_sample.groupBy(\"churn3\").count().show()\n",
    "\n",
    "df_clean = df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b78991-b1af-48ae-9a5f-73a6c955770b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# Split estratificado 80/10/10 (train/val/test) sin romper el driver\n",
    "# Usamos una columna aleatoria para el split\n",
    "df_split = df_clean.withColumn(\"rand\", F.rand(seed=42))\n",
    "\n",
    "# Estratificaci√≥n manual por clase\n",
    "train_df = df_split.filter(\n",
    "    ((F.col(\"churn3\") == 0) & (F.col(\"rand\") <= 0.8)) |\n",
    "    ((F.col(\"churn3\") == 1) & (F.col(\"rand\") <= 0.8))\n",
    ").drop(\"rand\")\n",
    "\n",
    "val_df = df_split.filter(\n",
    "    ((F.col(\"churn3\") == 0) & (F.col(\"rand\") > 0.8) & (F.col(\"rand\") <= 0.9)) |\n",
    "    ((F.col(\"churn3\") == 1) & (F.col(\"rand\") > 0.8) & (F.col(\"rand\") <= 0.9))\n",
    ").drop(\"rand\")\n",
    "\n",
    "test_df = df_split.filter(\n",
    "    ((F.col(\"churn3\") == 0) & (F.col(\"rand\") > 0.9)) |\n",
    "    ((F.col(\"churn3\") == 1) & (F.col(\"rand\") > 0.9))\n",
    ").drop(\"rand\")\n",
    "\n",
    "# Verificar distribuci√≥n (sin .count() para no materializar todo)\n",
    "print(\"Distribuci√≥n aproximada por split:\")\n",
    "print(\"Train:\")\n",
    "train_df.groupBy(\"churn3\").count().show()\n",
    "print(\"Validation:\")\n",
    "val_df.groupBy(\"churn3\").count().show()\n",
    "print(\"Test:\")\n",
    "test_df.groupBy(\"churn3\").count().show()\n",
    "\n",
    "# Guardar en Delta (CR√çTICO: esto evita recomputar y permite cargar en chunks)\n",
    "train_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_train_churn3\")\n",
    "val_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_val_churn3\")\n",
    "test_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_test_churn3\")\n",
    "\n",
    "print(\"\\n‚úÖ Splits guardados en Delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a555e9-f8d4-4d27-8c78-bed1c7235378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Leer train desde Delta\n",
    "train_df = spark.table(\"teams.data_science.gp_pp_train_churn3\")\n",
    "\n",
    "# Separar features de target\n",
    "feature_cols = [col for col in train_df.columns if col != 'churn3']\n",
    "print(f\"Features a normalizar: {len(feature_cols)}\")\n",
    "\n",
    "# Pipeline de normalizaci√≥n\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\", \n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit SOLO en train (calcula mean/std)\n",
    "scaler_model = pipeline.fit(train_df)\n",
    "\n",
    "# Transformar todos los splits\n",
    "train_scaled = scaler_model.transform(train_df).select(\"features_scaled\", \"churn3\")\n",
    "val_scaled = scaler_model.transform(spark.table(\"teams.data_science.gp_pp_val_churn3\")).select(\"features_scaled\", \"churn3\")\n",
    "test_scaled = scaler_model.transform(spark.table(\"teams.data_science.gp_pp_test_churn3\")).select(\"features_scaled\", \"churn3\")\n",
    "\n",
    "# Guardar datos normalizados\n",
    "train_scaled.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_train_churn3_scaled\")\n",
    "val_scaled.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_val_churn3_scaled\")\n",
    "test_scaled.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams.data_science.gp_pp_test_churn3_scaled\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3bd7e8-bfc8-4c51-9785-dc74948a1883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "\n",
    "vacuum_lst = [\"teams.data_science.gp_pp_train_churn3_scaled\",\n",
    "\"teams.data_science.gp_pp_val_churn3_scaled\",\n",
    "\"teams.data_science.gp_pp_test_churn3_scaled\",\n",
    "\"teams.data_science.gp_pp_train_churn3\",\n",
    "\"teams.data_science.gp_pp_val_churn3\",\n",
    "\"teams.data_science.gp_pp_test_churn3\"]\n",
    "\n",
    "for table in vacuum_lst:\n",
    "    spark.sql(f\"VACUUM {table} RETAIN 0 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7aa41d-2348-4ceb-a602-b82480067de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar el scaler para producci√≥n\n",
    "scaler_model.write().overwrite().save(\"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_scaler_model\")\n",
    "\n",
    "print(\"‚úÖ Normalizaci√≥n completada y guardada\")\n",
    "print(\"\\nEjemplo de dato transformado:\")\n",
    "train_scaled.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780c893a-6b2a-4745-b26c-41297e4ca941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql.functions import col, udf\n",
    "import math\n",
    "\n",
    "vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "def write_tfrecords_dynamic(table_name, output_path, target_rows_per_file=500_000, min_files=4, max_files=200):\n",
    "    \"\"\"\n",
    "    Convierte tabla Delta a Parquet con n√∫mero de archivos din√°mico seg√∫n tama√±o del dataset.\n",
    "    \"\"\"\n",
    "    df = spark.table(table_name)\n",
    "    \n",
    "    # Contar filas\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    # Calcular n√∫mero de archivos ideal\n",
    "    num_files = max(min_files, min(max_files, math.ceil(total_rows / target_rows_per_file)))\n",
    "    \n",
    "    # Convertir Vector a Array y seleccionar columnas relevantes\n",
    "    df = df.withColumn(\"features\", vector_to_array(col(\"features_scaled\"))) \\\n",
    "           .select(\"features\", \"churn3\")\n",
    "    \n",
    "    # Repartir seg√∫n tama√±o estimado\n",
    "    df = df.repartition(num_files)\n",
    "    \n",
    "    # Guardar como Parquet\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Guardado: {output_path}\")\n",
    "    print(f\"‚û°Ô∏è Total filas: {total_rows:,}\")\n",
    "    print(f\"‚û°Ô∏è Archivos generados: {num_files}\")\n",
    "    return output_path\n",
    "\n",
    "train_path = write_tfrecords_dynamic(\n",
    "    \"teams.data_science.gp_pp_train_churn3_scaled\",\n",
    "    f\"{PREFIX_PATH}gpereyra/pp_nn/churn3_tfdata/train\"\n",
    ")\n",
    "val_path = write_tfrecords_dynamic(\n",
    "    \"teams.data_science.gp_pp_val_churn3_scaled\",\n",
    "    f\"{PREFIX_PATH}gpereyra/pp_nn/churn3_tfdata/val\"\n",
    ")\n",
    "test_path = write_tfrecords_dynamic(\n",
    "    \"teams.data_science.gp_pp_test_churn3_scaled\",\n",
    "    f\"{PREFIX_PATH}gpereyra/pp_nn/churn3_tfdata/test\"\n",
    ")\n",
    "\n",
    "print(\"\\nTodos los datasets convertidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e197542-0f72-4c56-8b2f-8bf4e6bc6d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from pyspark.sql.functions import col, udf\n",
    "# from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# # Funci√≥n para convertir Vector a Array\n",
    "# vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "# def write_tfrecords(table_name, output_path, num_files=50):\n",
    "#     \"\"\"\n",
    "#     Convierte tabla Delta a TFRecords (distribuido, sin usar driver memory)\n",
    "#     \"\"\"\n",
    "#     df = spark.table(table_name)\n",
    "    \n",
    "#     # Convertir Vector a Array\n",
    "#     df = df.withColumn(\"features\", vector_to_array(col(\"features_scaled\"))) \\\n",
    "#            .select(\"features\", \"churn3\")\n",
    "    \n",
    "#     # Repartir para escritura paralela\n",
    "#     df = df.repartition(num_files)\n",
    "    \n",
    "#     # Guardar como Parquet primero (m√°s eficiente)\n",
    "#     df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "#     print(f\"‚úÖ Datos guardados en Parquet: {output_path}\")\n",
    "#     return output_path\n",
    "\n",
    "# # Escribir los 3 datasets\n",
    "# print(\"Convirtiendo a formato Parquet...\")\n",
    "# train_path = write_tfrecords(\n",
    "#     \"teams.data_science.gp_pp_train_churn3_scaled\",\n",
    "#     \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/train\",\n",
    "#     num_files=100\n",
    "# )\n",
    "# val_path = write_tfrecords(\n",
    "#     \"teams.data_science.gp_pp_val_churn3_scaled\",\n",
    "#     \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/val\",\n",
    "#     num_files=20\n",
    "# )\n",
    "# test_path = write_tfrecords(\n",
    "#     \"teams.data_science.gp_pp_test_churn3_scaled\",\n",
    "#     \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/test\",\n",
    "#     num_files=20\n",
    "# )\n",
    "\n",
    "# print(\"\\nTodos los datasets convertidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3c8357-9619-4297-8572-11a114726223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# def create_tf_dataset_from_parquet(parquet_path, batch_size=2048, shuffle=True):\n",
    "#     \"\"\"\n",
    "#     Lee Parquet con TensorFlow de forma eficiente\n",
    "#     \"\"\"\n",
    "#     # Normalizar path para dbutils\n",
    "#     parquet_path_dbfs = parquet_path.replace('/dbfs', 'dbfs:')\n",
    "    \n",
    "#     # Listar archivos parquet\n",
    "#     files = dbutils.fs.ls(parquet_path_dbfs)\n",
    "#     parquet_files = [f.path for f in files if f.name.endswith('.parquet')]\n",
    "    \n",
    "#     print(f\"Encontrados {len(parquet_files)} archivos parquet\")\n",
    "    \n",
    "#     def generator():\n",
    "#         \"\"\"Lee Parquet files en batches\"\"\"\n",
    "#         for file_path in parquet_files:\n",
    "#             # Convertir de dbfs: a /dbfs/ para pyarrow\n",
    "#             local_path = file_path.replace('dbfs:', '/dbfs')\n",
    "            \n",
    "#             # Leer archivo parquet\n",
    "#             table = pq.read_table(local_path)\n",
    "            \n",
    "#             # Convertir a numpy\n",
    "#             features = np.array([np.array(x, dtype=np.float32) for x in table['features'].to_pylist()])\n",
    "#             labels = table['churn3'].to_numpy().astype(np.int32)\n",
    "            \n",
    "#             # Yield todos los rows del archivo\n",
    "#             for i in range(len(features)):\n",
    "#                 yield features[i], labels[i]\n",
    "    \n",
    "#     # Crear dataset\n",
    "#     dataset = tf.data.Dataset.from_generator(\n",
    "#         generator,\n",
    "#         output_signature=(\n",
    "#             tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "#             tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "    \n",
    "#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# # Crear datasets\n",
    "# print(\"Creando TF Datasets desde Parquet...\")\n",
    "# BATCH_SIZE = 2048\n",
    "\n",
    "# train_dataset = create_tf_dataset_from_parquet(\"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/train\", BATCH_SIZE, shuffle=True)\n",
    "# val_dataset = create_tf_dataset_from_parquet(\"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/val\", BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# # Calcular steps\n",
    "# train_size = 20_829_767\n",
    "# val_size = 2_601_368\n",
    "# steps_per_epoch = train_size // BATCH_SIZE\n",
    "# validation_steps = val_size // BATCH_SIZE\n",
    "\n",
    "# print(f\"\\n‚úÖ Steps por epoch: {steps_per_epoch}\")\n",
    "# print(f\"‚úÖ Validation steps: {validation_steps}\")\n",
    "\n",
    "# # TEST cr√≠tico\n",
    "# print(\"\\nüß™ Probando lectura de 1 batch...\")\n",
    "# for x_batch, y_batch in train_dataset.take(1):\n",
    "#     print(f\"‚úÖ Features shape: {x_batch.shape}\")\n",
    "#     print(f\"‚úÖ Labels shape: {y_batch.shape}\")\n",
    "#     print(f\"‚úÖ Distribuci√≥n labels: {np.bincount(y_batch.numpy())}\")\n",
    "#     print(f\"‚úÖ Rango features: [{x_batch.numpy().min():.2f}, {x_batch.numpy().max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8228be6e-7a89-443b-abbd-3ac9c8c576e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y tensorflow tensorflow-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a264e530-3b51-441f-aeff-5301e3691072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install \"tensorflow[and-cuda]==2.17.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94484ab1-6985-4d2a-aebd-0a17ca6af3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5e4cc5-f132-42b7-8144-7630de562845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5d5c4f-ce4e-4bf3-a541-027b433f1a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05061186-c747-42bb-8583-f3a9dc0d3a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7900061e-c983-4c4a-a08c-1e59ba66233e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PREFIX_PATH = '/tmp/'\n",
    "# PREFIX_PATH = '/mnt/jc-analytics-databricks-work/analytics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dab8446-8a85-45d5-9980-3718d438d5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "def create_tf_dataset_from_parquet(parquet_path, batch_size=2048, shuffle=True):\n",
    "    \"\"\"\n",
    "    Lee Parquet con TensorFlow de forma portable (compatible con Serverless GPU)\n",
    "    \"\"\"\n",
    "    # Parquet path debe usar prefijo dbfs:\n",
    "    parquet_path_dbfs = parquet_path if parquet_path.startswith(\"dbfs:\") else f\"dbfs:{parquet_path}\"\n",
    "    \n",
    "    # Listar archivos en la carpeta\n",
    "    files = dbutils.fs.ls(parquet_path_dbfs)\n",
    "    parquet_files = [f.path for f in files if f.name.endswith(\".parquet\")]\n",
    "\n",
    "    print(f\"üì¶ Encontrados {len(parquet_files)} archivos parquet en {parquet_path_dbfs}\")\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Lee Parquet files directamente desde DBFS en memoria\"\"\"\n",
    "        for file_path in parquet_files:\n",
    "            with dbutils.fs.open(file_path, \"rb\") as f:\n",
    "                data = f.read()\n",
    "                table = pq.read_table(io.BytesIO(data))\n",
    "            \n",
    "            features = np.array(\n",
    "                [np.array(x, dtype=np.float32) for x in table[\"features\"].to_pylist()]\n",
    "            )\n",
    "            labels = table[\"churn3\"].to_numpy().astype(np.int32)\n",
    "\n",
    "            for i in range(len(features)):\n",
    "                yield features[i], labels[i]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(65,), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Crear datasets din√°micamente\n",
    "# --------------------------------------------------------\n",
    "print(\"Creando TF Datasets desde Parquet...\")\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "PREFIX_PATH = \"dbfs:/tmp/gpereyra/pp_nn/churn3_tfdata/\"\n",
    "\n",
    "train_path = f\"{PREFIX_PATH}train\"\n",
    "val_path = f\"{PREFIX_PATH}val\"\n",
    "\n",
    "# üîπ Contar filas din√°micamente\n",
    "train_size = spark.table(\"teams.data_science.gp_pp_train_churn3_scaled\").count()\n",
    "val_size = spark.table(\"teams.data_science.gp_pp_val_churn3_scaled\").count()\n",
    "\n",
    "steps_per_epoch = max(1, train_size // BATCH_SIZE)\n",
    "validation_steps = max(1, val_size // BATCH_SIZE)\n",
    "\n",
    "print(f\"\\nüìä Filas train: {train_size:,}\")\n",
    "print(f\"üìä Filas val: {val_size:,}\")\n",
    "print(f\"‚úÖ Steps por epoch: {steps_per_epoch}\")\n",
    "print(f\"‚úÖ Validation steps: {validation_steps}\")\n",
    "\n",
    "# üîπ Crear datasets\n",
    "train_dataset = create_tf_dataset_from_parquet(train_path, BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_tf_dataset_from_parquet(val_path, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# TEST cr√≠tico\n",
    "print(\"\\nüß™ Probando lectura de 1 batch...\")\n",
    "for x_batch, y_batch in train_dataset.take(1):\n",
    "    print(f\"‚úÖ Features shape: {x_batch.shape}\")\n",
    "    print(f\"‚úÖ Labels shape: {y_batch.shape}\")\n",
    "    print(f\"‚úÖ Distribuci√≥n labels: {np.bincount(y_batch.numpy())}\")\n",
    "    print(f\"‚úÖ Rango features: [{x_batch.numpy().min():.2f}, {x_batch.numpy().max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e33fb1-9aa2-4989-8c4c-1b765df6960d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Calcular class weights din√°micamente desde train\n",
    "train_class_counts = spark.table(\"teams.data_science.gp_pp_train_churn3_scaled\") \\\n",
    "    .groupBy(\"churn3\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"churn3\") \\\n",
    "    .collect()\n",
    "\n",
    "# Extraer counts\n",
    "count_class_0 = train_class_counts[0]['count']\n",
    "count_class_1 = train_class_counts[1]['count']\n",
    "total = count_class_0 + count_class_1\n",
    "\n",
    "# M√©todo 1: Inverse frequency (el que usabas)\n",
    "class_weight = {\n",
    "    0: 1.0,\n",
    "    1: count_class_0 / count_class_1\n",
    "}\n",
    "\n",
    "# M√©todo 2: Balanced (alternativa, a veces mejor)\n",
    "# class_weight = {\n",
    "#     0: total / (2 * count_class_0),\n",
    "#     1: total / (2 * count_class_1)\n",
    "# }\n",
    "\n",
    "print(f\"Distribuci√≥n train:\")\n",
    "print(f\"  Clase 0: {count_class_0:,} ({count_class_0/total*100:.2f}%)\")\n",
    "print(f\"  Clase 1: {count_class_1:,} ({count_class_1/total*100:.2f}%)\")\n",
    "print(f\"\\nClass weights: {class_weight}\")\n",
    "print(f\"  Ratio: 1:{class_weight[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7529ee-8c8d-4ffa-b764-3fa4840957c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "# Arquitectura simple y efectiva para clasificaci√≥n binaria\n",
    "def create_model(input_dim=65, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    NN simple con regularizaci√≥n para evitar overfitting en desbalance\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Layer 1\n",
    "        layers.Dense(128, activation='relu', \n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Layer 2\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Layer 3\n",
    "        layers.Dense(32, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Output (sigmoid para binaria)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# Compilar con m√©tricas apropiadas para desbalance\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.AUC(name='pr_auc', curve='PR')  # Precision-Recall AUC\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Modelo creado y compilado\")\n",
    "print(f\"‚úÖ Total par√°metros: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac870320-e687-40ed-aadc-d6b880f1c671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.ls(\"/tmp/gpereyra/pp_nn/churn3_tfdata/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2257531-a89d-4ed5-883c-e8e80c4ba85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Directorio para guardar modelo\n",
    "model_path = f\"{PREFIX_PATH}gpereyra/pp_nn/churn3_models\"\n",
    "dbutils.fs.mkdirs(f\"dbfs:{model_path}\")\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Early stopping en validation AUC (mejor m√©trica para desbalance)\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=5,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Guardar mejor modelo\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{model_path}/best_model.keras\",\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reducir learning rate si no mejora\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_auc',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        mode='max',\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard (opcional)\n",
    "    TensorBoard(\n",
    "        log_dir=f\"{model_path}/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        histogram_freq=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configurados\")\n",
    "\n",
    "# Entrenar\n",
    "print(\"\\nIniciando entrenamiento...\")\n",
    "print(f\"Epochs: 20 (con early stopping)\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs found:\", gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception as e:\n",
    "        print(\"Could not set memory growth:\", e)\n",
    "\n",
    "# Ajustar batch size si GPU disponible\n",
    "if gpus:\n",
    "    BATCH_SIZE = BATCH_SIZE * 2  \n",
    "    print(f\"Adjusted batch size for GPU: {BATCH_SIZE}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a2530c-a5b0-4794-a83d-434a57125b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "# Cargar mejor modelo\n",
    "best_model = keras.models.load_model(f\"{model_path}/best_model.keras\")\n",
    "\n",
    "# Leer test set\n",
    "test_dataset = create_tf_dataset_from_parquet(\"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/test\", BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af631cc-26d2-4bce-adcc-0308e887086c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_size = spark.table(\"teams.data_science.gp_pp_test_churn3_scaled\").count()\n",
    "test_steps = test_size // BATCH_SIZE\n",
    "\n",
    "print(\"üìä Evaluando en test set...\")\n",
    "test_results = best_model.evaluate(test_dataset, steps=test_steps, verbose=1)\n",
    "\n",
    "print(\"\\nüìà M√©tricas en Test:\")\n",
    "for name, value in zip(best_model.metrics_names, test_results):\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# Obtener predicciones (probabilidades)\n",
    "print(\"\\nüîÆ Generando predicciones...\")\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    preds = best_model.predict(x_batch, verbose=0)\n",
    "    y_pred_proba.extend(preds.flatten())\n",
    "    y_true.extend(y_batch.numpy())\n",
    "    \n",
    "    if len(y_true) >= test_size:\n",
    "        break\n",
    "\n",
    "y_pred_proba = np.array(y_pred_proba[:test_size])\n",
    "y_true = np.array(y_true[:test_size])\n",
    "\n",
    "print(f\"‚úÖ Predicciones obtenidas: {len(y_pred_proba):,}\")\n",
    "\n",
    "# Evaluar con threshold default (0.5)\n",
    "y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nüìä Resultados con threshold=0.5:\")\n",
    "print(classification_report(y_true, y_pred_default, target_names=['No Churn', 'Churn']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d01a47-07ff-4653-bf41-fe15a4c11de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üìà M√©tricas en Test:\n",
    "# loss: 0.6047\n",
    "# compile_metrics: 0.8058\n",
    "\n",
    "# üîÆ Generando predicciones...\n",
    "# ‚úÖ Predicciones obtenidas: 10,033\n",
    "\n",
    "# üìä Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.81      0.89      9701\n",
    "#        Churn       0.13      0.82      0.22       332\n",
    "\n",
    "#     accuracy                           0.81     10033\n",
    "#    macro avg       0.56      0.81      0.55     10033\n",
    "# weighted avg       0.96      0.81      0.87     10033\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "# [[7828 1873]\n",
    "#  [  60  272]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c24cdb7-3e77-4414-badc-edf87b95d12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10k\n",
    "# ‚úÖ Predicciones obtenidas: 1,057\n",
    "\n",
    "# üìä Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.84      0.91      1013\n",
    "#        Churn       0.18      0.84      0.30        44\n",
    "\n",
    "#     accuracy                           0.84      1057\n",
    "#    macro avg       0.59      0.84      0.60      1057\n",
    "# weighted avg       0.96      0.84      0.88      1057\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "# [[846 167]\n",
    "#  [  7  37]]\n",
    "\n",
    "\n",
    "\n",
    "# 100k\n",
    "# üìà M√©tricas en Test:\n",
    "# loss: 0.5824\n",
    "# compile_metrics: 0.8002\n",
    "\n",
    "# üîÆ Generando predicciones...\n",
    "# ‚úÖ Predicciones obtenidas: 10,073\n",
    "\n",
    "# üìä Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.80      0.88      9730\n",
    "#        Churn       0.12      0.81      0.22       343\n",
    "\n",
    "#     accuracy                           0.80     10073\n",
    "#    macro avg       0.56      0.81      0.55     10073\n",
    "# weighted avg       0.96      0.80      0.86     10073\n",
    "\n",
    "\n",
    "# onfusion Matrix:\n",
    "# [[7764 1966]\n",
    "#  [  64  279]]\n",
    "\n",
    "# 100k -- red mas simple\n",
    "# üìà M√©tricas en Test:\n",
    "# loss: 0.5279\n",
    "# compile_metrics: 0.8391\n",
    "\n",
    "# üîÆ Generando predicciones...\n",
    "# ‚úÖ Predicciones obtenidas: 10,073\n",
    "\n",
    "# üìä Resultados con threshold=0.5:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     No Churn       0.99      0.84      0.91      9730\n",
    "#        Churn       0.14      0.77      0.24       343\n",
    "\n",
    "#     accuracy                           0.84     10073\n",
    "#    macro avg       0.57      0.80      0.58     10073\n",
    "# weighted avg       0.96      0.84      0.89     10073\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "# [[8173 1557]\n",
    "#  [  80  263]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ff7e5b-e670-4ce4-a602-ef4d43608130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve, f1_score\n",
    "\n",
    "# Cargar mejor modelo\n",
    "best_model = keras.models.load_model(f\"{model_path}/best_model.keras\")\n",
    "\n",
    "# Leer test set\n",
    "test_path = \"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn3_tfdata/test\"\n",
    "test_dataset = create_tf_dataset_from_parquet(test_path, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Calcular size din√°micamente\n",
    "test_size = spark.table(\"teams.data_science.gp_pp_test_churn3_scaled\").count()\n",
    "test_steps = test_size // BATCH_SIZE\n",
    "\n",
    "print(f\"Test size: {test_size:,}\")\n",
    "\n",
    "print(\"\\nüìä Evaluando en test set...\")\n",
    "test_results = best_model.evaluate(test_dataset, steps=test_steps, verbose=1)\n",
    "\n",
    "print(\"\\nüìà M√©tricas en Test:\")\n",
    "for name, value in zip(best_model.metrics_names, test_results):\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# Obtener predicciones\n",
    "print(\"\\nüîÆ Generando predicciones...\")\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    preds = best_model.predict(x_batch, verbose=0)\n",
    "    y_pred_proba.extend(preds.flatten())\n",
    "    y_true.extend(y_batch.numpy())\n",
    "    \n",
    "    if len(y_true) >= test_size:\n",
    "        break\n",
    "\n",
    "y_pred_proba = np.array(y_pred_proba[:test_size])\n",
    "y_true = np.array(y_true[:test_size])\n",
    "\n",
    "print(f\"‚úÖ Predicciones obtenidas: {len(y_pred_proba):,}\")\n",
    "print(f\"   Distribuci√≥n real - Churn: {y_true.sum():,} ({y_true.sum()/len(y_true)*100:.2f}%)\")\n",
    "\n",
    "# Evaluar con threshold default (0.5)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESULTADOS CON THRESHOLD = 0.5 (default)\")\n",
    "print(\"=\"*60)\n",
    "y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_true, y_pred_default, target_names=['No Churn', 'Churn'], digits=4))\n",
    "\n",
    "cm_default = confusion_matrix(y_true, y_pred_default)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_default)\n",
    "print(f\"\\nFalsos Negativos (churners perdidos): {cm_default[1,0]:,}\")\n",
    "print(f\"Verdaderos Positivos (churners detectados): {cm_default[1,1]:,}\")\n",
    "\n",
    "# Encontrar threshold √≥ptimo seg√∫n diferentes criterios\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç B√öSQUEDA DE THRESHOLD √ìPTIMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Threshold que maximiza F1-Score\n",
    "thresholds_to_test = np.arange(0.1, 0.9, 0.05)\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    y_pred_temp = (y_pred_proba >= thresh).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred_temp)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_threshold = thresholds_to_test[best_f1_idx]\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ Threshold que maximiza F1-Score: {best_f1_threshold:.2f}\")\n",
    "print(f\"   F1-Score: {f1_scores[best_f1_idx]:.4f}\")\n",
    "\n",
    "# # 2. Curva Precision-Recall para decidir seg√∫n negocio\n",
    "# precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "\n",
    "# # Encontrar threshold con Recall >= 0.70 (capturar 70% de churners)\n",
    "# target_recall = 0.70\n",
    "# idx_recall_70 = np.argmin(np.abs(recall - target_recall))\n",
    "# threshold_recall_70 = pr_thresholds[idx_recall_70] if idx_recall_70 < len(pr_thresholds) else 0.5\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ Threshold para Recall ‚â• {target_recall:.0%}: {threshold_recall_70:.3f}\")\n",
    "print(f\"   Recall logrado: {recall[idx_recall_70]:.4f}\")\n",
    "print(f\"   Precision: {precision[idx_recall_70]:.4f}\")\n",
    "\n",
    "# 3. Encontrar threshold con Precision >= 0.15 (15% de aciertos en predicciones)\n",
    "target_precision = 0.15\n",
    "idx_prec_15 = np.argmin(np.abs(precision - target_precision))\n",
    "threshold_prec_15 = pr_thresholds[idx_prec_15] if idx_prec_15 < len(pr_thresholds) else 0.5\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ Threshold para Precision ‚â• {target_precision:.0%}: {threshold_prec_15:.3f}\")\n",
    "print(f\"   Precision lograda: {precision[idx_prec_15]:.4f}\")\n",
    "print(f\"   Recall: {recall[idx_prec_15]:.4f}\")\n",
    "\n",
    "# Evaluaci√≥n detallada con threshold √≥ptimo F1\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä RESULTADOS CON THRESHOLD √ìPTIMO = {best_f1_threshold:.2f}\")\n",
    "print(\"=\"*60)\n",
    "y_pred_optimal = (y_pred_proba >= best_f1_threshold).astype(int)\n",
    "print(classification_report(y_true, y_pred_optimal, target_names=['No Churn', 'Churn'], digits=4))\n",
    "\n",
    "cm_optimal = confusion_matrix(y_true, y_pred_optimal)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_optimal)\n",
    "print(f\"\\nFalsos Negativos (churners perdidos): {cm_optimal[1,0]:,}\")\n",
    "print(f\"Verdaderos Positivos (churners detectados): {cm_optimal[1,1]:,}\")\n",
    "print(f\"Mejora en detecci√≥n: {(cm_optimal[1,1] - cm_default[1,1]):,} churners m√°s detectados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c686a1-59ee-41e4-b6ba-ea4ad6ed57cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cm_optimal = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_optimal)\n",
    "print(f\"\\nFalsos Negativos (churners perdidos): {cm_optimal[1,0]:,}\")\n",
    "print(f\"Verdaderos Positivos (churners detectados): {cm_optimal[1,1]:,}\")\n",
    "print(f\"Mejora en detecci√≥n: {(cm_optimal[1,1] - cm_default[1,1]):,} churners m√°s detectados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0240bd20-cf06-4c66-b165-f550e9ebf4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(pr_thresholds, recall[:-1], label='Recall', color='green')\n",
    "plt.plot(pr_thresholds, precision[:-1], label='Precision', color='orange')\n",
    "plt.axvline(0.85, color='gray', linestyle='--', label='Threshold 0.85')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision y Recall seg√∫n Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) nn-test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7491c6ec-2d8d-4a86-afcd-93874665aff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  PyTorch MLflow tutorial\n",
    "\n",
    "This tutorial covers the full lifecycle of experimentation, training, tuning, registration, evaluation, and deployment for a deep learning modeling project. It shows you how to use MLflow to keep track of every aspect of the model development and deployment processes.\n",
    "\n",
    "\n",
    "In this step-by-step tutorial, you'll discover how to:\n",
    "- **Generate and visualize data:** Create synthetic data to simulate real-world scenarios, and visualize feature relationships.\n",
    "- **Design and train neural networks:** Build a PyTorch neural network for regression and train it with proper optimization techniques.\n",
    "- **Track with MLflow:** Log important metrics, parameters, artifacts, and models using MLflow, including visualizations.\n",
    "- **Tune hyperparameters:** Use Optuna for hyperparameter optimization with PyTorch models.\n",
    "- **Register models:** Register your model with Unity Catalog, preparing it for review and future deployment.\n",
    "- **Deploy models:** Load your registered model, make predictions, and perform error analysis, both locally and in a distributed setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa7aad8-848a-46b3-a795-06b7c48cd69d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "(Optional) Install the latest version of MLflow"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -Uqqq mlflow pytorch-lightning optuna skorch uv optuna-integration[pytorch_lightning]\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910fbeb0-29cc-430c-8181-cf615beeb869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Dict, List, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Metric, Param  \n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd1fe1e4-c7bf-4e2f-9e99-6882d440ae32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0. Configure the Model Registry with Unity Catalog\n",
    "\n",
    "One of the key advantages of using MLflow on Databricks is the seamless integration with **Unity Catalog**. This integration simplifies model management and governance, ensuring that every model you develop is tracked, versioned, and secure. For more information about Unity Catalog, see ([AWS](https://docs.databricks.com/aws/en/data-governance/unity-catalog) | [Azure](https://learn.microsoft.com/azure/databricks/data-governance/unity-catalog) | [GCP](https://docs.databricks.com/gcp/en/data-governance/unity-catalog)).\n",
    "\n",
    "### Set the registry URI\n",
    "\n",
    "The following cell configures MLflow to use Unity Catalog for model registration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250ce633-3314-4142-851b-512ae2bf9667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11384491-7b4b-4410-85cc-c6558d7bc708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 1. Create a synthetic regression dataset\n",
    "\n",
    "The next cell defines the `create_regression_data` function. This function generates synthetic data for regression. The resulting dataset includes linear and non-linear relationships between the features and the target, noise, and features with varying importance. These features are designed to mimic real-world data scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e733302-eb9c-40b2-89c7-e064bc2b3e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_regression_data(\n",
    "    n_samples: int, \n",
    "    n_features: int,\n",
    "    seed: int = 1994,\n",
    "    noise_level: float = 0.3,\n",
    "    nonlinear: bool = True\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Generates synthetic regression data with interesting correlations for MLflow and PyTorch demonstrations.\n",
    "\n",
    "    This function creates a DataFrame of continuous features and computes a target variable with nonlinear\n",
    "    relationships and interactions between features. The data is designed to be complex enough to demonstrate\n",
    "    the capabilities of deep learning, but not so complex that a reasonable model can't be learned.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Number of samples (rows) to generate.\n",
    "        n_features (int): Number of feature columns.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 1994.\n",
    "        noise_level (float, optional): Level of Gaussian noise to add to the target. Defaults to 0.3.\n",
    "        nonlinear (bool, optional): Whether to add nonlinear feature transformations. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.Series]:\n",
    "            - pd.DataFrame: DataFrame containing the synthetic features.\n",
    "            - pd.Series: Series containing the target labels.\n",
    "\n",
    "    Example:\n",
    "        >>> df, target = create_regression_data(n_samples=1000, n_features=10)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Generate random continuous features\n",
    "    X = rng.uniform(-5, 5, size=(n_samples, n_features))\n",
    "    \n",
    "    # Create feature DataFrame with meaningful names\n",
    "    columns = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    \n",
    "    # Generate base target variable with linear relationship to a subset of features\n",
    "    # Use only the first n_features//2 features to create some irrelevant features\n",
    "    weights = rng.uniform(-2, 2, size=n_features//2)\n",
    "    target = np.dot(X[:, :n_features//2], weights)\n",
    "    \n",
    "    # Add some nonlinear transformations if requested\n",
    "    if nonlinear:\n",
    "        # Add square term for first feature\n",
    "        target += 0.5 * X[:, 0]**2\n",
    "        \n",
    "        # Add interaction between the second and third features\n",
    "        if n_features >= 3:\n",
    "            target += 1.5 * X[:, 1] * X[:, 2]\n",
    "        \n",
    "        # Add sine transformation of fourth feature\n",
    "        if n_features >= 4:\n",
    "            target += 2 * np.sin(X[:, 3])\n",
    "        \n",
    "        # Add exponential of fifth feature, scaled down\n",
    "        if n_features >= 5:\n",
    "            target += 0.1 * np.exp(X[:, 4] / 2)\n",
    "            \n",
    "        # Add threshold effect for sixth feature\n",
    "        if n_features >= 6:\n",
    "            target += 3 * (X[:, 5] > 1.5).astype(float)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = rng.normal(0, noise_level * target.std(), size=n_samples)\n",
    "    target += noise\n",
    "    \n",
    "    # Add a few more interesting features to the DataFrame\n",
    "    \n",
    "    # Add a correlated feature (but not used in target calculation)\n",
    "    if n_features >= 7:\n",
    "        df['feature_correlated'] = df['feature_0'] * 0.8 + rng.normal(0, 0.2, size=n_samples)\n",
    "    \n",
    "    # Add a cyclical feature\n",
    "    df['feature_cyclical'] = np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
    "    \n",
    "    # Add a feature with outliers\n",
    "    df['feature_with_outliers'] = rng.normal(0, 1, size=n_samples)\n",
    "    # Add outliers to ~1% of samples\n",
    "    outlier_idx = rng.choice(n_samples, size=n_samples//100, replace=False)\n",
    "    df.loc[outlier_idx, 'feature_with_outliers'] = rng.uniform(10, 15, size=len(outlier_idx))\n",
    "    \n",
    "    return df, pd.Series(target, name='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "300713d7-a1c0-41f0-b696-bfff0def4c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Exploratory data analysis (EDA) visualizations\n",
    "\n",
    "Visualizations help you understand the data. The code in the following cell creates 6 functions, each of which generates a different plot to help you visually inspect your dataset. \n",
    "\n",
    "You can use MLflow to log visualizations as artifacts, making your experimentation fully reproducible.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b07875f-f1f6-419b-bcb4-83a7f4e8d606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_distributions(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of histograms for each feature in the dataset.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the distribution plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            sns.histplot(X[feature], ax=ax, kde=True, color='skyblue')\n",
    "            ax.set_title(f'Distribution of {feature}')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature Distributions', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_heatmap(X: pd.DataFrame, y: pd.Series) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a correlation heatmap of all features and the target variable.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the heatmap.\n",
    "    \"\"\"\n",
    "    # Combine features and target into one DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Draw the heatmap with a color bar\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap=cmap,\n",
    "                center=0, square=True, linewidths=0.5, ax=ax)\n",
    "    \n",
    "    ax.set_title('Feature Correlation Heatmap', fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_feature_target_relationships(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of scatter plots showing the relationship between each feature and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the relationship plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Scatter plot with regression line\n",
    "            sns.regplot(x=X[feature], y=y, ax=ax, \n",
    "                       scatter_kws={'alpha': 0.5, 'color': 'blue'}, \n",
    "                       line_kws={'color': 'red'})\n",
    "            ax.set_title(f'{feature} vs Target')\n",
    "    \n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature vs Target Relationships', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_pairwise_relationships(X: pd.DataFrame, y: pd.Series, features: list[str]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a pairplot showing relationships between selected features and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        features (List[str]): List of feature names to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the pairplot.\n",
    "    \"\"\"\n",
    "    # Ensure features exist in the DataFrame\n",
    "    valid_features = [f for f in features if f in X.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.text(0.5, 0.5, \"No valid features provided\", ha='center', va='center')\n",
    "        return fig\n",
    "    \n",
    "    # Combine selected features and target\n",
    "    data = X[valid_features].copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Create pairplot\n",
    "    pairgrid = sns.pairplot(data, diag_kind=\"kde\", \n",
    "                          plot_kws={\"alpha\": 0.6, \"s\": 50},\n",
    "                          corner=True)\n",
    "    \n",
    "    pairgrid.fig.suptitle(\"Pairwise Feature Relationships\", y=1.02, fontsize=16)\n",
    "    plt.close(pairgrid.fig)\n",
    "    return pairgrid.fig\n",
    "\n",
    "def plot_outliers(X: pd.DataFrame, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of box plots to detect outliers in each feature.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the outlier plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Box plot to detect outliers\n",
    "            sns.boxplot(x=X[feature], ax=ax, color='skyblue')\n",
    "            ax.set_title(f'Outlier Detection for {feature}')\n",
    "            ax.set_xlabel(feature)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Outlier Detection for Features', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_residuals(y_true: pd.Series, y_pred: np.ndarray) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a residual plot to analyze model prediction errors.\n",
    "    \n",
    "    Args:\n",
    "        y_true (pd.Series): True target values.\n",
    "        y_pred (np.ndarray): Predicted target values.\n",
    "        \n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the residual plot.\n",
    "    \"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot of predicted values vs residuals\n",
    "    ax.scatter(y_pred, residuals, alpha=0.5)\n",
    "    ax.axhline(y=0, color='r', linestyle='-')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.set_title('Residual Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "323b9352-52fe-4a51-af9c-e9910f3c6baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Design a PyTorch neural network for regression\n",
    "\n",
    "The code in the following cell defines the PyTorch model architecture. It creates a flexible neural network with the following characteristics:\n",
    "- **Configurable architecture:** Adjustable number and size of hidden layers.\n",
    "- **Activation functions:** ReLU for hidden layers, linear for output.\n",
    "- **Regularization:** Optional dropout to prevent overfitting.\n",
    "- **Layer normalization:** To stabilize training and accelerate convergence.\n",
    "\n",
    "To demonstrate different approaches, the following cells show how to create the neural network first using a standard PyTorch module and then using a PyTorch Lightning module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1c21fb-fd67-408f-b7af-217cf9e66465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RegressionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible feedforward neural network for regression tasks.\n",
    "    \n",
    "    Attributes:\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "        dropout_rate (float): Dropout probability for regularization.\n",
    "        use_layer_norm (bool): Whether to use layer normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: List[int] = [64, 32],\n",
    "        dropout_rate: float = 0.1,\n",
    "        use_layer_norm: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout_rate (float): Dropout probability for regularization.\n",
    "            use_layer_norm (bool): Whether to use layer normalization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        \n",
    "        # Build layers dynamically based on hidden_dims\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            \n",
    "            if use_layer_norm:\n",
    "                layers.append(nn.LayerNorm(dim))\n",
    "                \n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "                \n",
    "            prev_dim = dim\n",
    "        \n",
    "        # Output layer (single output for regression)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        # Combine all layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.model(x).squeeze()\n",
    "    \n",
    "    def get_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return model parameters as a dictionary for MLflow logging.\"\"\"\n",
    "        return {\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_dims\": self.hidden_dims,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"use_layer_norm\": self.use_layer_norm\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a68c36a-4934-41c6-b9fc-f6ef164af75e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RegressionLightningModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for regression tasks.\n",
    "    \n",
    "    This class wraps the RegressionNN model and adds training, validation,\n",
    "    and testing logic using the PyTorch Lightning framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: List[int] = [64, 32],\n",
    "        dropout_rate: float = 0.1,\n",
    "        use_layer_norm: bool = True,\n",
    "        learning_rate: float = 1e-3,\n",
    "        weight_decay: float = 1e-5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Lightning module.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout_rate (float): Dropout probability for regularization.\n",
    "            use_layer_norm (bool): Whether to use layer normalization.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "            weight_decay (float): Weight decay for L2 regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = RegressionNN(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_layer_norm=use_layer_norm\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure the optimizer for training.\"\"\"\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Perform a training step.\"\"\"\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Perform a validation step.\"\"\"\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        rmse = torch.sqrt(loss)\n",
    "        mae = torch.mean(torch.abs(y_pred - y))\n",
    "        \n",
    "        self.log('val_rmse', rmse, prog_bar=True)\n",
    "        self.log('val_mae', mae, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Perform a test step.\"\"\"\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        \n",
    "        # Calculate metrics for test set\n",
    "        rmse = torch.sqrt(loss)\n",
    "        mae = torch.mean(torch.abs(y_pred - y))\n",
    "        \n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_rmse', rmse)\n",
    "        self.log('test_mae', mae)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return model parameters as a dictionary for MLflow logging.\"\"\"\n",
    "        return {\n",
    "            \"input_dim\": self.hparams.input_dim,\n",
    "            \"hidden_dims\": self.hparams.hidden_dims,\n",
    "            \"dropout_rate\": self.hparams.dropout_rate,\n",
    "            \"use_layer_norm\": self.hparams.use_layer_norm,\n",
    "            \"learning_rate\": self.hparams.learning_rate,\n",
    "            \"weight_decay\": self.hparams.weight_decay\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00757850-164b-4030-bb69-79c959276280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, batch_size: int = 32\n",
    "):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders for training, validation, and testing.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_val, y_val: Validation data and labels.\n",
    "        X_test, y_test: Test data and labels.\n",
    "        batch_size (int): Batch size for the DataLoaders.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader, scaler)\n",
    "    \"\"\"\n",
    "    # Initialize a scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors - explicitly set dtype to float32\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    \n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "    \n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "    \n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4425521-5ca2-4804-bdb7-7e86368f60fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Standard modeling workflow\n",
    "\n",
    "The code in the next cell implements a standard PyTorch modeling workflow with MLflow integration, using the following steps:\n",
    "1. Generate and explore synthetic data.\n",
    "2. Split the data into training, validation, and test sets.\n",
    "3. Scale the data and create PyTorch DataLoaders.\n",
    "4. Define and train a neural network model.\n",
    "5. Evaluate the model's performance.\n",
    "6. Log metrics, parameters, and artifacts to MLflow.\n",
    "\n",
    "This standard workflow provides a baseline model. You can then use hyperparameter tuning to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cd9b4a-65d8-4c6d-9b52-73205294d799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"teams.data_science.pp_churn_features_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac28f5f7-c609-4c1a-a6ae-9075b1760cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_path = spark.sql(\"DESCRIBE DETAIL teams.data_science.pp_churn_features_v2\").collect()[0]['location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1824d61-2487-4df9-8ab7-420a54507776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6ab21e-d7c1-469f-9bee-49d910dc697a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Primero, preprocesás tus datos en Spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "cols_to_drop = ['judi', 'date', 'churn7', 'churn14','subs_lifetime_amt','subs_revenue_amt','daily_avg_boosters_used_ref','daily_avg_boosters_used_ref']\n",
    "df_clean = (\n",
    "    df.drop(*cols_to_drop)\n",
    "      .withColumn(\"churn3\", F.col(\"churn3\").cast(\"int\"))\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "# Guardás el dataset en un path accesible para todos los workers\n",
    "data_path = \"dbfs:/tmp/churn_dataset\"\n",
    "df_clean.write.mode(\"overwrite\").format(\"parquet\").save(\"/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f92d84-cd7e-41e4-8887-993bf1cb23ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# en una celda notebook\n",
    "%pip install petastorm\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2510cc6c-5c22-43c2-b9b4-bb46a9b39298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definís la función de entrenamiento: se ejecuta en cada worker\n",
    "def train_fn(checkpoint_dir):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    import petastorm\n",
    "    from petastorm import make_batch_reader\n",
    "\n",
    "    reader = make_batch_reader(\"/dbfs/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/churn_dataset\")\n",
    "    X_list, y_list = [], []\n",
    "    for batch in reader:\n",
    "        X_list.append(batch.asdict()[\"features\"])  # ajustá según tus columnas\n",
    "        y_list.append(batch.asdict()[\"churn3\"])\n",
    "\n",
    "    X = torch.tensor(X_list, dtype=torch.float32)\n",
    "    y = torch.tensor(y_list, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X.shape[1], 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for Xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(Xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, loss={loss.item():.4f}\")\n",
    "\n",
    "    # guardamos el modelo en la ruta compartida\n",
    "    torch.save(model.state_dict(), f\"{checkpoint_dir}/model.pt\")\n",
    "\n",
    "# Lanzar entrenamiento distribuido\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "TorchDistributor(\n",
    "    num_processes=1,       # o el número de workers que tengas\n",
    "    local_mode=False,      # para usar todos los nodos del cluster\n",
    "    use_gpu=False          # <<< esto evita que intente inicializar CUDA\n",
    ").run(lambda: train_fn(\"/dbfs/mnt/jc-analytics-databricks-work/analytics/gpereyra/pp_nn/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1febe1-f9f5-4cd2-a9b4-95ad8a7b7ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45cbd78c-7635-4bd1-8e09-8052a409c9aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"teams.data_science.pp_churn_features_v2\")\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# --- Filtramos y preparamos el DataFrame ---\n",
    "\n",
    "# Eliminamos columnas que no son features\n",
    "cols_to_drop = ['judi', 'date', 'churn7', 'churn14','subs_lifetime_amt','subs_revenue_amt','daily_avg_boosters_used_ref','daily_avg_boosters_used_ref']\n",
    "df_clean = df.drop(*cols_to_drop)\n",
    "\n",
    "# Convertir la columna booleana churn3 a int\n",
    "df_clean = df_clean.withColumn(\"churn3\", F.col(\"churn3\").cast(\"int\"))\n",
    "\n",
    "df_pandas = df_clean.toPandas()\n",
    "\n",
    "# Convertimos la columna target \"churn3\" a numérica (0 y 1)\n",
    "df_clean['churn3'] = df_clean['churn3'].astype(int)\n",
    "\n",
    "# Aseguramos que no haya NaN en los features\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# --- Separar features (X) y target (y) ---\n",
    "\n",
    "# Todas las columnas excepto churn3 serán features\n",
    "X = df_clean.drop(columns=['churn3']).values\n",
    "y = df_clean['churn3'].values\n",
    "\n",
    "# --- Convertir a tensores de PyTorch ---\n",
    "# float32 es el tipo esperado por la mayoría de las redes neuronales\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "print(\"Shape de X:\", X_tensor.shape)\n",
    "print(\"Shape de y:\", y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65cfcb0e-f1e7-4ecc-ba6e-a256f906ff57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create EDA plots\n",
    "dist_plot = plot_feature_distributions(X, y)\n",
    "corr_plot = plot_correlation_heatmap(X, y)\n",
    "scatter_plot = plot_feature_target_relationships(X, y)\n",
    "corr_with_target = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "top_features = corr_with_target.head(4).index.tolist()\n",
    "pairwise_plot = plot_pairwise_relationships(X, y, top_features)\n",
    "outlier_plot = plot_outliers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228a7b40-51bb-41c5-9463-9e4bcf0e9fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader, scaler = prepare_dataloader(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [64, 32]\n",
    "dropout_rate = 0.1\n",
    "use_layer_norm = True\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Create the PyTorch Lightning model\n",
    "model = RegressionLightningModule(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    dropout_rate=dropout_rate,\n",
    "    use_layer_norm=use_layer_norm,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Define early stopping and model checkpoint callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='./checkpoints',\n",
    "    filename='pytorch-regression-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=5\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Test the model\n",
    "test_results = trainer.test(model, test_loader)\n",
    "\n",
    "# Make predictions on the test set for evaluation\n",
    "model.eval()\n",
    "test_preds = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        y_pred = model(x)\n",
    "        test_preds.extend(y_pred.numpy())\n",
    "        true_values.extend(y.numpy())\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(true_values, test_preds))\n",
    "mae = mean_absolute_error(true_values, test_preds)\n",
    "r2 = r2_score(true_values, test_preds)\n",
    "\n",
    "# Create residual plot\n",
    "residual_plot = plot_residuals(pd.Series(true_values), test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcfa1e20-e931-4d65-a723-8cdb3d400df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Log the model using MLflow\n",
    "\n",
    "When you log a model using MLflow on Databricks, important artifacts and metadata are captured. This ensures that your model is not only reproducible but also ready for deployment with all necessary dependencies and clear API contracts. For details on what is logged, see the [MLflow documentation](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.xgboost.html).  \n",
    "\n",
    "The code in the next cell starts an MLflow run using `with mlflow.start_run():`. This initializes the MLflow context manager for the run and encloses the run in a code block. When the code block ends, all logged metrics, parameters, and artifacts are saved, and the MLflow run is automatically terminated.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90176a73-23fa-4fa0-a74f-17e95362aab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model and training results with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    # Create MLflow client for batch logging\n",
    "    mlflow_client = MlflowClient()\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # Extract metrics\n",
    "    final_train_loss = trainer.callback_metrics.get(\"train_loss\").item() if \"train_loss\" in trainer.callback_metrics else None\n",
    "    final_val_loss = trainer.callback_metrics.get(\"val_loss\").item() if \"val_loss\" in trainer.callback_metrics else None\n",
    "    \n",
    "    # Extract parameters for logging\n",
    "    model_params = model.get_params()\n",
    "     \n",
    "    # Create a list to store all metrics for batch logging\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Add each metric to the list\n",
    "    if final_train_loss is not None:\n",
    "        all_metrics.append(Metric(key=\"train_loss\", value=final_train_loss, timestamp=0, step=0))\n",
    "    if final_val_loss is not None:\n",
    "        all_metrics.append(Metric(key=\"val_loss\", value=final_val_loss, timestamp=0, step=0))\n",
    "    \n",
    "    # Add test metrics\n",
    "    all_metrics.append(Metric(key=\"test_rmse\", value=rmse, timestamp=0, step=0))\n",
    "    all_metrics.append(Metric(key=\"test_mae\", value=mae, timestamp=0, step=0))\n",
    "    all_metrics.append(Metric(key=\"test_r2\", value=r2, timestamp=0, step=0))\n",
    "    \n",
    "    # Collect all parameters to log\n",
    "    # Note: The code uses log_params for model_params since there could be many parameters,\n",
    "    # but converts the individual param calls to batch\n",
    "    from mlflow.entities import Param\n",
    "    all_params = [\n",
    "        Param(key=\"batch_size\", value=str(batch_size)),\n",
    "        Param(key=\"early_stopping_patience\", value=str(early_stopping.patience)),\n",
    "        Param(key=\"max_epochs\", value=str(trainer.max_epochs)),\n",
    "        Param(key=\"actual_epochs\", value=str(trainer.current_epoch))\n",
    "    ]\n",
    "    \n",
    "    # Generate a model signature using the infer signature utility in MLflow\n",
    "    input_example = X_train.iloc[[0]].values.astype(np.float32)  # Ensure float32 type\n",
    "    input_example_scaled = scaler.transform(input_example).astype(np.float32)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Ensure tensor is float32\n",
    "        tensor_input = torch.tensor(input_example_scaled, dtype=torch.float32)\n",
    "        signature_preds = model(tensor_input)\n",
    "    \n",
    "    # Ensure prediction is also float32\n",
    "    signature = infer_signature(input_example, signature_preds.numpy().reshape(-1).astype(np.float32))\n",
    "    \n",
    "    # Log model parameters first (since these could be numerous)\n",
    "    mlflow.log_params(model_params)\n",
    "    \n",
    "    # Log all metrics and remaining parameters in a single batch operation\n",
    "    mlflow_client.log_batch(\n",
    "        run_id=run_id,\n",
    "        metrics=all_metrics,\n",
    "        params=all_params\n",
    "    )\n",
    "    \n",
    "    # Log the model to MLflow and register the model to Unity Catalog\n",
    "    model_info = mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"demo.pytorch_regression_model\",\n",
    "    )\n",
    "    \n",
    "    # Log feature analysis plots\n",
    "    mlflow.log_figure(dist_plot, \"feature_distributions.png\")\n",
    "    mlflow.log_figure(corr_plot, \"correlation_heatmap.png\")\n",
    "    mlflow.log_figure(scatter_plot, \"feature_target_relationships.png\")\n",
    "    mlflow.log_figure(pairwise_plot, \"pairwise_relationships.png\")\n",
    "    mlflow.log_figure(outlier_plot, \"outlier_detection.png\")\n",
    "    mlflow.log_figure(residual_plot, \"residual_plot.png\")\n",
    "    \n",
    "    # Run MLflow evaluation to generate additional metrics without having to implement them\n",
    "    evaluation_data = X_test.copy()\n",
    "    evaluation_data[\"label\"] = y_test\n",
    "    \n",
    "    # Skip mlflow.evaluate for now to avoid type mismatch issues\n",
    "    # Instead, log the metrics directly\n",
    "    print(f\"Model logged: {model_info.model_uri}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"Test MAE: {mae:.4f}\")\n",
    "    print(f\"Test R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09197f7e-7a72-45b4-b01c-3efde90e4536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Hyperparameter tuning\n",
    "\n",
    "This section shows how to automate hyperparameter tuning using [Optuna](https://optuna.org/) and [nested runs in MLflow](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs/). In this way you can explore a range of parameter configurations and capture all of the experimental details.\n",
    "\n",
    "The code in the next cell does the following:\n",
    "1. Uses the `create_regression_data` function defined previously to generate a synthetic regression dataset.\n",
    "2. Splits the dataset into separate training and test datasets, and saves a copy of the test dataset for evaluation.\n",
    "3. Creates an objective function for the hyperparameter tuning process. The objective function defines the search space for hyperparameters of the PyTorch model, such as the number of layers, hidden dimensions, dropout rate, learning rate, and regularization parameters. Optuna dynamically samples these values, ensuring that each trial tests a different combination of parameters.\n",
    "4. Initiates a nested MLflow run inside the objective function. This nested run automatically captures and logs all details specific to the current hyperparameter trial. By isolating each trial in its own nested run, you can keep a well-organized record of each configuration and its corresponding performance metrics. The nested run logs the following:   \n",
    "    - The specific hyperparameters used for that trial.\n",
    "    - The performance metric (in this case, validation loss) computed on the test set.\n",
    "    - The trained model instance is also stored as part of the trial’s metadata. This makes it easy to retrieve the best-performing model later.  \n",
    "\n",
    "    The code does not record each model to MLflow. While doing hyperparameter tuning, each iteration is not guaranteed to be particularly good, so there is no reason to record the model artifact for each one.\n",
    "\n",
    "5. Create a parent MLflow run. This run initiates an Optuna study designed to identify the optimal set of hyperparameters (the set that produces the lowest validation loss). Optuna runs a series of trials where each trial uses a unique combination of hyperparameters. During each trial, the nested MLflow run captures all the experiment details, so you can later track and compare the performance of each model configuration.\n",
    "6. The study identifies the best trial based on the lowest validation loss. The code extracts the best model and the optimal parameter values. The code uses `infer_signature` to save a model signature, which specifies the expected input and output schemas and is important for consistent deployment and integration with systems like Unity Catalog. Finally, the best model is logged and registered to Unity Catalog. Additional artifacts such as EDA plots and feature importance charts are also recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d1ba3b-ac26-4b0e-a1e6-9945450e6528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom pruning callback as a fallback\n",
    "class PyTorchLightningPruningCallback(pl.Callback):\n",
    "    \"\"\"PyTorch Lightning callback to prune unpromising trials.\n",
    "    \n",
    "    This is a simplified version for use when the optuna-integration package isn't available.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trial, monitor):\n",
    "        super().__init__()\n",
    "        self._trial = trial\n",
    "        self.monitor = monitor\n",
    "        \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        # Report the validation metric to Optuna\n",
    "        metrics = trainer.callback_metrics\n",
    "        current_score = metrics.get(self.monitor)\n",
    "        \n",
    "        if current_score is not None:\n",
    "            self._trial.report(current_score.item(), trainer.current_epoch)\n",
    "            \n",
    "            # If trial should be pruned based on current value,\n",
    "            # stop the training\n",
    "            if self._trial.should_prune():\n",
    "                message = \"Trial was pruned at epoch {}.\".format(trainer.current_epoch)\n",
    "                raise optuna.TrialPruned(message)\n",
    "\n",
    "# Generate a larger dataset for hyperparameter tuning\n",
    "n_samples = 2000\n",
    "n_features = 10\n",
    "\n",
    "X, y = create_regression_data(n_samples=n_samples, n_features=n_features, nonlinear=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare the evaluation data\n",
    "evaluation_data = X_test.copy()\n",
    "evaluation_data[\"label\"] = y_test\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader, scaler = prepare_dataloader(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function to minimize validation loss.\"\"\"\n",
    "    \n",
    "    # Define the hyperparameter search space\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    \n",
    "    # Create hidden dimensions based on number of layers\n",
    "    hidden_dims = []\n",
    "    for i in range(n_layers):\n",
    "        hidden_dims.append(trial.suggest_int(f\"hidden_dim_{i}\", 16, 128))\n",
    "    \n",
    "    # Other hyperparameters\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    use_layer_norm = trial.suggest_categorical(\"use_layer_norm\", [True, False])\n",
    "    \n",
    "    # Start a nested MLflow run for this trial\n",
    "    with mlflow.start_run(nested=True) as child_run:\n",
    "        # Create MLflow client for batch logging\n",
    "        mlflow_client = MlflowClient()\n",
    "        run_id = child_run.info.run_id\n",
    "        \n",
    "        # Prepare parameters for batch logging\n",
    "        params_list = []\n",
    "        param_dict = {\n",
    "            \"n_layers\": n_layers,\n",
    "            \"hidden_dims\": str(hidden_dims),  # Convert list to string\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"use_layer_norm\": use_layer_norm,\n",
    "            \"batch_size\": batch_size\n",
    "        }\n",
    "        \n",
    "        # Convert parameters to Param objects\n",
    "        for key, value in param_dict.items():\n",
    "            params_list.append(Param(key, str(value)))\n",
    "        \n",
    "        # Create the model with these hyperparameters\n",
    "        model = RegressionLightningModule(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_layer_norm=use_layer_norm,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        pruning_callback = PyTorchLightningPruningCallback(\n",
    "            trial, monitor=\"val_loss\"\n",
    "        )\n",
    "        \n",
    "        # Define trainer with early stopping and pruning\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=50,\n",
    "            callbacks=[early_stopping, pruning_callback],\n",
    "            enable_progress_bar=False,\n",
    "            log_every_n_steps=10\n",
    "        )\n",
    "        \n",
    "        # Train and validate the model\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "        # Get the best validation loss\n",
    "        best_val_loss = trainer.callback_metrics.get(\"val_loss\").item()\n",
    "        val_rmse = np.sqrt(best_val_loss)\n",
    "        \n",
    "        # Prepare metrics for batch logging\n",
    "        current_time = int(time.time() * 1000)  # Current time in milliseconds\n",
    "        metrics_list = [\n",
    "            Metric(\"val_loss\", best_val_loss, current_time, 0),\n",
    "            Metric(\"val_rmse\", val_rmse, current_time, 0)\n",
    "        ]\n",
    "        \n",
    "        # Use log_batch through the client for efficient logging\n",
    "        mlflow_client.log_batch(run_id, metrics=metrics_list, params=params_list)\n",
    "        \n",
    "    # Store the model in the trial's user attributes\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "    \n",
    "    # Return the value to minimize (validation loss)\n",
    "    return best_val_loss\n",
    "\n",
    "best_model_version = None\n",
    "# The parent run stores the best iteration from the hyperparameter tuning execution\n",
    "with mlflow.start_run() as run:\n",
    "    # Create MLflow client for batch logging\n",
    "    mlflow_client = MlflowClient()\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_model = best_trial.user_attrs[\"model\"]\n",
    "    \n",
    "    # Test the best model\n",
    "    trainer = pl.Trainer(\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=5\n",
    "    )\n",
    "    test_results = trainer.test(best_model, test_loader)\n",
    "    \n",
    "    # Make predictions on the test set for evaluation\n",
    "    best_model.eval()\n",
    "    test_preds = []\n",
    "    true_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            y_pred = best_model(x)\n",
    "            test_preds.extend(y_pred.numpy())\n",
    "            true_values.extend(y.numpy())\n",
    "    \n",
    "    test_preds = np.array(test_preds)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, test_preds))\n",
    "    mae = mean_absolute_error(true_values, test_preds)\n",
    "    r2 = r2_score(true_values, test_preds)\n",
    "    \n",
    "    # Prepare parameters for batch logging\n",
    "    best_params_list = []\n",
    "    for key, value in best_trial.params.items():\n",
    "        best_params_list.append(Param(f\"best_{key}\", str(value)))\n",
    "    \n",
    "    # Prepare metrics for batch logging\n",
    "    current_time = int(time.time() * 1000)  # Current time in milliseconds\n",
    "    metrics_list = [\n",
    "        Metric(\"best_val_loss\", best_trial.value, current_time, 0),\n",
    "        Metric(\"test_rmse\", rmse, current_time, 0),\n",
    "        Metric(\"test_mae\", mae, current_time, 0),\n",
    "        Metric(\"test_r2\", r2, current_time, 0)\n",
    "    ]\n",
    "    \n",
    "    # Log metrics and parameters in a single batch call\n",
    "    mlflow_client.log_batch(run_id, metrics=metrics_list, params=best_params_list)\n",
    "\n",
    "    # Generate model signature - ensure consistent float32 types\n",
    "    input_example = X_train.iloc[[0]].values.astype(np.float32)\n",
    "    input_example_scaled = scaler.transform(input_example).astype(np.float32)\n",
    "    \n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor_input = torch.tensor(input_example_scaled, dtype=torch.float32)\n",
    "        signature_preds = best_model(tensor_input)\n",
    "    \n",
    "    signature = infer_signature(input_example, signature_preds.numpy().reshape(-1).astype(np.float32))\n",
    "\n",
    "    # Log and register the PyTorch model\n",
    "    model_info = mlflow.pytorch.log_model(\n",
    "        best_model,\n",
    "        artifact_path=\"model\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"demo.pytorch_regression_optimized\",\n",
    "    )\n",
    "    \n",
    "    # Create residual plot\n",
    "    residual_plot = plot_residuals(pd.Series(true_values), test_preds)\n",
    "    \n",
    "    # Log figures (no batch equivalent for figures)\n",
    "    mlflow.log_figure(dist_plot, \"feature_distributions.png\")\n",
    "    mlflow.log_figure(corr_plot, \"correlation_heatmap.png\")\n",
    "    mlflow.log_figure(scatter_plot, \"feature_target_relationships.png\")\n",
    "    mlflow.log_figure(pairwise_plot, \"pairwise_relationships.png\")\n",
    "    mlflow.log_figure(outlier_plot, \"outlier_detection.png\")\n",
    "    mlflow.log_figure(residual_plot, \"residual_plot.png\")\n",
    "\n",
    "    # Skip mlflow.evaluate for now to avoid type mismatch issues\n",
    "    # Instead, log the metrics directly\n",
    "    print(f\"Best model logged: {model_info.model_uri}\")\n",
    "    print(f\"Best parameters: {best_trial.params}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"Test MAE: {mae:.4f}\")\n",
    "    print(f\"Test R²: {r2:.4f}\")\n",
    "\n",
    "    best_model_version = model_info.registered_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7556300-f197-41f8-9c77-d553e492d180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "# Initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set a human-readable alias for the best model version\n",
    "# This makes it easier to reference specific model versions programmatically\n",
    "client.set_registered_model_alias(\"demo.pytorch_regression_optimized\", \"best\", int(best_model_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9579ed28-2d13-4e1a-97c6-3cacce5f2f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Pre-deployment validation\n",
    "MLflow provides the `mlflow.models.predict` utility to simulate a production-like environment and validate that your model is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5aafba8-7f07-4866-ad28-9b7509a19e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reference the model by its alias\n",
    "model_uri = \"models:/demo.pytorch_regression_optimized@best\"\n",
    "\n",
    "# Validate the model's deployment readiness\n",
    "mlflow.models.predict(model_uri=model_uri, input_data=X_test, env_manager=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5107b962-a9ed-489d-8b69-1b6a3c4a19e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Load the registered model and make predictions\n",
    "\n",
    "\n",
    "\n",
    "The code in this section shows how to load the registered model from MLflow and use it to make predictions locally. This is useful for testing or for batch inference scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3dbe796-7c56-4438-abe3-b06c5dd1da69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the data type of X_test to float32\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Load the model using the pyfunc interface (recommended for deployment)\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri)\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "print(f\"Shape of predictions: {predictions.shape}\")\n",
    "print(f\"First 5 predictions: {predictions[:5]}\")\n",
    "print(f\"First 5 actual values: {y_test.values[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3ffe416-4a0a-4645-8168-0565c378de71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Batch prediction using Spark UDF in MLflow\n",
    "\n",
    "For large-scale predictions, you can convert the model to a Spark UDF and apply it to a Spark DataFrame, enabling distributed inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547a3124-a230-49c1-a3f7-71bbde5b9fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "\n",
    "# Convert the test data to a Spark DataFrame\n",
    "X_spark = spark.createDataFrame(X_test)\n",
    "\n",
    "# Create an array of all feature columns\n",
    "# This step is necessary because:\n",
    "# 1. The PyTorch model expects an input tensor with shape [-1, 13]\n",
    "# 2. The model_udf needs to receive each row as a single array of 13 values\n",
    "# 3. Without this array transformation, 13 separate columns would be passed to the model\n",
    "#    which wouldn't match the expected tensor structure\n",
    "X_spark_with_array = X_spark.withColumn(\n",
    "    \"features_array\", \n",
    "    array(*[col(c) for c in X_spark.columns])\n",
    ")\n",
    "\n",
    "# Create a Spark UDF from the registered model\n",
    "model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri)\n",
    "\n",
    "# Apply MLflow UDF to the array column\n",
    "# Pass the single array column to the model, which matches the expected tensor format\n",
    "X_spark_with_predictions = X_spark_with_array.withColumn(\n",
    "    \"prediction\", \n",
    "    model_udf(\"features_array\")\n",
    ")\n",
    "\n",
    "display(X_spark_with_predictions.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "mlflow-deep-learning-e2e",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

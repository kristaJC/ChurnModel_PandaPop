{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99588b18-d4f4-4e5b-913e-c76f2ac1c2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow xgboost\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8dfb97-a7b6-4ba6-a5b8-575ccdcb8587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "import mlflow.spark\n",
    "from mlflow.artifacts import download_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb74358-93d4-4617-823d-26c86da509c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.config import *\n",
    "from src.sampling import *\n",
    "from src.tracking_helpers import *\n",
    "from src.tracking import *\n",
    "from src.tuning import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15a0f9e-5b1a-4a39-a08a-f39f1c0b8e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LABEL_COL = \"churn7\" #Loaded from config\n",
    "\n",
    "DATE_FILTER = \"2025-10-26\"\n",
    "DATE_INTERVAL = 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f34f45-7b37-4ac4-91df-b0cc3f0822fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\").withColumn('market_name', col('market')).drop('market').withColumnRenamed('market_name','market')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766a5ca1-b759-48fe-825b-e11e250513e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, NumericType, BooleanType\n",
    "\n",
    "string_features = []\n",
    "numerical_features = []\n",
    "churn_labels = []\n",
    "\n",
    "drop_cols = ['judi','date','ts_last_updated','processed_date','churn3','churn5','churn7','churn14']\n",
    "\n",
    "\n",
    "for field in df.schema.fields:\n",
    "    if isinstance(field.dataType, StringType) and field.name not in drop_cols:\n",
    "        string_features.append(field.name)\n",
    "    elif isinstance(field.dataType, NumericType) and field.name not in drop_cols:\n",
    "        numerical_features.append(field.name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498d1a5c-4278-4117-b631-ded799abdb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### This is the churn feature SET!!\n",
    "churn_features = df.withColumn('label', when(col(LABEL_COL)==True,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f00e568-508e-440b-a381-693996481a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "split_payers = True\n",
    "upsample=True\n",
    "undersample=True\n",
    "\n",
    "if split_payers:\n",
    "    payers = ['P','S']\n",
    "    non_payers= ['N']\n",
    "    \n",
    "    unioned_sets = get_stratified_sets(churn_features, split=None, undersample=undersample, upsample=upsample)\n",
    "\n",
    "    payers_sets = get_stratified_sets(churn_features.filter(col('payer_type_cd').isin(payers)),split='payers',undersample=undersample, upsample=upsample)\n",
    "\n",
    "    nonpayers_sets = get_stratified_sets(churn_features.filter(col('payer_type_cd').isin(non_payers)), split='nonpayers',undersample=undersample, upsample=upsample)\n",
    "\n",
    "    all_sets = unioned_sets + payers_sets + nonpayers_sets\n",
    "else:\n",
    "    all_sets = get_stratified_sets(churn_features, split=None, undersample=undersample, upsample=upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd0e948-6ec6-4fd7-8eaa-22344d933c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fceba73d-e571-45e4-8c7c-5954797f7336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unecessary because we only have 1 worker?\n",
    "\n",
    "for val in all_sets:\n",
    "    repartitioned, safe_workers = get_safe_works_repartition(val['dataset'])\n",
    "    val['dataset']=repartitioned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afec6ca2-ba0b-4e10-9d91-e8c3f947150b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For XGBoost we don't need to standarize any features\n",
    "indexers = [StringIndexer(inputCol=x, \n",
    "                          outputCol=x+\"_index\", \n",
    "                          handleInvalid=\"keep\") for x in string_features]\n",
    "indexed_cols = [ x+\"_index\" for x in string_features]\n",
    "\n",
    "inputs = numerical_features + indexed_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=inputs, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "\n",
    "# Now add the xgb model to the pipeline\n",
    "#eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "eval_metrics = [\"aucpr\"]\n",
    "\n",
    "\n",
    "safe_workers=1\n",
    "\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col = \"features\",\n",
    "  label_col = \"label\",\n",
    "  num_workers = safe_workers,\n",
    "  eval_metric = eval_metrics,\n",
    ")\n",
    "\n",
    "# Set the pipeline stages for the entire process\n",
    "pipeline = Pipeline().setStages(indexers+[vec_assembler]+ [xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03254e5d-f737-4388-b107-27fd946a1356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spec = {\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=2,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41021f21-aa61-4370-ac01-7171c9d79264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the MLflow logging level to INFO\n",
    "logger = logging.getLogger(\"mlflow\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79035bd-2615-4f29-a9fd-a6ebafb866db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plus other useful information ... can actually do this elsewhere or whatever.. but this works for now\n",
    "extra_tags = { \n",
    "                'label': LABEL_COL,\n",
    "                'safe_workers':safe_workers, \n",
    "                'date_filter':DATE_FILTER, \n",
    "                'date_interval':DATE_INTERVAL, \n",
    "                'source_table_name':FEATURES_TABLE_NAME\n",
    "            }\n",
    "\n",
    "for val in all_sets:\n",
    "    val['extra_tags']= {**extra_tags, **val['dataset_info']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98e2442-a5b5-43fc-97dc-5928ae326f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3710ac75-a262-4db3-8ce7-60bec60cbc94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#all_sets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ebeb23-2c9a-4e62-b0a4-0ef1ef459388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_list = []\n",
    "best_estimators = []\n",
    "\n",
    "\n",
    "results, best_estimator = run_spark_ml_training(estimator = cv_xgb, \n",
    "                        train_df = all_sets[6][\"dataset\"], \n",
    "                        test_df = all_sets[6][\"relevant_test_set\"], \n",
    "                        val_df = all_sets[6][\"relevant_val_set\"], \n",
    "                        extra_tags = all_sets[6][\"extra_tags\"])\n",
    "results_list.append(results)\n",
    "best_estimators.append(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de76b8f-a8fb-4c7f-8faf-d5a18991b306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "results_list = []\n",
    "best_estimators = []\n",
    "#need index 6,7,8 still\n",
    "\n",
    "\n",
    "for i in all_sets[6:]:\n",
    "    #print(f\"Starting run on set {ix+7} out of {len(all_sets)}\")\n",
    "    #print(f\"With dataset and run info:\", i[\"extra_tags\"])\n",
    "\n",
    "    ### Strat train up: \n",
    "    results, best_estimator = run_spark_ml_training(estimator = cv_xgb, \n",
    "                        train_df = i[\"dataset\"], \n",
    "                        test_df = i[\"relevant_test_set\"], \n",
    "                        val_df = i[\"relevant_val_set\"], \n",
    "                        extra_tags = i[\"extra_tags\"])\n",
    "    results_list.append(results)\n",
    "    best_estimators.append(best_estimator)\n",
    "\n",
    "mlflow.end_run()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca5d6a7-2c1c-4aca-a0ae-0c923aa275fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "12-4-25- test Sampling method",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

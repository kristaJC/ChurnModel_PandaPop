{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99588b18-d4f4-4e5b-913e-c76f2ac1c2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow xgboost\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8dfb97-a7b6-4ba6-a5b8-575ccdcb8587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "import mlflow.spark\n",
    "from mlflow.artifacts import download_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb74358-93d4-4617-823d-26c86da509c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.config import *\n",
    "from src.sampling import *\n",
    "from src.tracking import *\n",
    "from src.tuning import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15a0f9e-5b1a-4a39-a08a-f39f1c0b8e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn7\"\n",
    "DATE_FILTER = \"2025-10-26\"\n",
    "DATE_INTERVAL = 30\n",
    "\n",
    "# Payer split: None --> no split, \"0\" --> non-payer, \"1,2\" --> payer\n",
    "#payer_split = \"1,2\"\n",
    "\n",
    "\n",
    "#payer_split = None\n",
    "#payer_split = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a801104-8be3-4b60-9959-412e0e9b9cb8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"data_type\":130},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763668760345}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(f\"\"\" describe table {FEATURES_TABLE_NAME}\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f34f45-7b37-4ac4-91df-b0cc3f0822fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\").withColumn('market_name', col('market')).drop('market').withColumnRenamed('market_name','market')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766a5ca1-b759-48fe-825b-e11e250513e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, NumericType, BooleanType\n",
    "\n",
    "string_features = []\n",
    "numerical_features = []\n",
    "churn_labels = []\n",
    "\n",
    "drop_cols = ['judi','date','ts_last_updated','processed_date','churn3','churn5','churn7','churn14']\n",
    "\n",
    "\n",
    "for field in df.schema.fields:\n",
    "    if isinstance(field.dataType, StringType) and field.name not in drop_cols:\n",
    "        string_features.append(field.name)\n",
    "    elif isinstance(field.dataType, NumericType) and field.name not in drop_cols:\n",
    "        numerical_features.append(field.name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498d1a5c-4278-4117-b631-ded799abdb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### This is the churn feature SET!!\n",
    "churn_features = df.withColumn('label', when(col(LABEL_COL)==True,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c4fdb9-7723-4f74-8e72-1abf72cc6937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#churn_features.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f00e568-508e-440b-a381-693996481a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "split_payers = True\n",
    "upsample=True\n",
    "undersample=True\n",
    "\n",
    "if split_payers:\n",
    "    payers = ['P','S']\n",
    "    non_payers= ['N']\n",
    "    \n",
    "    unioned_sets = get_stratified_sets(churn_features, split=None, undersample=undersample, upsample=upsample)\n",
    "\n",
    "    payers_sets = get_stratified_sets(churn_features.filter(col('payer_type_cd').isin(payers)),split='payers',undersample=undersample, upsample=upsample)\n",
    "\n",
    "    nonpayers_sets = get_stratified_sets(churn_features.filter(col('payer_type_cd').isin(non_payers)), split='nonpayers',undersample=undersample, upsample=upsample)\n",
    "\n",
    "    all_sets = unioned_sets + payers_sets + nonpayers_sets\n",
    "else:\n",
    "    all_sets = get_stratified_sets(churn_features, split=None, undersample=undersample, upsample=upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5c37a823-c247-4a53-802a-563e0eb15f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "split_payers = True\n",
    "\n",
    "\n",
    "if split_payers:\n",
    "    payers = ['P','S']\n",
    "    non_payers= ['N']\n",
    "\n",
    "    # Split nonpayer and payer separately\n",
    "    strat_train_payer, strat_val_payer, strat_test_payer = stratified_sampling(churn_features.filter(col('payer_type_cd').isin(payers)), P_TEST=0.2, P_VAL=0.2)\n",
    "\n",
    "    strat_train_nonpayer, strat_val_nonpayer, strat_test_nonpayer = stratified_sampling(churn_features.filter(col('payer_type_cd').isin(non_payers)), P_TEST=0.2, P_VAL=0.2)\n",
    "\n",
    "\n",
    "    strat_train, strat_val, strat_test = stratified_sampling(churn_features, P_TEST=0.2, P_VAL=0.2) \n",
    "    \n",
    "\n",
    "### This is just the stratified sampling, without a payer/nonpayer split\n",
    "else:\n",
    "    strat_train, strat_val, strat_test = stratified_sampling(churn_features, P_TEST=0.2, P_VAL=0.2)    \n",
    "\n",
    "#Base info \n",
    "stratified_info =  {\n",
    "                        'sampling':'stratified', \n",
    "                        'split':None,\n",
    "                        'P_TEST':0.2,\n",
    "                        'P_VAL':0.2,\n",
    "                        'P_TRAIN':0.6,\n",
    "                        'strategy':None\n",
    "                    }\n",
    "\n",
    "all_sets = [{\n",
    "                'dataset': strat_train,\n",
    "                'dataset_info': {**stratified_info, 'type':'training'},\n",
    "                'relavent_test_set':strat_test,\n",
    "                'relavent_val_set':strat_val,\n",
    "            },      \n",
    "            #{\n",
    "            #    'dataset': strat_val, \n",
    "            #    'dataset_info':{**stratified_info, 'type':'validation'}\n",
    "            #}, \n",
    "            #{\n",
    "            #    'dataset': strat_test,\n",
    "            #    'dataset_info': {**stratified_info, 'type':'testing'}\n",
    "            #}\n",
    "        ]\n",
    "\n",
    "#stratified_sets= all_sets\n",
    "\n",
    "split_info = {\n",
    "    'sampling':'stratified', \n",
    "    'P_TEST':0.2,\n",
    "    'P_VAL':0.2,\n",
    "    'P_TRAIN':0.6,\n",
    "},\n",
    "\n",
    "if split_payers:\n",
    "\n",
    "    # non_payer\n",
    "    non_payer_slug = {\n",
    "                'dataset':strat_train_nonpayer, \n",
    "                'dataset_info':{\n",
    "                    **stratified_info,\n",
    "                    'split':'nonpayer', \n",
    "                    'type':'training'\n",
    "                    },\n",
    "                'relavent_test_set': strat_test_nonpayer,\n",
    "                'relavent_val_set': strat_val_nonpayer,\n",
    "            }\n",
    "    all_sets.append(non_payer_slug)\n",
    "    \n",
    "    #payers\n",
    "    payers_slug = {\n",
    "                'dataset':strat_train_payer,\n",
    "                'dataset_info':{\n",
    "                    **stratified_info, \n",
    "                    'split':'payer', \n",
    "                    'type':'training'\n",
    "                    },\n",
    "                'relavent_test_set': strat_test_payer,\n",
    "                'relavent_val_set': strat_val_payer,\n",
    "            }\n",
    "    all_sets.append(payers_slug)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a66ca9f-f1e2-4fe5-8868-4d1c9f2c0093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Upsampling\n",
    "upsample=True\n",
    "undersample = True\n",
    "\n",
    "#upsample=\"True\"\n",
    "#undersample = \"True\"\n",
    "\n",
    "# Now deal with player splits\n",
    "if split_payers: #if split_payers==\"True\"\n",
    "    if upsample==True: #if upsample==\"True\"\n",
    "        strat_train_up_payer, train_up_payer_info = upsample_minority(strat_train_payer,split='payer')\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up_payer, \n",
    "                'dataset_info':train_up_payer_info,\n",
    "                'relevant_test_set': strat_test_payer,\n",
    "                'relevant_val_set': strat_val_payer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        strat_train_up_nonpayer, train_up_nonpayer_info = upsample_minority(strat_train_nonpayer,split='nonpayer')\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up_nonpayer, \n",
    "                'dataset_info':train_up_nonpayer_info,\n",
    "                'relevant_test_set': strat_test_nonpayer,\n",
    "                'relevant_val_set': strat_val_nonpayer,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Union for upsampling - remove splits\n",
    "        strat_train_up = strat_train_up_payer.union(strat_train_up_nonpayer)\n",
    "        train_up_info = {'majority_label': 0,\n",
    "                            'minority_label': 1,\n",
    "                            'strategy':'sampling',\n",
    "                            'sampling': 'upsample',\n",
    "                            'split':None,\n",
    "                            'type':\"training\"}\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up, \n",
    "                'dataset_info':train_up_info,\n",
    "                'relevant_test_set': strat_test,\n",
    "                'relevant_val_set': strat_val,\n",
    "\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    if undersample==True: #if undersample==\"True\":\n",
    "        strat_train_under_payer, train_under_payer_info = undersample_majority(strat_train_payer,split='payer') \n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under_payer, \n",
    "                'dataset_info':train_under_payer_info,\n",
    "                'relevant_test_set':strat_test_payer,\n",
    "                'relevant_val_set':strat_val_payer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        strat_train_under_nonpayer, train_under_nonpayer_info = undersample_majority(strat_train_nonpayer,split='nonpayer')\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under_nonpayer, \n",
    "                'dataset_info':train_under_nonpayer_info,\n",
    "                'relevant_test_set':strat_test_nonpayer,\n",
    "                'relevant_val_set':strat_val_nonpayer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Union for undersampling - remove split\n",
    "        strat_train_under = strat_train_under_payer.union(strat_train_under_nonpayer)\n",
    "        train_under_info = {\n",
    "                                'majority_label': 0,\n",
    "                                'minority_label': 1,\n",
    "                                'strategy':'sampling',\n",
    "                                'sampling':'undersample',\n",
    "                                'split':None,\n",
    "                                'type':\"training\",\n",
    "                            }\n",
    "        \n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under, \n",
    "                'dataset_info':train_under_info,\n",
    "                'relevant_test_set':strat_test,\n",
    "                'relevant_val_set':strat_val,\n",
    "            })\n",
    "\n",
    "else:\n",
    "    if upsample==True: # if upsample==\"True\"\n",
    "        ## Upsampling\n",
    "        strat_train_up, train_up_info = upsample_minority(strat_train)\n",
    "        train_up_info['type']='training'\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up, \n",
    "                'dataset_info':train_up_info,\n",
    "                'relevant_test_set':strat_test,\n",
    "                'relevant_val_set':strat_val,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if undersample==True: # if undersample==\"True\"\n",
    "        ## Undersampling\n",
    "        strat_train_under, train_under_info = undersample_majority(strat_train)\n",
    "        train_under_info['type'] = 'training'\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under, \n",
    "                'dataset_info':train_under_info,\n",
    "                'relevant_test_set':strat_test,\n",
    "                'relevant_val_set':strat_val\n",
    "            }\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd0e948-6ec6-4fd7-8eaa-22344d933c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fceba73d-e571-45e4-8c7c-5954797f7336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unecessary because we only have 1 worker?\n",
    "\n",
    "for val in all_sets:\n",
    "    repartitioned, safe_workers = get_safe_works_repartition(val['dataset'])\n",
    "    val['dataset']=repartitioned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afec6ca2-ba0b-4e10-9d91-e8c3f947150b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For XGBoost we don't need to standarize any features\n",
    "indexers = [StringIndexer(inputCol=x, \n",
    "                          outputCol=x+\"_index\", \n",
    "                          handleInvalid=\"keep\") for x in string_features]\n",
    "indexed_cols = [ x+\"_index\" for x in string_features]\n",
    "\n",
    "inputs = numerical_features + indexed_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=inputs, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "\n",
    "# Now add the xgb model to the pipeline\n",
    "#eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "eval_metrics = [\"aucpr\"]\n",
    "\n",
    "\n",
    "safe_workers=1\n",
    "\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col = \"features\",\n",
    "  label_col = \"label\",\n",
    "  num_workers = safe_workers,\n",
    "  eval_metric = eval_metrics,\n",
    ")\n",
    "\n",
    "# Set the pipeline stages for the entire process\n",
    "pipeline = Pipeline().setStages(indexers+[vec_assembler]+ [xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f74c3682-cb2d-4e8c-8493-1ccdabc86f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "spec = {\n",
    "    # \"n_estimators\": (\"int_uniform\", 50, 1000),\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "    #\"gamma\": (\"uniform\", 0.0, 0.2),\n",
    "    #\"learning_rate\": (\"uniform\", 0.01,0.5),\n",
    "    # \"subsample\": (\"uniform\", 0.7, 0.9),\n",
    "    #\"colsample_bytree\": (\"uniform\", 0.7, 0.9),\n",
    "    # \"min_child_weight\": (\"int_uniform\", 1, 5),\n",
    "    #\"reg_alpha\": (\"uniform\", 0.0, 0.1),\n",
    "    #\"reg_lambda\": (\"int_uniform\", 1, 10),\n",
    "    #\"colsample_bylevel\": (\"uniform\", 0, 0.6),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03254e5d-f737-4388-b107-27fd946a1356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spec = {\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41021f21-aa61-4370-ac01-7171c9d79264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the MLflow logging level to INFO\n",
    "logger = logging.getLogger(\"mlflow\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79035bd-2615-4f29-a9fd-a6ebafb866db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plus other useful information ... can actually do this elsewhere or whatever.. but this works for now\n",
    "extra_tags = { \n",
    "                'label': LABEL_COL,\n",
    "                'safe_workers':safe_workers, \n",
    "                'date_filter':DATE_FILTER, \n",
    "                'date_interval':DATE_INTERVAL, \n",
    "                'source_table_name':FEATURES_TABLE_NAME\n",
    "            }\n",
    "\n",
    "for val in all_sets:\n",
    "    val['extra_tags']= {**extra_tags, **val['dataset_info']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98e2442-a5b5-43fc-97dc-5928ae326f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "997eef9d-3c35-4931-9979-381d646ddacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Add run_id to be like f\"XGB_{days_back}_{split}_{sampling}_{churn_label}\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00060e12-d1e8-4f00-a31e-89648e831b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#temp = all_sets[0]['dataset'].select('total_rounds_l7d','label').limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdea76d5-7e0c-4908-b1c9-719a565e15cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "temp = pd.DataFrame([[x,random.random()] for x in range(20)],columns = ['features','feature_importances']).sort_values('feature_importances',ascending=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f01f644-37ff-4ae6-922c-10139174ad0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def test_function(df):\n",
    "\n",
    "    features = pd.Series(df['feature_importances'], index=df['features'])\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    features.plot.barh(ax=ax)\n",
    "    ax.set_title(\"XGBoost Feature Importances\")\n",
    "    ax.set_ylabel(\"Feature Name\")\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig_importances = test_function(temp)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84aa10d1-dabd-46b9-8bbe-d208fe6a060d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"fig = plt.figure()\n",
    "plt.barh(temp['features_importances'],temp['feature_importances'])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"XGBoost Feature Importances\")\n",
    "#plt.tight_layout()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e2ba06-cfe0-44a1-ad3a-83b182ded765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(temp['feature_importances'], index=temp['features'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3710ac75-a262-4db3-8ce7-60bec60cbc94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_sets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de76b8f-a8fb-4c7f-8faf-d5a18991b306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_list = []\n",
    "best_estimators = []\n",
    "#need index 6,7,8 still\n",
    "\n",
    "\n",
    "for i in all_sets[6:]:\n",
    "    #print(f\"Starting run on set {ix+7} out of {len(all_sets)}\")\n",
    "    #print(f\"With dataset and run info:\", i[\"extra_tags\"])\n",
    "\n",
    "    ### Strat train up: \n",
    "    results, best_estimator = run_spark_ml_training(estimator = cv_xgb, \n",
    "                        train_df = i[\"dataset\"], \n",
    "                        test_df = i[\"relevant_test_set\"], \n",
    "                        val_df = i[\"relevant_val_set\"], \n",
    "                        extra_tags = i[\"extra_tags\"])\n",
    "    results_list.append(results)\n",
    "    best_estimators.append(best_estimator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca5d6a7-2c1c-4aca-a0ae-0c923aa275fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18962c6b-d0b0-4642-b2fd-c0f3938ca1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###RNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/762753878692720/76b0863a5e5947378a19c9fec1384fcd/artifacts/models/best_model/sparkml, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10979d82-647e-4880-989c-9e6f36797ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.spark.load_model("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0070f119-ce1b-444f-b2c2-1f467bb067f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### \n",
    "\n",
    "# import mlflow\n",
    "\n",
    "# model_uri = 'runs:/77facbd0a5f044ce807b92e5a9df96e3/best_model/spark-model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468bf35b-090d-4503-924e-1192bd622fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run_spark_ml_training(cv_xgb, strat_train_up, test_df = strat_test, val_df = strat_val, extra_tags = train_up_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ca6138-19ab-4feb-a6d7-a5f92ce78ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "# Modify and test the tracking functions (log all to the mlflow experiment, vs. the notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "12-2-25- test Sampling method",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdca6ef-7d36-4766-8233-ce038931c504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow xgboost\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c9c289-852d-4992-ab5b-ce691f8eb6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "import mlflow.spark\n",
    "from mlflow.artifacts import download_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8bdcf9-aa2f-4543-a603-6f171c63ec5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.config import *\n",
    "from src.sampling import *\n",
    "from src.tracking import *\n",
    "# from src.tuning import * \n",
    "#from Deprecated.deprecated_tracking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b455a9-de4b-4ebc-90a4-1f84a7758beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AnÃ¡lisis of proportion of churn for different targets\n",
    "\n",
    "FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features_v4_cluster\"\n",
    "\n",
    "display(spark.sql(f\"\"\"select count(case when churn3 = 1 then 1 end) / count(case when churn3 is not null then 1 end) as churn3_rt,\n",
    "       count(case when churn5 = 1 then 1 end) / count(case when churn5 is not null then 1 end) as churn5_rt,\n",
    "       count(case when churn7 = 1 then 1 end) / count(case when churn7 is not null then 1 end) as churn7_rt\n",
    "        from {FEATURES_TABLE_NAME}\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22db0c6-79bf-431e-9036-75491b97620f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn7\"\n",
    "#FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features_v3_small\"\n",
    "\n",
    "DATE_FILTER = \"2025-10-26\"\n",
    "DATE_INTERVAL = 30\n",
    "\n",
    "# Payer split: None --> no split, \"0\" --> non-payer, \"1,2\" --> payer\n",
    "payer_split = \"1,2\"\n",
    "\n",
    "# Cluster vars: None --> no extra cluster vars\n",
    "cluster_vars = None\n",
    "# cluster_vars = [\"cluster_0\",\"cluster_1\", \"cluster_2\", \"cluster_3\", \"cluster_4\", \"cluster_5\", \"cluster_6\", \"cluster_7\", \"cluster_8\", \"cluster_9\", \"cluster_10\", \"cluster_11\", \"cluster_12\", \"cluster_13\", \"cluster_14\", \"cluster_15\", \"cluster_16\", \"cluster_17\", \"cluster_18\", \"cluster_19\", \"cluster_20\", \"cluster_21\", \"cluster_22\", \"cluster_23\", \"cluster_24\", \"cluster_25\", \"cluster_26\", \"cluster_27\", \"cluster_28\"]\n",
    "\n",
    "# Sampling method: None --> \"no sampling\", \"up\" --> oversampling, \"under\" --> undersampling\n",
    "sampling_method = \"up\"\n",
    "\n",
    "# These are loaded in config already\n",
    "#EXPERIMENT_NAME = \"/Users/krista@jamcity.com/PP-Churn-Model\"\n",
    "#FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d3811b-9360-450c-bdd0-30917755b7fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_features = []\n",
    "other_features = ['unique_levels_played', 'market_idx','dayofweek','rounds_played', 'avg_attempts', 'total_attempts', 'avg_moves', 'win_rate', 'assist_success_rate', 'unassist_success_rate', 'assist_rate', 'total_boosters_used', 'total_boosters_spent', 'used_boosters_rate', 'spend_boosters_rate', 'avg_difficulty_score', 'rate_hard_levels', 'rate_superhard_levels', 'min_room_id_int', 'max_room_id_int', 'daily_win_rate_ref', 'daily_avg_boosters_used_ref', 'daily_avg_boosters_spent_ref', 'attribution_source_cd_idx', 'country_cd_idx', 'payer_type_cd_idx', 'iap_lifetime_amt', 'days_since_install', 'days_since_last_purchase', 'ad_revenue_amt', 'iap_revenue_amt', 'session_qty', 'total_session_length_qty', 'avg_session_length', 'sessions_per_round', 'avg_population_wr_on_levels_played_today', 'avg_population_assisted_rate_today', 'avg_population_attempts_today', 'wr_diff_vs_population', 'attempts_diff_vs_population', 'assist_rate_diff_vs_population', 'active_days_l7d', 'total_rounds_l7d', 'avg_rounds_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d', 'boosters_used_l7d', 'avg_used_boosters_rate_l7d', 'active_days_l14d', 'avg_rounds_l14d', 'avg_win_rate_l14d', 'std_rounds_l14d', 'std_win_rate_l14d', 'active_days_l30d', 'avg_rounds_l30d', 'rounds_trend_weekly', 'win_rate_trend_weekly', 'boosters_usage_trend_weekly', 'rounds_ratio_7d_vs_14_7d', 'frequency_ratio_7d_vs_14d', 'levels_progressed_l7d', 'levels_progressed_l14d', 'levels_progressed_l30d', 'days_on_current_max_level', 'level_diversity_ratio',]\n",
    "\n",
    "if cluster_vars is not None:\n",
    "    other_features = other_features + cluster_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc5dab7-81e8-4861-af5c-71f052197a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get data from table\n",
    "\n",
    "# If there is payer split\n",
    "if payer_split is None:\n",
    "\n",
    "    churn_features = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\")\\\n",
    "        .withColumn(\"label\",col(LABEL_COL))\n",
    "\n",
    "else:\n",
    "\n",
    "    churn_features = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                and payer_type_cd_idx in ({payer_split})\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\")\\\n",
    "        .withColumn(\"label\",col(LABEL_COL))\n",
    "    if payer_split == \"0\":\n",
    "        other_features.remove(\"payer_type_cd_idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecf9857-8566-4a80-8662-b99ced78bdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get stratified train, validation, test set\n",
    "strat_train, strat_val, strat_test = stratified_sampling(churn_features, P_TEST=0.2, P_VAL=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34d7df4-7caa-4e87-a146-09fae4078088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sampling method:\n",
    "\n",
    "if sampling_method == \"under\":\n",
    "\n",
    "  # Undersample majority class\n",
    "  strat_train, train_info = undersample_majority(churn_features)\n",
    "  print(train_info)\n",
    "\n",
    "elif sampling_method == \"up\":\n",
    "\n",
    "  #Upsample minority class\n",
    "  strat_train, train_info = upsample_minority(churn_features)\n",
    "  print(train_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea40cf1-2728-4955-a288-7b119bdf8832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build Pipeline for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1efbe14-c00a-4084-9edd-0afe94c20337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc4c5fa-394d-4046-a7af-308a36b9c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: would love to have a function that automatically sorts the columns by type\n",
    "#drop_for_features = {\"judi\",\"date\",\"churn3\"} \n",
    "#feature_cols = [c for c in df.columns if c not in drop_for_features and c not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14131b91-a056-49b5-bee1-22e2fabd4525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92016ccf-d515-4c32-a1d0-11f33a24e36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if num_workers > available slots, fitting fails\n",
    "# determine number of workers and repartition the training data\n",
    "strat_train, safe_workers = get_safe_works_repartition(strat_train)\n",
    "print(safe_workers)\n",
    "# strat_train_up, _ = get_safe_works_repartition(strat_train_up)\n",
    "# strat_train_under, _ = get_safe_works_repartition(strat_train_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7d8a2b-fac3-4a72-b816-1344b5333a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c262725c-a991-4e20-9f05-a0fd10727267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For XGBoost we don't need to standarize any features\n",
    "indexers = [StringIndexer(inputCol=x, \n",
    "                          outputCol=x+\"_index\", \n",
    "                          handleInvalid=\"keep\") for x in string_features]\n",
    "indexed_cols = [ x+\"_index\" for x in string_features]\n",
    "\n",
    "inputs = other_features + indexed_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=inputs, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "\n",
    "# Now add the xgb model to the pipeline\n",
    "#eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "eval_metrics = [\"aucpr\"]\n",
    "\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col = \"features\",\n",
    "  label_col = \"label\",\n",
    "  num_workers = safe_workers,\n",
    "  eval_metric = eval_metrics,\n",
    ")\n",
    "\n",
    "# Set the pipeline stages for the entire process\n",
    "pipeline = Pipeline().setStages(indexers+[vec_assembler]+ [xgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d294e78b-58d7-4df7-96b1-1e42a3e5a0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can fit your pipeline model here with MLFlow tracking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ad8669-0bc4-4438-9aaa-578fb47a8600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Param specs for random grid builder\n",
    "spec = {\n",
    "    # \"n_estimators\": (\"int_uniform\", 50, 1000),\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "    #\"gamma\": (\"uniform\", 0.0, 0.2),\n",
    "    #\"learning_rate\": (\"uniform\", 0.01,0.5),\n",
    "    # \"subsample\": (\"uniform\", 0.7, 0.9),\n",
    "    #\"colsample_bytree\": (\"uniform\", 0.7, 0.9),\n",
    "    # \"min_child_weight\": (\"int_uniform\", 1, 5),\n",
    "    #\"reg_alpha\": (\"uniform\", 0.0, 0.1),\n",
    "    #\"reg_lambda\": (\"int_uniform\", 1, 10),\n",
    "    #\"colsample_bylevel\": (\"uniform\", 0, 0.6),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ac9be2-0637-42fd-82d1-5dc6fa8723a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the MLflow logging level to INFO\n",
    "logger = logging.getLogger(\"mlflow\")\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51141cb1-661c-4225-b696-884d0a843786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# MLflow run: train, evaluate and log the best XGB pipeline model\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import builtins\n",
    "import statistics\n",
    "import json\n",
    "\n",
    "# Use PR AUC as the CV metric\n",
    "cv_xgb.setEvaluator(\n",
    "    BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        rawPredictionCol=\"probability\",   # XGB outputs probability column\n",
    "        metricName=\"areaUnderPR\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build a descriptive run name\n",
    "run_name_parts = [\n",
    "    \"XGB\",\n",
    "    str(DATE_INTERVAL),\n",
    "    sampling_method or \"no_sampling\",\n",
    "    LABEL_COL,\n",
    "]\n",
    "\n",
    "if payer_split == \"1,2\":\n",
    "    run_name_parts.append(\"payer\")\n",
    "elif payer_split == \"0\":\n",
    "    run_name_parts.append(\"non_payer\")\n",
    "\n",
    "run_name = \"_\".join(run_name_parts)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    # ------------ Log config / data context -------------\n",
    "    mlflow.log_param(\"label_col\", LABEL_COL)\n",
    "    mlflow.log_param(\"features_table\", FEATURES_TABLE_NAME)\n",
    "    mlflow.log_param(\"date_filter\", DATE_FILTER)\n",
    "    mlflow.log_param(\"date_interval_days\", DATE_INTERVAL)\n",
    "    mlflow.log_param(\"payer_split\", payer_split or \"None\")\n",
    "    mlflow.log_param(\"sampling_method\", sampling_method or \"None\")\n",
    "    mlflow.log_param(\"num_workers\", safe_workers)\n",
    "    mlflow.log_param(\"cluster_vars\", \",\".join(cluster_vars) if cluster_vars else \"None\")\n",
    "\n",
    "    for k, (dist, low, high) in spec.items():\n",
    "        mlflow.log_param(f\"search_{k}_dist\", dist)\n",
    "        mlflow.log_param(f\"search_{k}_low\", low)\n",
    "        mlflow.log_param(f\"search_{k}_high\", high)\n",
    "\n",
    "    # ------------ Fit CrossValidator ---------------------\n",
    "    cv_model = cv_xgb.fit(strat_train)\n",
    "    best_model = cv_model.bestModel  # PipelineModel\n",
    "\n",
    "    # 3) Average CV metric over the 5 folds (for best param set)\n",
    "    # CrossValidator chooses the param set with the highest avgMetrics value\n",
    "    avg_cv_aupr = float(builtins.max(cv_model.avgMetrics))\n",
    "    mlflow.log_metric(\"cv_mean_areaUnderPR\", avg_cv_aupr)\n",
    "\n",
    "    # ------------ Find assembler & XGB stages ------------\n",
    "    xgb_stage = None\n",
    "    vec_assembler_stage = None\n",
    "\n",
    "    for s in best_model.stages:\n",
    "        if isinstance(s, SparkXGBClassifier):\n",
    "            xgb_stage = s\n",
    "        if isinstance(s, VectorAssembler):\n",
    "            vec_assembler_stage = s\n",
    "\n",
    "    # ------------ 1) Merge val + test and evaluate once ---\n",
    "    eval_df = strat_val.unionByName(strat_test)\n",
    "\n",
    "    evaluator_pr = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        rawPredictionCol=\"probability\",\n",
    "        metricName=\"areaUnderPR\",\n",
    "    )\n",
    "\n",
    "    eval_preds = best_model.transform(eval_df)\n",
    "    eval_aupr = evaluator_pr.evaluate(eval_preds)\n",
    "    mlflow.log_metric(\"eval_areaUnderPR\", eval_aupr)\n",
    "\n",
    "    # Sizes of splits (still nice to know)\n",
    "    mlflow.log_metric(\"n_train_rows\", strat_train.count())\n",
    "    mlflow.log_metric(\"n_val_rows\", strat_val.count())\n",
    "    mlflow.log_metric(\"n_test_rows\", strat_test.count())\n",
    "    mlflow.log_metric(\"n_eval_rows\", eval_df.count())\n",
    "\n",
    "    # ------------ 2) Feature importance from XGBoost -----\n",
    "        # ------------ 2) Feature importance from XGBoost -----\n",
    "    # Re-find XGB & assembler stages in a robust way\n",
    "    xgb_stage = None\n",
    "    vec_assembler_stage = None\n",
    "\n",
    "    for s in best_model.stages:\n",
    "        # Any stage that exposes get_booster() is our XGBoost model\n",
    "        if hasattr(s, \"get_booster\"):\n",
    "            xgb_stage = s\n",
    "        # Standard Spark VectorAssembler\n",
    "        if isinstance(s, VectorAssembler):\n",
    "            vec_assembler_stage = s\n",
    "\n",
    "    if xgb_stage is None:\n",
    "        print(\"[WARN] No XGBoost stage with get_booster() found in best_model.stages; skipping feature importance logging.\")\n",
    "    else:\n",
    "        try:\n",
    "            booster = xgb_stage.get_booster()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not extract booster from XGB stage: {e}\")\n",
    "        else:\n",
    "            # importance_type can be \"gain\", \"weight\", \"cover\", \"total_gain\", etc.\n",
    "            score_dict = booster.get_score(importance_type=\"gain\")\n",
    "\n",
    "            # Try to map f0, f1, ... back to original feature names\n",
    "            feature_names = None\n",
    "            if vec_assembler_stage is not None:\n",
    "                feature_names = list(vec_assembler_stage.getInputCols())\n",
    "\n",
    "            mapped_scores = []\n",
    "            for fname, score in score_dict.items():\n",
    "                orig_name = fname\n",
    "                if feature_names and fname.startswith(\"f\"):\n",
    "                    try:\n",
    "                        idx = int(fname[1:])\n",
    "                        if idx < len(feature_names):\n",
    "                            orig_name = feature_names[idx]\n",
    "                        # else leave as fN\n",
    "                    except ValueError:\n",
    "                        # Not in the f<number> pattern; leave as is\n",
    "                        pass\n",
    "\n",
    "                mapped_scores.append(\n",
    "                    {\n",
    "                        \"feature\": orig_name,\n",
    "                        \"xgb_feature\": fname,\n",
    "                        \"gain\": float(score),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Sort by importance (gain) descending\n",
    "            mapped_scores.sort(key=lambda x: x[\"gain\"], reverse=True)\n",
    "\n",
    "            # Log top 50 as metrics for quick inspection in MLflow UI\n",
    "            for row in mapped_scores[:50]:\n",
    "                # Metric names must be ASCII / reasonably short\n",
    "                safe_name = row[\"feature\"].replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "                mlflow.log_metric(f\"feat_gain__{safe_name}\", row[\"gain\"])\n",
    "\n",
    "            # Log full importance as JSON artifact\n",
    "            import json\n",
    "            importance_path = \"/tmp/xgb_feature_importance.json\"\n",
    "            with open(importance_path, \"w\") as f:\n",
    "                json.dump(mapped_scores, f, indent=2)\n",
    "\n",
    "            mlflow.log_artifact(importance_path, artifact_path=\"feature_importance\")\n",
    "            print(\"[INFO] Logged feature importance JSON under artifact path 'feature_importance'.\")\n",
    "\n",
    "    # ------------ Log the full Spark pipeline model ------\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=best_model,\n",
    "        artifact_path=\"model\",\n",
    "    )\n",
    "\n",
    "    mlflow.set_tag(\"model_type\", \"SparkXGBClassifier\")\n",
    "    mlflow.set_tag(\"label\", LABEL_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7d0fa5-2bbb-4ab6-867f-512d703be5c5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"params\":317},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763131726802}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display results (based on CV performance only)\n",
    "\n",
    "experiment_lst = [{\"run_id\":\"a679118511bd46c3b13b76bb22e24972\",\"name\":\"XGB_30_up_churn7\"},\n",
    "                  {\"run_id\":\"d68b7ac6064c4ce7989a5716fe4b2322\",\"name\":\"XGB_30_up_churn3\"},\n",
    "                  {\"run_id\":\"3fdd4de3b3bb42b9a765a50d1e2254e3\",\"name\":\"XGB_30_up_churn5\"},\n",
    "                  {\"run_id\":\"0ae51ea70f3b48cab60c0dc9ea8049fc\",\"name\":\"XGB_30_up_churn7_non_payer\"},\n",
    "                  {\"run_id\":\"954ff7ad42d04d0eaa29eee31d0de69b\",\"name\":\"XGB_30_up_churn7_payer\"},\n",
    "                  {\"run_id\":\"a7c6fab2d77846a8918e49368f9c7e1c\",\"name\":\"XGB_30_up_churn7_non_payer_cluster\"},\n",
    "                  {\"run_id\":\"d92501999472499d85e1e794716fb5b5\",\"name\":\"XGB_30_up_churn7_payer_cluster\"},\n",
    "                  {\"run_id\":\"9b478d4b0ac94f9f8175fccc21fed8ee\",\"name\":\"XGB_30_under_churn7_payer\"},\n",
    "                  {\"run_id\":\"13fb5876ecd346de9f012bf9529545d8\",\"name\":\"XGB_30_under_churn7_non_payer\"},\n",
    "                  {\"run_id\":\"82dd7b0b6a3f427ca70a0ef47c04ca54\",\"name\":\"XGB_30_no_sampling_churn7_non_payer\"},\n",
    "                  {\"run_id\":\"aa2f0431a0434aa18eb5d5737ae59c11\",\"name\":\"XGB_30_no_sampling_churn7_payer\"}]\n",
    "\n",
    "df_lst = []\n",
    "\n",
    "val_eval = False\n",
    "\n",
    "for experiment in experiment_lst:\n",
    "\n",
    "    run_id = experiment[\"run_id\"]\n",
    "\n",
    "    artifact_path = download_artifacts(artifact_uri=f\"runs:/{run_id}/search_results.csv\")\n",
    "    df_tmp = pd.read_csv(artifact_path)\n",
    "    df_tmp[\"model\"] = experiment[\"name\"]\n",
    "\n",
    "    df_lst.append(df_tmp)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.concat(df_lst, axis=0)\n",
    "\n",
    "display(df.sort_values([\"params\",\"model\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e8cbe2-8c65-4512-bda4-0e071f3a26fe",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"tags.mlflow.runName\":249},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764104679179}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display results (based on CV performance and test performance)\n",
    "\n",
    "import mlflow\n",
    "\n",
    "def get_model_by_name(run_name, model_path=\"model\"):\n",
    "    # 1. Search for the run by name\n",
    "    # \"run_name\" is a first-class attribute in search filters\n",
    "    runs = mlflow.search_runs(\n",
    "    filter_string=f\"run_name = '{run_name}'\",\n",
    "    order_by=[\"attribute.start_time DESC\"],  # Sort by newest first\n",
    "    max_results=1\n",
    "    )\n",
    "    \n",
    "    # 2. Safety check: Ensure a run was found\n",
    "    if runs.empty:\n",
    "        raise ValueError(f\"No run found with name: {run_name}\")\n",
    "    \n",
    "    # 3. Get the Run ID (first match)\n",
    "    run_id = runs.iloc[0][\"run_id\"]\n",
    "    print(f\"Found Run ID: {run_id} for name: {run_name}\")\n",
    "    \n",
    "    # 4. Construct the Model URI\n",
    "    # Format: runs:/<run_id>/<artifact_path>\n",
    "    model_uri = f\"runs:/{run_id}/{model_path}\"\n",
    "    \n",
    "    return model_uri\n",
    "\n",
    "# Usage\n",
    "\n",
    "run_name_lst = [\"XGB_30_no_sampling_churn7_non_payer\",\n",
    "                \"XGB_30_no_sampling_churn7_payer\",\n",
    "                \"XGB_30_under_churn7_non_payer\",\n",
    "                \"XGB_30_under_churn7_payer\",\n",
    "                \"XGB_30_up_churn7_non_payer\",\n",
    "                \"XGB_30_up_churn7_payer\"]\n",
    "df_lst = []\n",
    "\n",
    "for run_name in run_name_lst:\n",
    "    \n",
    "    df_temp = mlflow.search_runs(filter_string=f\"run_name = '{run_name}'\")\n",
    "    df_temp = df_temp[(df_temp[\"start_time\"] >= \"2025-11-24\") & (df_temp[\"tags.model_type\"] == \"SparkXGBClassifier\")][[\"tags.mlflow.runName\",\"metrics.cv_mean_areaUnderPR\",\"metrics.eval_areaUnderPR\"]]\n",
    "    df_lst.append(df_temp)\n",
    "\n",
    "df = pd.concat(df_lst, axis=0)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93b3b90-8880-4561-a897-5a4c9141ac56",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"tags.mlflow.runName\":442},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764869156589}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check new models trained using the definitive table\n",
    "\n",
    "import mlflow\n",
    "\n",
    "def get_model_by_name(run_name, model_path=\"model\"):\n",
    "    # 1. Search for the run by name\n",
    "    # \"run_name\" is a first-class attribute in search filters\n",
    "    runs = mlflow.search_runs(\n",
    "    filter_string=f\"run_name = '{run_name}'\",\n",
    "    order_by=[\"attribute.start_time DESC\"],  # Sort by newest first\n",
    "    max_results=1\n",
    "    )\n",
    "    \n",
    "    # 2. Safety check: Ensure a run was found\n",
    "    if runs.empty:\n",
    "        raise ValueError(f\"No run found with name: {run_name}\")\n",
    "    \n",
    "    # 3. Get the Run ID (first match)\n",
    "    run_id = runs.iloc[0][\"run_id\"]\n",
    "    print(f\"Found Run ID: {run_id} for name: {run_name}\")\n",
    "    \n",
    "    # 4. Construct the Model URI\n",
    "    # Format: runs:/<run_id>/<artifact_path>\n",
    "    model_uri = f\"runs:/{run_id}/{model_path}\"\n",
    "    \n",
    "    return model_uri\n",
    "\n",
    "# Usage\n",
    "\n",
    "run_name_lst = [\"XGB_date_interval_30_nonpayer_churnlabel_churn7_sampling_stratified\",\n",
    "                \"XGB_date_interval_30_payer_churnlabel_churn7_sampling_stratified\",\n",
    "                \"XGB_date_interval_30_None_churnlabel_churn7_sampling_stratified\",\n",
    "                \"XGB_date_interval_30_nonpayer_churnlabel_churn7_sampling_upsample\",\n",
    "                \"XGB_date_interval_30_payer_churnlabel_churn7_sampling_upsample\",\n",
    "                \"XGB_date_interval_30_None_churnlabel_churn7_sampling_upsample\",\n",
    "                \"XGB_date_interval_30_nonpayer_churnlabel_churn7_sampling_undersample\",\n",
    "                \"XGB_date_interval_30_payer_churnlabel_churn7_sampling_undersample\",\n",
    "                \"XGB_date_interval_30_None_churnlabel_churn7_sampling_undersample\"]\n",
    "df_lst = []\n",
    "\n",
    "for run_name in run_name_lst:\n",
    "    \n",
    "    df_temp = mlflow.search_runs(filter_string=f\"run_name = '{run_name}'\")\n",
    "    df_temp = df_temp[(df_temp[\"start_time\"].astype(str) >= \"2025-11-24\")][[\"tags.mlflow.runName\",\"metrics.area_under_pr\"]]\n",
    "    df_lst.append(df_temp)\n",
    "\n",
    "df = pd.concat(df_lst, axis=0)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f95383-0d3b-4cbd-acda-f365b9e98e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check probability threshold\n",
    "model_name_lst = [\"XGB_date_interval_180_payer_churnlabel_churn7_sampling_upsample\"]\n",
    "\n",
    "for model_name in model_name_lst:\n",
    "\n",
    "    model_uri = get_model_by_name(model_name,model_path=\"best_model/spark-model\")\n",
    "    model = mlflow.spark.load_model(model_uri)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb9742a-3d4f-4748-8065-432a7764cbc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print([p.name for p in model.stages[-1].params])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8960266622078795,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "XGB Training - Different Targets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

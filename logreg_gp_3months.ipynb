{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd47ab30-8f8d-48e1-a0d7-8bcacc4f50ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "desc teams.data_science.pp_churn_features_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845d0c80-c2e6-443d-8a57-48aa8785c5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0a9c1b-67f7-4fbe-9235-1874ba48298f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90037fe1-5285-4e6e-86bc-14c75858948d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e487eb-ec22-4b5f-9fbb-349136080d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get data from table\n",
    "spark.sql(f\"select * from {FEATURES_TABLE_NAME}\").withColumn(\"label\",col(LABEL_COL)).createOrReplaceTempView(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448bcdfc-1ab3-42ac-8a96-3cff75899388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW VO_EVAL AS\n",
    "SELECT * FROM dataset where DATE BETWEEN '2025-09-22'  AND '2025-10-13';\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW VO_TRAIN AS\n",
    "SELECT * FROM dataset where DATE BETWEEN DATE '2025-09-21' - INTERVAL 3 Months  AND '2025-09-21';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d015b73c-f7c3-437d-b219-3be7bdcb05db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strat_train = spark.table(\"VO_TRAIN\")\n",
    "strat_val = spark.table(\"VO_EVAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194b48b0-6186-485f-af52-662db2191448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set ML Flow experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd52d49-9acf-4dca-8285-719da86f43f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_features = []\n",
    "other_features = ['unique_levels_played', 'market_idx','dayofweek','rounds_played', 'avg_attempts', 'total_attempts', 'avg_moves', 'win_rate', 'assist_success_rate', 'unassist_success_rate', 'assist_rate', 'total_boosters_used', 'total_boosters_spent', 'used_boosters_rate', 'spend_boosters_rate', 'avg_difficulty_score', 'rate_hard_levels', 'rate_superhard_levels', 'min_room_id_int', 'max_room_id_int', 'daily_win_rate_ref', 'daily_avg_boosters_used_ref', 'daily_avg_boosters_spent_ref', 'attribution_source_cd_idx', 'country_cd_idx', 'payer_type_cd_idx', 'iap_lifetime_amt', 'days_since_install', 'days_since_last_purchase', 'ad_revenue_amt', 'iap_revenue_amt', 'session_qty', 'total_session_length_qty', 'avg_session_length', 'sessions_per_round', 'avg_population_wr_on_levels_played_today', 'avg_population_assisted_rate_today', 'avg_population_attempts_today', 'wr_diff_vs_population', 'attempts_diff_vs_population', 'assist_rate_diff_vs_population', 'active_days_l7d', 'total_rounds_l7d', 'avg_rounds_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d', 'boosters_used_l7d', 'boosters_spent_l7d', 'avg_used_boosters_rate_l7d', 'active_days_l14d', 'avg_rounds_l14d', 'avg_win_rate_l14d', 'std_rounds_l14d', 'std_win_rate_l14d', 'active_days_l30d', 'avg_rounds_l30d', 'rounds_trend_weekly', 'win_rate_trend_weekly', 'boosters_usage_trend_weekly', 'rounds_ratio_7d_vs_14_7d', 'frequency_ratio_7d_vs_14d', 'levels_progressed_l7d', 'levels_progressed_l14d', 'levels_progressed_l30d', 'days_on_current_max_level', 'level_diversity_ratio',] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e68493-00ae-47c0-bac3-431688d3044b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_total = strat_train.count()\n",
    "train_churn = strat_train.filter(col('label') == 1).count()\n",
    "train_no_churn = train_total - train_churn\n",
    "\n",
    "churn_ratio = train_churn / train_total\n",
    "print(f\"Churn ratio en train: {churn_ratio:.2%}\")\n",
    "\n",
    "#Calcular pesos balanceados\n",
    "# Formula: weight = n_samples / (n_classes * n_samples_class)\n",
    "weight_churn = train_total / (2 * train_churn)\n",
    "weight_no_churn = train_total / (2 * train_no_churn)\n",
    "\n",
    "print(f\"Weight para churn=1: {weight_churn:.2f}\")\n",
    "print(f\"Weight para churn=0: {weight_no_churn:.2f}\")\n",
    "\n",
    "# 3. Agregar columna weight SOLO a train\n",
    "strat_train_weighted = strat_train.withColumn('weight', \n",
    "    when(col('label') == 1, weight_churn)\n",
    "    .otherwise(weight_no_churn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fc0208-6833-4435-8893-364acc7be824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Logistic Regression Pipeline\n",
    "\n",
    "#Prepare Data\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "imputer = Imputer(inputCols=other_features, outputCols=other_features).setStrategy(\"mean\")\n",
    "assembler = VectorAssembler(inputCols=other_features, outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "\n",
    "# Add classifier\n",
    "eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    family='binomial',\n",
    "    weightCol='weight'\n",
    ")\n",
    "\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[imputer, assembler, scaler, lr])\n",
    "\n",
    "\n",
    "# lr_grid = (ParamGridBuilder()\n",
    "#     .addGrid(lr.regParam, [1e-5, 1e-4, 1e-3, 1e-2, 0.1]) \n",
    "#     .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "#     .addGrid(lr.maxIter, [100, 200])  \n",
    "#     .addGrid(lr.fitIntercept, [True, False]) \n",
    "#     .build())\n",
    "\n",
    "lr_grid = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.001]) \n",
    "    .addGrid(lr.elasticNetParam, [1.0])\n",
    "    .addGrid(lr.maxIter, [100])  \n",
    "    .addGrid(lr.fitIntercept, [True]) \n",
    "    .build())\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    metricName='areaUnderPR'  # Precision-Recall AUC\n",
    ")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr_pipeline,\n",
    "    estimatorParamMaps=lr_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "042976f8-2721-427a-b90e-3cc6681734f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "\n",
    "# Data handling / mÃ©tricas\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "\n",
    "# VisualizaciÃ³n\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "mlflow.spark.autolog()\n",
    "\n",
    "# # Entrenamiento\n",
    "# lr_model = lr_cv.fit(strat_train_weighted.persist(StorageLevel.MEMORY_AND_DISK))\n",
    "# best_lr = lr_model.bestModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3db52710-6d21-4f7b-9429-32aad39886ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "with mlflow.start_run(run_name=\"logistic_regression_churn_TEST\") as run:\n",
    "    # Entrenamiento\n",
    "    lr_model = lr_cv.fit(strat_train_weighted.persist(StorageLevel.MEMORY_AND_DISK))\n",
    "    best_lr = lr_model.bestModel\n",
    "\n",
    "    # Loguear el modelo entrenado en MLflow\n",
    "    mlflow.spark.log_model(best_lr, artifact_path=\"model\")\n",
    "\n",
    "    # Predicciones\n",
    "    predictions = best_lr.transform(strat_val)\n",
    "\n",
    "    # Evaluaciones\n",
    "    evaluator_roc = BinaryClassificationEvaluator(labelCol=\"churn3\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "    auc = evaluator_roc.evaluate(predictions)\n",
    "    mlflow.log_metric(\"auc_roc\", auc)\n",
    "\n",
    "    evaluator_pr = BinaryClassificationEvaluator(labelCol=\"churn3\", rawPredictionCol=\"probability\", metricName=\"areaUnderPR\")\n",
    "    auc_pr = evaluator_pr.evaluate(predictions)\n",
    "    mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "\n",
    "    # =======================================================\n",
    "    # 4. Extraer prob_churn del DenseVector\n",
    "    # =======================================================\n",
    "\n",
    "\n",
    "    @F.udf(DoubleType())\n",
    "    def extract_prob_churn(v):\n",
    "        if v is None:\n",
    "            return None\n",
    "        return float(v[1]) if isinstance(v, DenseVector) else float(v.values[1])\n",
    "\n",
    "    pred_with_prob = predictions.withColumn(\"prob_churn\", extract_prob_churn(F.col(\"probability\")))\n",
    "\n",
    "    pred_with_prob = predictions.withColumn(\"prob_churn\", extract_prob_churn(F.col(\"probability\")))\n",
    "\n",
    "    # =======================================================\n",
    "    # 5. Calcular mÃ©tricas\n",
    "    # =======================================================\n",
    "    # AUC ROC y AUC PR\n",
    "    evaluator_roc = BinaryClassificationEvaluator(labelCol=\"churn3\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "    auc = evaluator_roc.evaluate(predictions)\n",
    "\n",
    "    evaluator_pr = BinaryClassificationEvaluator(labelCol=\"churn3\", rawPredictionCol=\"probability\", metricName=\"areaUnderPR\")\n",
    "    auc_pr = evaluator_pr.evaluate(predictions)\n",
    "\n",
    "    # TP, FP, FN, TN\n",
    "    tp = pred_with_prob.filter((F.col(\"churn3\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "    fp = pred_with_prob.filter((F.col(\"churn3\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "    fn = pred_with_prob.filter((F.col(\"churn3\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "    tn = pred_with_prob.filter((F.col(\"churn3\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-9)\n",
    "    recall = tp / (tp + fn + 1e-9)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-9)\n",
    "\n",
    "    # LogLoss (usa la prob_churn)\n",
    "    pdf = pred_with_prob.select(\"churn3\", \"prob_churn\").toPandas()\n",
    "    logloss = log_loss(pdf[\"churn3\"], pdf[\"prob_churn\"])\n",
    "\n",
    "    # =======================================================\n",
    "    # 6. Matriz de confusiÃ³n (visual)\n",
    "    # =======================================================\n",
    "    cm = confusion_matrix(pdf[\"churn3\"], (pdf[\"prob_churn\"] >= 0.5).astype(int))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "                xticklabels=[\"Pred No Churn\", \"Pred Churn\"],\n",
    "                yticklabels=[\"Real No Churn\", \"Real Churn\"])\n",
    "    plt.title(\"Confusion Matrix (threshold=0.5)\")\n",
    "    plt.xlabel(\"PredicciÃ³n\")\n",
    "    plt.ylabel(\"Real\")\n",
    "\n",
    "    # =======================================================\n",
    "    # 7. Loguear todo dentro del mismo run activo\n",
    "    # =======================================================\n",
    "    mlflow.log_metric(\"auc_roc\", auc)\n",
    "    mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"log_loss\", logloss)\n",
    "    mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "\n",
    "    print(f\"\"\"\n",
    "    ðŸ“Š MÃ©tricas en Test:\n",
    "    AUC ROC:   {auc:.4f}\n",
    "    AUC PR:    {auc_pr:.4f}\n",
    "    Precision: {precision:.4f}\n",
    "    Recall:    {recall:.4f}\n",
    "    F1-score:  {f1:.4f}\n",
    "    Accuracy:  {accuracy:.4f}\n",
    "    LogLoss:   {logloss:.4f}\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"âœ… Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ff13d0-39ef-4f05-8c5a-a5a2d714c4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a74f36a-fc2f-4e91-9ef7-2b2760b65293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # =======================================================\n",
    "# # 3. Predicciones sobre test\n",
    "# # =======================================================\n",
    "# predictions = best_lr.transform(strat_val)\n",
    "\n",
    "# # =======================================================\n",
    "# # 4. Extraer prob_churn del DenseVector\n",
    "# # =======================================================\n",
    "\n",
    "\n",
    "# @F.udf(DoubleType())\n",
    "# def extract_prob_churn(v):\n",
    "#     if v is None:\n",
    "#         return None\n",
    "#     return float(v[1]) if isinstance(v, DenseVector) else float(v.values[1])\n",
    "\n",
    "# pred_with_prob = predictions.withColumn(\"prob_churn\", extract_prob_churn(F.col(\"probability\")))\n",
    "\n",
    "# pred_with_prob = predictions.withColumn(\"prob_churn\", extract_prob_churn(F.col(\"probability\")))\n",
    "\n",
    "# # =======================================================\n",
    "# # 5. Calcular mÃ©tricas\n",
    "# # =======================================================\n",
    "# # AUC ROC y AUC PR\n",
    "# evaluator_roc = BinaryClassificationEvaluator(labelCol=\"churn3\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "# auc = evaluator_roc.evaluate(predictions)\n",
    "\n",
    "# evaluator_pr = BinaryClassificationEvaluator(labelCol=\"churn3\", rawPredictionCol=\"probability\", metricName=\"areaUnderPR\")\n",
    "# auc_pr = evaluator_pr.evaluate(predictions)\n",
    "\n",
    "# # TP, FP, FN, TN\n",
    "# tp = pred_with_prob.filter((F.col(\"churn3\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "# fp = pred_with_prob.filter((F.col(\"churn3\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "# fn = pred_with_prob.filter((F.col(\"churn3\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "# tn = pred_with_prob.filter((F.col(\"churn3\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "# precision = tp / (tp + fp + 1e-9)\n",
    "# recall = tp / (tp + fn + 1e-9)\n",
    "# f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "# accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-9)\n",
    "\n",
    "# # LogLoss (usa la prob_churn)\n",
    "# pdf = pred_with_prob.select(\"churn3\", \"prob_churn\").toPandas()\n",
    "# logloss = log_loss(pdf[\"churn3\"], pdf[\"prob_churn\"])\n",
    "\n",
    "# # =======================================================\n",
    "# # 6. Matriz de confusiÃ³n (visual)\n",
    "# # =======================================================\n",
    "# cm = confusion_matrix(pdf[\"churn3\"], (pdf[\"prob_churn\"] >= 0.5).astype(int))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5,4))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "#             xticklabels=[\"Pred No Churn\", \"Pred Churn\"],\n",
    "#             yticklabels=[\"Real No Churn\", \"Real Churn\"])\n",
    "# plt.title(\"Confusion Matrix (threshold=0.5)\")\n",
    "# plt.xlabel(\"PredicciÃ³n\")\n",
    "# plt.ylabel(\"Real\")\n",
    "\n",
    "# # =======================================================\n",
    "# # 7. Loguear todo dentro del mismo run activo\n",
    "# # =======================================================\n",
    "# mlflow.log_metric(\"auc_roc\", auc)\n",
    "# mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "# mlflow.log_metric(\"precision\", precision)\n",
    "# mlflow.log_metric(\"recall\", recall)\n",
    "# mlflow.log_metric(\"f1_score\", f1)\n",
    "# mlflow.log_metric(\"accuracy\", accuracy)\n",
    "# mlflow.log_metric(\"log_loss\", logloss)\n",
    "# mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "\n",
    "# print(f\"\"\"\n",
    "# ðŸ“Š MÃ©tricas en Test:\n",
    "# AUC ROC:   {auc:.4f}\n",
    "# AUC PR:    {auc_pr:.4f}\n",
    "# Precision: {precision:.4f}\n",
    "# Recall:    {recall:.4f}\n",
    "# F1-score:  {f1:.4f}\n",
    "# Accuracy:  {accuracy:.4f}\n",
    "# LogLoss:   {logloss:.4f}\n",
    "# \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6833368668330666,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "logreg_gp_3months",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

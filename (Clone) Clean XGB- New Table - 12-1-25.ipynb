{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99588b18-d4f4-4e5b-913e-c76f2ac1c2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow xgboost\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8dfb97-a7b6-4ba6-a5b8-575ccdcb8587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "import mlflow.spark\n",
    "from mlflow.artifacts import download_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb74358-93d4-4617-823d-26c86da509c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.config import *\n",
    "from src.sampling import *\n",
    "from src.tracking import *\n",
    "from src.tuning import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15a0f9e-5b1a-4a39-a08a-f39f1c0b8e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn7\"\n",
    "DATE_FILTER = \"2025-10-26\"\n",
    "DATE_INTERVAL = 30\n",
    "\n",
    "# Payer split: None --> no split, \"0\" --> non-payer, \"1,2\" --> payer\n",
    "#payer_split = \"1,2\"\n",
    "\n",
    "\n",
    "#payer_split = None\n",
    "#payer_split = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a801104-8be3-4b60-9959-412e0e9b9cb8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"data_type\":130},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763668760345}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(f\"\"\" describe table {FEATURES_TABLE_NAME}\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f34f45-7b37-4ac4-91df-b0cc3f0822fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"select * from {FEATURES_TABLE_NAME}\n",
    "                                where '{LABEL_COL}' is not null\n",
    "                                and date between date_sub('{DATE_FILTER}',{DATE_INTERVAL}) AND '{DATE_FILTER}' \"\"\").withColumn('market_name', col('market')).drop('market').withColumnRenamed('market_name','market')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766a5ca1-b759-48fe-825b-e11e250513e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, NumericType, BooleanType\n",
    "\n",
    "\n",
    "#churn_labels = ['churn3','churn5','churn7','churn14']\n",
    "#for label in churn_labels:\n",
    "#    df = df.withColumn(label, when(col(label) == True,1).otherwise(0))\n",
    "\n",
    "string_features = []\n",
    "numerical_features = []\n",
    "churn_labels = []\n",
    "\n",
    "drop_cols = ['judi','date','ts_last_updated','processed_date','churn3','churn5','churn7','churn14']\n",
    "\n",
    "'''\n",
    "for field in df.schema.fields:\n",
    "    #if isinstance(field.dataType, BooleanType):\n",
    "    #    df = df.withColumn(field.name, when(col(field.name)==True,1).otherwise(0))\n",
    "    #    churn_labels.append(field.name)\n",
    "    if ((isinstance(field.dataType, StringType)) and (field not in drop_cols)):\n",
    "        string_features.append(field.name)\n",
    "    elif ((isinstance(field.dataType, NumericType)) and (field not in drop_cols)):\n",
    "        numerical_features.append(field.name)'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6af71db-2f21-452e-90c0-de577478b4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for field in df.schema.fields:\n",
    "    #if isinstance(field.dataType, BooleanType):\n",
    "    #    df = df.withColumn(field.name, when(col(field.name)==True,1).otherwise(0))\n",
    "    #    churn_labels.append(field.name)\n",
    "    if isinstance(field.dataType, StringType) and field.name not in drop_cols:\n",
    "        string_features.append(field.name)\n",
    "    elif isinstance(field.dataType, NumericType) and field.name not in drop_cols:\n",
    "        numerical_features.append(field.name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98e2442-a5b5-43fc-97dc-5928ae326f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1fd280d-6e9b-4ea0-8e5d-53494820708f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Still have unioned sets here\n",
    "    #strat_train = strat_train_payer.union(strat_train_nonpayer)\n",
    "    #strat_val = strat_val_payer.union(strat_val_nonpayer)\n",
    "    #strat_test = strat_test_payer.union(strat_test_nonpayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b76037-5515-417c-9be2-86dc7ca5f48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.widgets.dropdown(\"split_payers\",False,[True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c37a823-c247-4a53-802a-563e0eb15f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Splitting by payer and non-payer?\n",
    "churn_features = df.withColumn('label', when(col(LABEL_COL)==True,1).otherwise(0))\n",
    "\n",
    "#split_payers=\"True\"\n",
    "split_payers = True\n",
    "#non_payer_sets=[]\n",
    "#payer_sets = []\n",
    "\n",
    "if split_payers:\n",
    "    payers = ['P','S']\n",
    "    non_payers= ['N']\n",
    "\n",
    "    # Split nonpayer and payer separately\n",
    "    strat_train_payer, strat_val_payer, strat_test_payer = stratified_sampling(churn_features.filter(col('payer_type_cd').isin(payers)), P_TEST=0.2, P_VAL=0.2)\n",
    "\n",
    "    strat_train_nonpayer, strat_val_nonpayer, strat_test_nonpayer = stratified_sampling(churn_features.filter(col('payer_type_cd').isin(non_payers)), P_TEST=0.2, P_VAL=0.2)\n",
    "\n",
    "\n",
    "    strat_train, strat_val, strat_test = stratified_sampling(churn_features, P_TEST=0.2, P_VAL=0.2) \n",
    "    \n",
    "\n",
    "### This is just the stratified sampling, without a payer/nonpayer split\n",
    "else:\n",
    "    strat_train, strat_val, strat_test = stratified_sampling(churn_features, P_TEST=0.2, P_VAL=0.2)    \n",
    "\n",
    "#Base info \n",
    "stratified_info =  {\n",
    "                        'sampling':'stratified', \n",
    "                        'split':None,\n",
    "                        'P_TEST':0.2,\n",
    "                        'P_VAL':0.2,\n",
    "                        'P_TRAIN':0.6,\n",
    "                        'strategy':None\n",
    "                    }\n",
    "\n",
    "all_sets = [{\n",
    "                'dataset': strat_train,\n",
    "                'dataset_info': {**stratified_info, 'type':'training'},\n",
    "                'relavent_test_set':strat_test,\n",
    "                'relavent_val_set':strat_val,\n",
    "            },      \n",
    "            #{\n",
    "            #    'dataset': strat_val, \n",
    "            #    'dataset_info':{**stratified_info, 'type':'validation'}\n",
    "            #}, \n",
    "            #{\n",
    "            #    'dataset': strat_test,\n",
    "            #    'dataset_info': {**stratified_info, 'type':'testing'}\n",
    "            #}\n",
    "        ]\n",
    "\n",
    "#stratified_sets= all_sets\n",
    "\n",
    "split_info = {\n",
    "    'sampling':'stratified', \n",
    "    'P_TEST':0.2,\n",
    "    'P_VAL':0.2,\n",
    "    'P_TRAIN':0.6,\n",
    "},\n",
    "\n",
    "if split_payers:\n",
    "\n",
    "    # non_payer\n",
    "    non_payer_slug = {\n",
    "                'dataset':strat_train_nonpayer, \n",
    "                'dataset_info':{\n",
    "                    **stratified_info,\n",
    "                    'split':'nonpayer', \n",
    "                    'type':'training'\n",
    "                    },\n",
    "                'relavent_test_set': strat_test_nonpayer,\n",
    "                'relavent_val_set': strat_val_nonpayer,\n",
    "            }\n",
    "    all_sets.append(non_payer_slug)\n",
    "    \n",
    "    #payers\n",
    "    payers_slug = {\n",
    "                'dataset':strat_train_payer,\n",
    "                'dataset_info':{\n",
    "                    **stratified_info, \n",
    "                    'split':'payer', \n",
    "                    'type':'training'\n",
    "                    },\n",
    "                'relavent_test_set': strat_test_payer,\n",
    "                'relavent_val_set': strat_val_payer,\n",
    "            }\n",
    "    all_sets.append(payers_slug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a66ca9f-f1e2-4fe5-8868-4d1c9f2c0093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upsampling\n",
    "upsample=True\n",
    "undersample = True\n",
    "\n",
    "#upsample=\"True\"\n",
    "#undersample = \"True\"\n",
    "\n",
    "# Now deal with player splits\n",
    "if split_payers: #if split_payers==\"True\"\n",
    "    if upsample==True: #if upsample==\"True\"\n",
    "        strat_train_up_payer, train_up_payer_info = upsample_minority(strat_train_payer,split='payer')\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up_payer, \n",
    "                'dataset_info':train_up_payer_info,\n",
    "                'relevant_test_set': strat_test_payer,\n",
    "                'relevant_val_set': strat_val_payer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        strat_train_up_nonpayer, train_up_nonpayer_info = upsample_minority(strat_train_nonpayer,split='nonpayer')\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up_nonpayer, \n",
    "                'dataset_info':train_up_nonpayer_info,\n",
    "                'relevant_test_set': strat_test_nonpayer,\n",
    "                'relevant_val_set': strat_val_nonpayer,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Union for upsampling - remove splits\n",
    "        strat_train_up = strat_train_up_payer.union(strat_train_up_nonpayer)\n",
    "        train_up_info = {'majority_label': 0,\n",
    "                            'minority_label': 1,\n",
    "                            'strategy':'sampling',\n",
    "                            'sampling': 'upsample',\n",
    "                            'split':None,\n",
    "                            'type':\"training\"}\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up, \n",
    "                'dataset_info':train_up_info,\n",
    "                'relevant_test_set': strat_test,\n",
    "                'relevant_val_set': strat_val,\n",
    "\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    if undersample==True: #if undersample==\"True\":\n",
    "        strat_train_under_payer, train_under_payer_info = undersample_majority(strat_train_payer,split='payer') \n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under_payer, \n",
    "                'dataset_info':train_under_payer_info,\n",
    "                'relevant_test_set':strat_test_payer,\n",
    "                'relevant_val_set':strat_val_payer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        strat_train_under_nonpayer, train_under_nonpayer_info = undersample_majority(strat_train_nonpayer,split='nonpayer')\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under_nonpayer, \n",
    "                'dataset_info':train_under_nonpayer_info,\n",
    "                'relevant_test_set':strat_test_nonpayer,\n",
    "                'relevant_val_set':strat_val_nonpayer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Union for undersampling - remove split\n",
    "        strat_train_under = strat_train_under_payer.union(strat_train_under_nonpayer)\n",
    "        train_under_info = {\n",
    "                                'majority_label': 0,\n",
    "                                'minority_label': 1,\n",
    "                                'strategy':'sampling',\n",
    "                                'sampling':'undersample',\n",
    "                                'split':None,\n",
    "                                'type':\"training\",\n",
    "                            }\n",
    "        \n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under, \n",
    "                'dataset_info':train_under_info,\n",
    "                'relevant_test_set':strat_test,\n",
    "                'relevant_val_set':strat_val,\n",
    "            })\n",
    "\n",
    "else:\n",
    "    if upsample==True: # if upsample==\"True\"\n",
    "        ## Upsampling\n",
    "        strat_train_up, train_up_info = upsample_minority(strat_train)\n",
    "        train_up_info['type']='training'\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_up, \n",
    "                'dataset_info':train_up_info,\n",
    "                'relevant_test_set':strat_test,\n",
    "                'relevant_val_set':strat_val,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if undersample==True: # if undersample==\"True\"\n",
    "        ## Undersampling\n",
    "        strat_train_under, train_under_info = undersample_majority(strat_train)\n",
    "        train_under_info['type'] = 'training'\n",
    "        all_sets.append(\n",
    "            {\n",
    "                'dataset':strat_train_under, \n",
    "                'dataset_info':train_under_info,\n",
    "                'relevant_test_set':strat_test,\n",
    "                'relevant_val_set':strat_val\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a958ccd7-db57-459b-a60f-c8113ab63976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#training_sets - no underampling.... (strat_train, (strat_val, strat_test)), (start_train_payer, (strat_val_payer,strat_test_payer))..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd0e948-6ec6-4fd7-8eaa-22344d933c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fceba73d-e571-45e4-8c7c-5954797f7336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unecessary because we only have 1 worker?\n",
    "\n",
    "for val in all_sets:\n",
    "    repartitioned, safe_workers = get_safe_works_repartition(val['dataset'])\n",
    "    val['dataset']=repartitioned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa8f805-7a26-40c0-b857-997aa0b14ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in all_sets:\n",
    "    print(i.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afec6ca2-ba0b-4e10-9d91-e8c3f947150b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For XGBoost we don't need to standarize any features\n",
    "indexers = [StringIndexer(inputCol=x, \n",
    "                          outputCol=x+\"_index\", \n",
    "                          handleInvalid=\"keep\") for x in string_features]\n",
    "indexed_cols = [ x+\"_index\" for x in string_features]\n",
    "\n",
    "inputs = numerical_features + indexed_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=inputs, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "\n",
    "# Now add the xgb model to the pipeline\n",
    "#eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "eval_metrics = [\"aucpr\"]\n",
    "\n",
    "\n",
    "safe_workers=1\n",
    "\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col = \"features\",\n",
    "  label_col = \"label\",\n",
    "  num_workers = safe_workers,\n",
    "  eval_metric = eval_metrics,\n",
    ")\n",
    "\n",
    "# Set the pipeline stages for the entire process\n",
    "pipeline = Pipeline().setStages(indexers+[vec_assembler]+ [xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f74c3682-cb2d-4e8c-8493-1ccdabc86f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "spec = {\n",
    "    # \"n_estimators\": (\"int_uniform\", 50, 1000),\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "    #\"gamma\": (\"uniform\", 0.0, 0.2),\n",
    "    #\"learning_rate\": (\"uniform\", 0.01,0.5),\n",
    "    # \"subsample\": (\"uniform\", 0.7, 0.9),\n",
    "    #\"colsample_bytree\": (\"uniform\", 0.7, 0.9),\n",
    "    # \"min_child_weight\": (\"int_uniform\", 1, 5),\n",
    "    #\"reg_alpha\": (\"uniform\", 0.0, 0.1),\n",
    "    #\"reg_lambda\": (\"int_uniform\", 1, 10),\n",
    "    #\"colsample_bylevel\": (\"uniform\", 0, 0.6),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03254e5d-f737-4388-b107-27fd946a1356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spec = {\n",
    "    \"max_depth\":  (\"int_uniform\", 8, 8), # Originally \"max_depth\":  (\"int_uniform\", 4, 8),\n",
    "}\n",
    "\n",
    "# build random xgb param map\n",
    "xgb_param_maps = build_random_param_maps(xgb, spec, n_samples=40, seed=7)\n",
    "\n",
    "\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=xgb_param_maps,\n",
    "    numFolds=5,\n",
    "    seed=7,\n",
    "    # parallelism=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d723be0c-c2d7-4316-8f9f-8cf148e0522a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41021f21-aa61-4370-ac01-7171c9d79264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the MLflow logging level to INFO\n",
    "logger = logging.getLogger(\"mlflow\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79035bd-2615-4f29-a9fd-a6ebafb866db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plus other useful information \n",
    "extra_tags = { \n",
    "                'label': LABEL_COL,\n",
    "                'safe_workers':safe_workers, \n",
    "                'date_filter':DATE_FILTER, \n",
    "                'date_interval':DATE_INTERVAL, \n",
    "                'source_table_name':FEATURES_TABLE_NAME\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac632ab0-93c7-488f-b2a9-16ab54265f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for val in all_sets:\n",
    "    val['extra_tags']= {**extra_tags, **val['dataset_info']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "25fa93cc-d57d-4261-baa1-cd9db30b9b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with mlflow.start_run(run_name=\"xgb_test\") as run:\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    evaluator.setMetricName('areaUnderPR')\n",
    "    evaluator.setLabelCol('label')\n",
    "    \n",
    "    cv_xgb.setEvaluator(evaluator)\n",
    "    cv_xgb.setCollectSubModels(True)\n",
    "    dataset = all_sets[0]['dataset']\n",
    "    cv_xgb.fit(dataset)\n",
    "\n",
    "    mlflow.log_metric('areaUnderPR', cv_xgb.avgMetrics[0])\n",
    "    mlflow.set_tags(all_sets[0]['extra_tags'])\n",
    "    print(cg_xgb.avgMetrics)\n",
    "    #mlflow.log_metric('areaUnderROC', cv_xgb.avgMetrics[1])')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2229dd35-8bea-4f80-b649-77b4dbcd85a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all_sets[10]['dataset']\n",
    "# all_sets[10]['dataset_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8469d61-1b96-4b2e-ac92-815ff820f95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dataset = all_sets[0]['dataset']\n",
    "#extra_tags = all_sets[0]['extra_tags']\n",
    "\n",
    "#print(all_sets[0]['dataset_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe21ba83-48c0-4e72-b961-9fdc664ca30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# '''training :{\"sampling\":\"upsampled\", \"type\":\"training\", \"split\":\"payer\"}\n",
    "# validation : stratified, validation, payer...\n",
    "# test: stratified, testing, nonpayer...''''''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94345124-6667-4fb3-9b88-1cba30c40177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9e022e-eb21-4add-a555-bf97d0c7f183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# copy_all_sets = all_sets.copy()\n",
    "\n",
    "# for val in copy_all_sets:\n",
    "#     if val['type']=='training':\n",
    "#         for match in all_sets:\n",
    "#             if val['split']==match['split']:\n",
    "#                 if match['type']=='testing':\n",
    "#                     val['testing_df']=match\n",
    "#                 if match['type']=='validation':\n",
    "#                     val['validation_df']=match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff15a06-ce01-4ee1-a4b1-14352c878775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ebb579-9afe-4385-96b4-1649282745ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(all_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4fbcbe-f78f-4dd0-9785-16ccf685ebba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(all_sets[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4cc339-ffa3-4a1c-9954-4506e88349c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in all_sets:\n",
    "    print(i.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ca29bf-8fc1-44a7-be4b-2af5286e7f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_sets[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40df53c9-950a-4766-b9fc-6472ed631b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for set in all_sets[7:]:\n",
    "    print(set[\"dataset_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de76b8f-a8fb-4c7f-8faf-d5a18991b306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#for i in all_sets[0:1]:\n",
    "\n",
    "for i in all_sets[7:]:\n",
    "    print(i[\"dataset_info\"])\n",
    "\n",
    "    extra_tags = i[\"extra_tags\"]\n",
    "    dataset_training = i[\"dataset\"]\n",
    "    strat_test_training = i[\"relevant_test_set\"]\n",
    "    strat_val_training = i[\"relevant_val_set\"]\n",
    "\n",
    "    print(extra_tags)\n",
    "\n",
    "    ### Strat train up: \n",
    "    run_spark_ml_training(estimator = cv_xgb, \n",
    "                        train_df = dataset_training, \n",
    "                        test_df = strat_test_training, \n",
    "                        val_df = strat_val_training, \n",
    "                        extra_tags = extra_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0070f119-ce1b-444f-b2c2-1f467bb067f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### \n",
    "\n",
    "# import mlflow\n",
    "\n",
    "# model_uri = 'runs:/77facbd0a5f044ce807b92e5a9df96e3/best_model/spark-model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468bf35b-090d-4503-924e-1192bd622fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run_spark_ml_training(cv_xgb, strat_train_up, test_df = strat_test, val_df = strat_val, extra_tags = train_up_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ca6138-19ab-4feb-a6d7-a5f92ce78ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "# Modify and test the tracking functions (log all to the mlflow experiment, vs. the notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4ccddc-677e-40cd-98c6-5cc70edace9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train on upsampled data \n",
    "\n",
    "#cv_xgb.setEvaluator(BinaryClassificationEvaluator(metricName=\"areaUnderPR\"))\n",
    "#cv_xgb.fit(strat_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b729418-56b3-4e2f-9e62-acefb6b71848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9100e81b-1ab9-4876-b286-3a1d7829df69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Clean XGB- New Table - 12-1-25",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

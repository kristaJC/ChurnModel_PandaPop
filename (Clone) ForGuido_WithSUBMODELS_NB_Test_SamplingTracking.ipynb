{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdca6ef-7d36-4766-8233-ce038931c504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow xgboost\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c9c289-852d-4992-ab5b-ce691f8eb6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel, ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8bdcf9-aa2f-4543-a603-6f171c63ec5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "from sampling import *\n",
    "from tracking import *\n",
    "#from Deprecated.deprecated_tracking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22db0c6-79bf-431e-9036-75491b97620f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn3\"\n",
    "DATE_FILTER = \"2025-10-17\"\n",
    "DATE_INTERVAL = 90\n",
    "\n",
    "FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc5dab7-81e8-4861-af5c-71f052197a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get data from table\n",
    "churn_features = spark.sql(f\"select * from {FEATURES_TABLE_NAME}\")\\\n",
    "    .withColumn(\"label\",col(LABEL_COL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecf9857-8566-4a80-8662-b99ced78bdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get stratified train, validation, test set\n",
    "strat_train, strat_val, strat_test = stratified_sampling(churn_features, P_TEST=0.2, P_VAL=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34d7df4-7caa-4e87-a146-09fae4078088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Undersample majority class\n",
    "strat_train_under, train_under_info = undersample_majority(churn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca51b17d-7a0b-4726-8188-c756d7289a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Upsample minority class\n",
    "strat_train_up, train_up_info = upsample_minority(churn_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea40cf1-2728-4955-a288-7b119bdf8832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build Pipeline for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1efbe14-c00a-4084-9edd-0afe94c20337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set ML Flow experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc4c5fa-394d-4046-a7af-308a36b9c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: would love to have a function that automatically sorts the columns by type - dynamic column selection/preprocessing\n",
    "#drop_for_features = {\"judi\",\"date\",\"churn3\"} \n",
    "#feature_cols = [c for c in df.columns if c not in drop_for_features and c not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6feb54e8-f2d3-4694-a60d-cf7044dc1bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_features = []\n",
    "other_features = ['unique_levels_played', 'market_idx','dayofweek','rounds_played', 'avg_attempts', 'total_attempts', 'avg_moves', 'win_rate', 'assist_success_rate', 'unassist_success_rate', 'assist_rate', 'total_boosters_used', 'total_boosters_spent', 'used_boosters_rate', 'spend_boosters_rate', 'avg_difficulty_score', 'rate_hard_levels', 'rate_superhard_levels', 'min_room_id_int', 'max_room_id_int', 'daily_win_rate_ref', 'daily_avg_boosters_used_ref', 'daily_avg_boosters_spent_ref', 'attribution_source_cd_idx', 'country_cd_idx', 'payer_type_cd_idx', 'iap_lifetime_amt', 'days_since_install', 'days_since_last_purchase', 'ad_revenue_amt', 'iap_revenue_amt', 'session_qty', 'total_session_length_qty', 'avg_session_length', 'sessions_per_round', 'avg_population_wr_on_levels_played_today', 'avg_population_assisted_rate_today', 'avg_population_attempts_today', 'wr_diff_vs_population', 'attempts_diff_vs_population', 'assist_rate_diff_vs_population', 'active_days_l7d', 'total_rounds_l7d', 'avg_rounds_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d', 'boosters_used_l7d', 'boosters_spent_l7d', 'avg_used_boosters_rate_l7d', 'active_days_l14d', 'avg_rounds_l14d', 'avg_win_rate_l14d', 'std_rounds_l14d', 'std_win_rate_l14d', 'active_days_l30d', 'avg_rounds_l30d', 'rounds_trend_weekly', 'win_rate_trend_weekly', 'boosters_usage_trend_weekly', 'rounds_ratio_7d_vs_14_7d', 'frequency_ratio_7d_vs_14d', 'levels_progressed_l7d', 'levels_progressed_l14d', 'levels_progressed_l30d', 'days_on_current_max_level', 'level_diversity_ratio',] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14131b91-a056-49b5-bee1-22e2fabd4525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92016ccf-d515-4c32-a1d0-11f33a24e36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if num_workers > available slots, fitting fails\n",
    "# determine number of workers and repartition the training data\n",
    "strat_train, safe_workers = get_safe_works_repartition(strat_train)\n",
    "strat_train_up, _ = get_safe_works_repartition(strat_train_up)\n",
    "strat_train_under, _ = get_safe_works_repartition(strat_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a33407f-a44c-4c0e-a114-d5c60ac7a992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(safe_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7d8a2b-fac3-4a72-b816-1344b5333a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c262725c-a991-4e20-9f05-a0fd10727267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For XGBoost we don't need to standarize any features\n",
    "indexers = [StringIndexer(inputCol=x, \n",
    "                          outputCol=x+\"_index\", \n",
    "                          handleInvalid=\"keep\") for x in string_features]\n",
    "indexed_cols = [ x+\"_index\" for x in string_features]\n",
    "\n",
    "inputs = other_features + indexed_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=inputs, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "\n",
    "# Now add the xgb model to the pipeline\n",
    "eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col = \"features\",\n",
    "  label_col = \"label\",\n",
    "  num_workers = safe_workers,\n",
    "  eval_metric = eval_metrics,\n",
    "  max_depth = 6, \n",
    "  subsample = 0.8, \n",
    "  colsample_bytree = 0.8,\n",
    ")\n",
    "\n",
    "# Set the pipeline stages for the entire process\n",
    "pipeline = Pipeline().setStages(indexers+[vec_assembler]+ [xgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d294e78b-58d7-4df7-96b1-1e42a3e5a0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can fit your pipeline model here with MLFlow tracking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da8e714-8c8b-4ed8-a609-7ef520d81904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from tracking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8646139d-1216-4b3f-93e5-4eeef1ed8053",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training on default xgb pipeline with upsampling\n",
    "\n",
    "\"\"\"\n",
    "### This is only fitting ithe pipeline, not CV\n",
    "run_info = run_spark_ml_training( \n",
    "    estimator = pipeline,\n",
    "    train_df = strat_train_up,\n",
    "    test_df = strat_test,\n",
    "    val_df = strat_val,     # prefer tuning on validation\n",
    "    run_name = \"spark-ml-search-xgb-up-fit-only\",\n",
    "    extra_tags = {'up_sampled':True},\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ad8669-0bc4-4438-9aaa-578fb47a8600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(xgb.n_estimators,[10, 20])\n",
    "             #.addGrid(xgb.max_depth, range(4,50))\n",
    "             .build())\n",
    "\n",
    "\n",
    "#TODO: Figure out how the evaluator is handled in the run_spark_cv_with_logging_spark_only()\n",
    "cv_xgb = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=xgb_paramGrid, \n",
    "                    numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "480cd665-76db-4500-bf61-959cb9e16850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if isinstance(cv_xgb, (CrossValidator, TrainValidationSplit)):\n",
    "    ### TODO: AND flag for submodels == True\n",
    "    print(\"Model is of type CrossValidatorModel\")\n",
    "    sub_models = cv_xgb.getCollectSubModels\n",
    "    #estimator = _best_model(cv_xgb)\n",
    "else:\n",
    "    print(\"Model is NOT of type CrossValidatorModel\")\n",
    "    sub_models = []\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b434f7-42f8-47d0-89d9-ed24ed0357a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# New - setting a default evaluator for binary classification CV\n",
    "def _set_evaluator(estimator, evaluator, label_col, prediction_col, probability_col, collect_submodels=False, metric=\"areaUnderPR\"):\n",
    "    if isinstance(estimator, (CrossValidator, TrainValidationSplit)):\n",
    "\n",
    "        evaluator.setLabelCol(label_col)\n",
    "        #evaluator.setPredictionCol(prediction_col)\n",
    "        evaluator.setRawPredictionCol(probability_col)\n",
    "        evaluator.setMetricName(metric)\n",
    "\n",
    "        # set Evaluator and collectSubModels flag\n",
    "        estimator.setEvaluator(evaluator).setCollectSubModels(collect_submodels)\n",
    "        print(\"setting estimator and evaluator\")\n",
    "\n",
    "        return estimator, evaluator\n",
    "    else:\n",
    "        #print(\"setting evaluator\")\n",
    "        #estimator.setEvaluator(evaluator)\n",
    "        return estimator, evaluator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ccf6838-93bc-4ceb-b3d7-9648c9a92670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "estimator, evaluator = _set_evaluator(cv_xgb, \n",
    "                                      BinaryClassificationEvaluator(), \n",
    "                                      label_col = \"label\", \n",
    "                                      probability_col = \"probability\",prediction_col=\"prediction\",collect_submodels=True, metric=\"areaUnderPR\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b285397f-dc98-43a1-b01f-1ad887d6febf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training on default xgb pipeline with upsampling\n",
    "run_info_cv_upsampled = run_spark_ml_training( \n",
    "    estimator = cv_xgb,\n",
    "    train_df = strat_train_up,\n",
    "    test_df = strat_test,\n",
    "    val_df = strat_val,     # prefer tuning on validation\n",
    "    run_name = \"spark-ml-search-xgb-up-fit-only\",\n",
    "    extra_tags = {'up_sampled':True,\n",
    "                  'under_sampled':False,\n",
    "                  'CV':True, \n",
    "                  'num_workers':safe_workers},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25ee833-852f-49f1-9a84-62249897ee3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e4b379-c307-4863-8f88-e1999d0707b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(run_info_cv_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7cb25a-e3d6-42dd-be53-7689205e5a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example using run_spark_cv_with_logging_spark_only() function to train a cross validation pipeline\n",
    "# This will take a long time to run\n",
    "\n",
    "\"\"\"\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(xgb.reg_alpha,[1e-5, 1e-2, 0.1])\n",
    "             .addGrid(xgb.reg_lambda,[1e-5, 1e-2, 0.1])\n",
    "             .addGrid(xgb.gamma, [i/10.0 for i in range(0,2)])\n",
    "             .addGrid(xgb.n_estimators,[10,500,20])\n",
    "             #.addGrid(xgb.learning_rate,[0.01,0.1])\n",
    "             .addGrid(xgb.max_depth, range(4,50))\n",
    "             #.addGrid(xgb.min_child_weight, [3.0, 4.0])\n",
    "             #.addGrid(xgb.colsample_bytree, [i/10.0 for i in range(3,6)])\n",
    "             #.addGrid(xgb.colsample_bylevel, [i/10.0 for i in range(3,6)])\n",
    "             .build())\n",
    "\n",
    "\n",
    "#TODO: Figure out how the evaluator is handled in the run_spark_cv_with_logging_spark_only()\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    numFolds=3)\n",
    "\"\"\"\n",
    "\n",
    "#run_info = run_spark_cv_with_logging_spark_only(\n",
    "#    estimator = cv,\n",
    "#    train_df = strat_train_under,\n",
    "#    test_df = strat_test,\n",
    "#    val_df = strat_val,     # prefer tuning on validation\n",
    "#    run_name = \"spark-ml-search-xgb-under-cv\",\n",
    "#    extra_tags = {'under_sampled':True,\"cv\":True}\n",
    "\n",
    "\n",
    "#best_model =  cv.bestModel"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) ForGuido_WithSUBMODELS_NB_Test_SamplingTracking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

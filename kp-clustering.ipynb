{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da53a51-000e-48f2-a756-08c48dbc4fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"teams.data_science.pp_churn_features_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e6dee1-317c-4c5a-8123-89a042b20521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_feature_names = ['unique_levels_played', 'market_idx','dayofweek','rounds_played', 'avg_attempts', 'total_attempts', 'avg_moves', 'win_rate', 'assist_success_rate', 'unassist_success_rate', 'assist_rate', 'total_boosters_used', 'total_boosters_spent', 'used_boosters_rate', 'spend_boosters_rate', 'avg_difficulty_score', 'rate_hard_levels', 'rate_superhard_levels', 'min_room_id_int', 'max_room_id_int', 'daily_win_rate_ref', 'daily_avg_boosters_used_ref', 'daily_avg_boosters_spent_ref', 'attribution_source_cd_idx', 'country_cd_idx', 'payer_type_cd_idx', 'iap_lifetime_amt', 'days_since_install', 'days_since_last_purchase', 'ad_revenue_amt', 'iap_revenue_amt', 'session_qty', 'total_session_length_qty', 'avg_session_length', 'sessions_per_round', 'avg_population_wr_on_levels_played_today', 'avg_population_assisted_rate_today', 'avg_population_attempts_today', 'wr_diff_vs_population', 'attempts_diff_vs_population', 'assist_rate_diff_vs_population', 'active_days_l7d', 'total_rounds_l7d', 'avg_rounds_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d', 'boosters_used_l7d', 'boosters_spent_l7d', 'avg_used_boosters_rate_l7d', 'active_days_l14d', 'avg_rounds_l14d', 'avg_win_rate_l14d', 'std_rounds_l14d', 'std_win_rate_l14d', 'active_days_l30d', 'avg_rounds_l30d', 'rounds_trend_weekly', 'win_rate_trend_weekly', 'boosters_usage_trend_weekly', 'rounds_ratio_7d_vs_14_7d', 'frequency_ratio_7d_vs_14d', 'levels_progressed_l7d', 'levels_progressed_l14d', 'levels_progressed_l30d', 'days_on_current_max_level', 'level_diversity_ratio',] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e14ddee-2951-4fb6-81d0-a43a89a85f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into payers and non-payers:\n",
    "#payers = df.filter(df.iap_lifetime_amt > 0)\n",
    "#non_payers = df.filter(df.iap_lifetime_amt == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30176da8-4eae-4ed0-90d9-b2f6787a7c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "historical = ['active_days_l30d', 'avg_rounds_l30d','levels_progressed_l30d','attribution_source_cd_idx', 'country_cd_idx', 'total_session_length_qty','assist_rate_diff_vs_population','attemps_diff_vs_population','wr_diff_vs_population','avg_session_length','max_room_id_int']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61c0b63-33d6-41d9-bbbd-7dc4ec040105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql describe teams.data_science.pp_churn_features_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea5d0665-44e7-46a9-ad27-eaa006615f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###select more historical, categorical columns of data\n",
    "['market_idx','dayofweek','rounds_played', 'avg_attempts', 'total_attempts', 'avg_moves', 'win_rate', 'assist_success_rate', 'unassist_success_rate', 'assist_rate', 'total_boosters_used', 'total_boosters_spent', 'used_boosters_rate', 'spend_boosters_rate', 'avg_difficulty_score', 'rate_hard_levels', 'rate_superhard_levels', 'min_room_id_int', 'max_room_id_int', 'daily_win_rate_ref', 'daily_avg_boosters_used_ref', 'daily_avg_boosters_spent_ref', 'attribution_source_cd_idx', 'country_cd_idx', 'payer_type_cd_idx', 'iap_lifetime_amt', 'days_since_install', 'days_since_last_purchase', 'ad_revenue_amt', 'iap_revenue_amt', 'session_qty', 'total_session_length_qty', 'avg_session_length', 'sessions_per_round', 'avg_population_wr_on_levels_played_today', 'avg_population_assisted_rate_today', 'avg_population_attempts_today', 'wr_diff_vs_population', 'attempts_diff_vs_population', 'assist_rate_diff_vs_population', 'active_days_l7d', 'total_rounds_l7d', 'avg_rounds_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d', 'boosters_used_l7d', 'boosters_spent_l7d', 'avg_used_boosters_rate_l7d', 'active_days_l14d', 'avg_rounds_l14d', 'avg_win_rate_l14d', 'std_rounds_l14d', 'std_win_rate_l14d', 'active_days_l30d', 'avg_rounds_l30d', 'rounds_trend_weekly', 'win_rate_trend_weekly', 'boosters_usage_trend_weekly', 'rounds_ratio_7d_vs_14_7d', 'frequency_ratio_7d_vs_14d', 'levels_progressed_l7d', 'levels_progressed_l14d', 'levels_progressed_l30d', 'days_on_current_max_level', 'level_diversity_ratio',] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e700aaf0-6051-4d71-91ee-d6c766ee5a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "assembler_corr = VectorAssembler(\n",
    "    inputCols=all_feature_names, \n",
    "    outputCol=\"features_corr\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "df_vector = assembler_corr.transform(df).select(\"features_corr\")\n",
    "\n",
    "\n",
    "matrix = Correlation.corr(df_vector, \"features_corr\", \"pearson\").head()\n",
    "\n",
    "corr_matrix = matrix[0].toArray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c24960e0-c62a-4d53-ae93-9ef7103d1eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 3. El umbral de correlaci√≥n para considerar la eliminaci√≥n\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "# --------------------\n",
    "\n",
    "print(f\"Analizando {len(all_feature_names)} features para alta correlaci√≥n (umbral={THRESHOLD})...\")\n",
    "\n",
    "# Set para guardar los nombres de las features a eliminar\n",
    "features_to_drop = set()\n",
    "\n",
    "# Obtener el n√∫mero de features\n",
    "n_features = len(all_feature_names)\n",
    "\n",
    "# Calcular la correlaci√≥n absoluta media para cada feature\n",
    "# Esta ser√° nuestra heur√≠stica para decidir cu√°l eliminar\n",
    "mean_abs_corr = np.mean(np.abs(corr_matrix), axis=1)\n",
    "\n",
    "# Iterar solo sobre el tri√°ngulo superior de la matriz\n",
    "for i in range(n_features):\n",
    "    for j in range(i + 1, n_features):\n",
    "        \n",
    "        feature_i = all_feature_names[i]\n",
    "        feature_j = all_feature_names[j]\n",
    "\n",
    "        # Si alguna de las features ya est√° marcada para eliminarse, saltar este par\n",
    "        if feature_i in features_to_drop or feature_j in features_to_drop:\n",
    "            continue\n",
    "            \n",
    "        correlation = corr_matrix[i, j]\n",
    "\n",
    "        # Comprobar si la correlaci√≥n absoluta supera el umbral\n",
    "        if abs(correlation) > THRESHOLD:\n",
    "            \n",
    "            # Tenemos un par altamente correlacionado.\n",
    "            # Decidimos cu√°l eliminar bas√°ndonos en la correlaci√≥n media.\n",
    "            # El que tenga la media m√°s alta es \"m√°s redundante\".\n",
    "            \n",
    "            if mean_abs_corr[i] > mean_abs_corr[j]:\n",
    "                features_to_drop.add(feature_i)\n",
    "                print(f\"üö® Par encontrado: ('{feature_i}', '{feature_j}') -> Corr: {correlation:.2f}. \")\n",
    "                print(f\"   -> Marcando '{feature_i}' para eliminar (m√°s redundante).\")\n",
    "            else:\n",
    "                features_to_drop.add(feature_j)\n",
    "                print(f\"üö® Par encontrado: ('{feature_i}', '{feature_j}') -> Corr: {correlation:.2f}. \")\n",
    "                print(f\"   -> Marcando '{feature_j}' para eliminar (m√°s redundante).\")\n",
    "\n",
    "print(\"\\n--- Resultado ---\")\n",
    "print(f\"Total de features a eliminar: {len(features_to_drop)}\")\n",
    "print(features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876027f4-066e-464f-81d7-0344b2a6ccb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener la lista final de features\n",
    "final_feature_names = [name for name in all_feature_names if name not in features_to_drop]\n",
    "\n",
    "print(f\"\\nFeatures originales: {len(all_feature_names)}\")\n",
    "print(f\"Features eliminadas: {len(features_to_drop)}\")\n",
    "print(f\"Features finales: {len(final_feature_names)}\")\n",
    "\n",
    "# Ahora, crea tu VectorAssembler para el modelo SOLO con 'final_feature_names'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f1b11b8-01c3-4449-8c88-ad1a3873a291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2819ed9-4d30-44eb-b8bc-22683855b4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 1: Data Preparation ---\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Select key features for clustering\n",
    "# We choose features representing Engagement, Skill, Monetization, and Progression.\n",
    "features_for_clustering = [\n",
    "    'avg_rounds_l7d',\n",
    "    'active_days_l7d',\n",
    "    'avg_win_rate_l7d',\n",
    "    'avg_attempts_l7d',\n",
    "    'boosters_used_l7d',\n",
    "    'iap_lifetime_amt',\n",
    "    'ad_revenue_amt',\n",
    "    'levels_progressed_l7d',\n",
    "    'days_on_current_max_level',\n",
    "    'avg_session_length',\n",
    "    'days_since_install',\n",
    "    'days_since_last_purchase',\n",
    "    'session_qty',\n",
    "    'active_days_l14d',\n",
    "    'active_days_l30d',\n",
    "    'market_idx',\n",
    "    'dayofweek',\n",
    "    'attribution_source_cd_idx',\n",
    "    'country_cd_idx'\n",
    "]\n",
    "\n",
    "# 2. Create imputation stage\n",
    "# K-Means cannot handle nulls. We'll use median to fill missing values.\n",
    "imputer = Imputer(\n",
    "    inputCols=features_for_clustering,\n",
    "    outputCols=[f\"{c}_imputed\" for c in features_for_clustering],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "\n",
    "# 3. Create vector assembler stage\n",
    "# This will use the *new* imputed columns\n",
    "imputed_cols = [f\"{c}_imputed\" for c in features_for_clustering]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=imputed_cols,\n",
    "    outputCol=\"unscaled_features\"\n",
    ")\n",
    "\n",
    "# 4. Create scaling stage\n",
    "# K-Means uses Euclidean distance, so scaling is critical.\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"unscaled_features\",\n",
    "    outputCol=\"features\", # This is the final column for the model\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# 5. Build the preparation pipeline\n",
    "preprocessing_pipeline = Pipeline(stages=[imputer, assembler, scaler])\n",
    "\n",
    "# 6. Fit and transform the data\n",
    "# 'df' is your source DataFrame\n",
    "try:\n",
    "    preprocessing_model = preprocessing_pipeline.fit(df)\n",
    "    scaled_data = preprocessing_model.transform(df)\n",
    "    \n",
    "    print(\"Data preparation successful.\")\n",
    "    print(\"DataFrame now contains a 'features' column (scaled) and an 'unscaled_features' column.\")\n",
    "    \n",
    "    # Show a sample for verification\n",
    "    scaled_data.select(\"features\", \"unscaled_features\").show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data preparation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add13ca2-de4b-4abf-9b67-3a99b3b96446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 2: Find Optimal K (Elbow Method) ---\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Define the range of K to test\n",
    "# We'll test from K=2 (minimum clusters) up to K=15\n",
    "k_values = range(7, 18)\n",
    "\n",
    "# 2. List to store the cost (WCSS) for each K\n",
    "wcss_values = []\n",
    "\n",
    "print(\"Starting K-Means training for Elbow Method...\")\n",
    "print(f\"Testing K values from {min(k_values)} to {max(k_values)}.\")\n",
    "\n",
    "# 3. Loop over each K, train model, and get cost\n",
    "# This will use the 'scaled_data' DataFrame from Step 1\n",
    "evaluator = ClusteringEvaluator(featuresCol='features')\n",
    "\n",
    "for k in k_values:\n",
    "    try:\n",
    "        kmeans = KMeans(featuresCol='features', k=k, seed=1)\n",
    "        model = kmeans.fit(scaled_data)\n",
    "        \n",
    "        # Calculate WCSS\n",
    "        # Evaluator computes silhouette, which is WCSS for squaredEuclidean\n",
    "        wcss = model.summary.trainingCost\n",
    "        wcss_values.append(wcss)\n",
    "        \n",
    "        print(f\"  K={k}, WCSS={wcss:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed for K={k}: {e}\")\n",
    "        wcss_values.append(None) # Append None to keep lists aligned\n",
    "\n",
    "print(\"Elbow method training complete.\")\n",
    "\n",
    "# 4. Prepare data for plotting\n",
    "# Convert results to a Pandas DataFrame for easy plotting\n",
    "elbow_data = pd.DataFrame({\n",
    "    'K': k_values,\n",
    "    'WCSS': wcss_values\n",
    "}).dropna() # Drop any K that failed\n",
    "\n",
    "# 5. Plot the Elbow curve\n",
    "print(\"Generating Elbow Plot...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(elbow_data['K'], elbow_data['WCSS'], 'bx-')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('WCSS (Training Cost)')\n",
    "plt.title('K-Means Elbow Method')\n",
    "plt.xticks(elbow_data['K'])\n",
    "plt.grid(True)\n",
    "display(plt.gcf()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975bfcc7-0356-4372-8137-82d823851569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 3: Train Final Model ---\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# 1. Define the optimal K based on our analysis\n",
    "OPTIMAL_K = 15\n",
    "\n",
    "print(f\"Training final K-Means model with K={OPTIMAL_K}...\")\n",
    "\n",
    "# 2. Initialize the model\n",
    "kmeans_final = KMeans(featuresCol='features', k=OPTIMAL_K, seed=1)\n",
    "\n",
    "# 3. Fit the model on the scaled data\n",
    "# This creates our final, trained clustering model\n",
    "final_model = kmeans_final.fit(scaled_data)\n",
    "\n",
    "# 4. Assign clusters to all players\n",
    "# This adds a new column 'prediction' (our cluster ID)\n",
    "data_with_clusters = final_model.transform(scaled_data)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "print(\"Added 'prediction' column with cluster IDs to the DataFrame.\")\n",
    "\n",
    "# 5. Show a sample of the results\n",
    "# We select the original features (unscaled) and the new prediction\n",
    "original_features_plus_cluster = [\n",
    "    'avg_rounds_l7d',\n",
    "    'active_days_l7d',\n",
    "    'avg_win_rate_l7d',\n",
    "    'boosters_used_l7d',\n",
    "    'iap_lifetime_amt',\n",
    "    'prediction'\n",
    "]\n",
    "\n",
    "data_with_clusters.select(original_features_plus_cluster).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d50e2f9-a8e9-41d9-b55f-2deac04b790b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761701525550}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 4: Profile and Analyze Clusters ---\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Get the list of imputed column names from Step 1\n",
    "# (This assumes 'imputer' is still in memory from Step 1)\n",
    "try:\n",
    "    imputed_cols = imputer.getOutputCols()\n",
    "    original_cols = features_for_clustering # List from Step 1\n",
    "except NameError:\n",
    "    # Fallback in case the notebook state was lost\n",
    "    print(\"Re-defining feature lists...\")\n",
    "    original_cols = [\n",
    "        'avg_rounds_l7d', 'active_days_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d',\n",
    "        'boosters_used_l7d', 'iap_lifetime_amt', 'ad_revenue_amt', 'levels_progressed_l7d',\n",
    "        'days_on_current_max_level', 'avg_session_length', 'days_since_install',\n",
    "        'days_since_last_purchase', 'session_qty', 'active_days_l14d', 'active_days_l30d'\n",
    "    ]\n",
    "    imputed_cols = [f\"{c}_imputed\" for c in original_cols]\n",
    "\n",
    "\n",
    "# 2. Create a list of aggregation expressions\n",
    "# We calculate the mean for each imputed feature and alias it back to the original name\n",
    "agg_expressions = []\n",
    "for original, imputed in zip(original_cols, imputed_cols):\n",
    "    agg_expressions.append(\n",
    "        F.mean(imputed).alias(original)\n",
    "    )\n",
    "\n",
    "# 3. Add an expression to count the size of each cluster\n",
    "agg_expressions.append(\n",
    "    F.count(\"*\").alias(\"cluster_size\")\n",
    ")\n",
    "\n",
    "print(\"Calculating cluster profiles...\")\n",
    "\n",
    "# 4. Group by cluster, apply aggregations, and order\n",
    "# This gives us the full profile for all 10 clusters\n",
    "cluster_profiles_spark = data_with_clusters \\\n",
    "    .groupBy(\"prediction\") \\\n",
    "    .agg(*agg_expressions) \\\n",
    "    .orderBy(\"prediction\")\n",
    "\n",
    "# 5. Convert to Pandas for easier analysis and viewing\n",
    "cluster_profiles_pandas = cluster_profiles_spark.toPandas()\n",
    "\n",
    "print(\"Cluster profiling complete.\")\n",
    "\n",
    "# 6. Display the profiles\n",
    "# This table is the \"answer\" - it tells us what each cluster means.\n",
    "display(cluster_profiles_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a82a90a-0449-4e9c-8d72-a704cc4ec9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 5: Visualize Cluster Profiles ---\n",
    "# (Assuming 'cluster_profiles_pandas' is our Pandas DataFrame from Step 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd # Ensure pandas is imported if not already\n",
    "\n",
    "# --- Visualization 1: Cluster Sizes (Bar Chart) ---\n",
    "\n",
    "print(\"Generating Cluster Size Bar Chart...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=cluster_profiles_pandas, \n",
    "    x='prediction', \n",
    "    y='cluster_size',\n",
    "    palette='viridis' # A nice color palette\n",
    ")\n",
    "plt.title('Distribution of Players per Cluster (Cluster Size)')\n",
    "plt.xlabel('Cluster ID (prediction)')\n",
    "plt.ylabel('Number of Players')\n",
    "plt.xticks(rotation=0)\n",
    "display(plt.gcf()) # Use display() in Databricks\n",
    "\n",
    "# --- Visualization 2: Feature Profiles (Heatmap) ---\n",
    "\n",
    "print(\"Generating Normalized Feature Profile Heatmap...\")\n",
    "\n",
    "# 1. Prepare data: Set index and drop non-feature column\n",
    "# We need to scale only the 15 features, not 'cluster_size'\n",
    "profiles_to_scale = cluster_profiles_pandas.set_index('prediction')\n",
    "features_only = profiles_to_scale.drop(columns=['cluster_size'])\n",
    "\n",
    "# 2. Scale the data (column-wise)\n",
    "# Each feature (column) will be scaled from 0 (min) to 1 (max)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features_only)\n",
    "\n",
    "# 3. Recreate DataFrame with scaled values\n",
    "scaled_profiles_df = pd.DataFrame(\n",
    "    scaled_features,\n",
    "    index=features_only.index,\n",
    "    columns=features_only.columns\n",
    ")\n",
    "\n",
    "# 4. Plot the heatmap\n",
    "plt.figure(figsize=(16, 10)) # Make it large for readability\n",
    "sns.heatmap(\n",
    "    scaled_profiles_df, \n",
    "    annot=True,     # Show the scaled values (0.0 to 1.0)\n",
    "    fmt=\".2f\",      # Format to 2 decimal places\n",
    "    cmap='coolwarm', # Blue (low) to Red (high) is very intuitive\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Normalized Feature Averages by Cluster')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Cluster ID (prediction)')\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22bdf8d7-e8b0-409a-a6aa-5533bb671e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select churn3, count(1) from teams.data_science.pp_churn_features_v3_clusters where cluster_6=1 group by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3707e5d6-fca5-48d0-85f3-bce4f6eee4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select churn3, count(1) from teams.data_science.pp_churn_features_v3_clusters where cluster_8=1 group by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9ba74b-fbb3-4323-8d39-0a0d5ce5438d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761701830263}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 6: Cross-reference Clusters with Churn3 ---\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Calculate churn rate per cluster\n",
    "print(\"Calculating churn rate per cluster...\")\n",
    "\n",
    "# We use the 'data_with_clusters' DataFrame from Step 3\n",
    "# It should contain the original 'churn3' column\n",
    "churn_by_cluster_spark = data_with_clusters \\\n",
    "    .groupBy(\"prediction\") \\\n",
    "    .agg(\n",
    "        F.avg(\"churn3\").alias(\"churn_rate\"),\n",
    "        F.count(\"*\").alias(\"cluster_size\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"churn_rate\"))\n",
    "\n",
    "# 2. Convert to Pandas for analysis and plotting\n",
    "churn_by_cluster_pandas = churn_by_cluster_spark.toPandas()\n",
    "\n",
    "# 3. Display the churn rate table\n",
    "print(\"Churn Rate by Cluster (Highest to Lowest):\")\n",
    "display(churn_by_cluster_pandas)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nGenerando Gr√°fico de Tasa de Churn (Paleta Corregida)...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=churn_by_cluster_pandas,\n",
    "    x='prediction',\n",
    "    y='churn_rate',\n",
    "    palette='coolwarm' # \n",
    ")\n",
    "plt.title('Tasa de Churn3 por Cluster de Jugador')\n",
    "plt.xlabel('Cluster ID (prediction)')\n",
    "plt.ylabel('Tasa de Churn3 (Avg)')\n",
    "plt.axhline(y=0.03, color='black', linestyle='--', label='Churn Promedio (3%)')\n",
    "plt.legend()\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b54e2d-04b7-4fd5-81c7-5245eeca68d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Show a Sample of Cluster 6 Players ---\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Define the features we want to inspect\n",
    "# These are the 15 features from our clustering\n",
    "features_to_show = [\n",
    "    'avg_rounds_l7d', 'active_days_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d',\n",
    "    'boosters_used_l7d', 'iap_lifetime_amt', 'ad_revenue_amt', 'levels_progressed_l7d',\n",
    "    'days_on_current_max_level', 'avg_session_length', 'days_since_install',\n",
    "    'days_since_last_purchase', 'session_qty', 'active_days_l14d', 'active_days_l30d',\n",
    "    'churn3', # Also show the target variable\n",
    "    'prediction' # And the cluster ID\n",
    "]\n",
    "\n",
    "print(\"Displaying a sample of players from Cluster 6 (High Churn Group)...\")\n",
    "\n",
    "# 2. Filter the DataFrame for Cluster 6 and select our columns\n",
    "cluster_6_sample = data_with_clusters \\\n",
    "    .filter(F.col(\"prediction\") == 6) \\\n",
    "    .select(features_to_show)\n",
    "\n",
    "# 3. Show the sample\n",
    "# truncate=False ensures we see the full values in the columns\n",
    "cluster_6_sample.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c5b77c-8ead-4129-bdb1-083edcde0773",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761702443538}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cluster_6_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf304884-30c4-4035-932c-732193a13da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Build Model Pipeline Using Cluster as a Feature ---\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "# Aseg√∫rate de importar tu clasificador de XGBoost\n",
    "# from pyspark.ml.classification import XGBoostClassifier \n",
    "\n",
    "# 1. List of your original 56 feature columns\n",
    "# (Aseg√∫rate de que esta lista est√© definida)\n",
    "original_56_features = all_feature_names\n",
    "\n",
    "# 2. Stage 1: StringIndexer for the cluster ID\n",
    "# Converts the 'prediction' column (e.g., 6.0) into a categorical index\n",
    "cluster_indexer = StringIndexer(\n",
    "    inputCol=\"prediction\",\n",
    "    outputCol=\"prediction_idx\",\n",
    "    handleInvalid=\"keep\" # Keep unseen labels\n",
    ")\n",
    "\n",
    "# 3. Stage 2: OneHotEncoder for the indexed cluster ID\n",
    "# Creates a sparse vector (e.g., [0,0,0,0,0,0,1,0,0,0])\n",
    "cluster_ohe = OneHotEncoder(\n",
    "    inputCols=[\"prediction_idx\"],\n",
    "    outputCols=[\"cluster_ohe_vec\"]\n",
    ")\n",
    "\n",
    "# 4. Stage 3: Final VectorAssembler\n",
    "# This combines your ORIGINAL features with your NEW cluster features\n",
    "final_feature_list = original_56_features + [\"cluster_ohe_vec\"]\n",
    "\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=final_feature_list,\n",
    "    outputCol=\"features\", # This is the final vector for XGBoost\n",
    "    handleInvalid=\"skip\" # O usa tu 'Imputer' si lo tienes en un pipeline\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9104cb78-5f81-4156-931c-2b959069c9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cluster_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42255c0c-4ace-4054-a826-37322d0d86b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Materialize Table with OHE Clusters ---\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# 1. Define all original columns to keep\n",
    "# We get all columns from your DataFrame EXCEPT the ones we created for clustering\n",
    "# (Esto asume que no has borrado ninguna columna todav√≠a)\n",
    "original_cols_to_keep = [\n",
    "    c for c in data_with_clusters.columns \n",
    "    if c not in ('unscaled_features', 'features', 'prediction')\n",
    "]\n",
    "\n",
    "# 2. Define the list of cluster IDs (0 to 9)\n",
    "# Esto es para decirle al pivot qu√© columnas crear\n",
    "num_clusters = 10\n",
    "cluster_ids = list(range(num_clusters)) # [0, 1, 2, ..., 9]\n",
    "\n",
    "print(f\"Pivoting 'prediction' column into {num_clusters} new OHE columns...\")\n",
    "\n",
    "# 3. Group by original columns, pivot on 'prediction', and fill\n",
    "materialized_df = data_with_clusters \\\n",
    "    .groupBy(original_cols_to_keep) \\\n",
    "    .pivot(\"prediction\", cluster_ids) \\\n",
    "    .agg(lit(1)) \\\n",
    "    .fillna(0)\n",
    "\n",
    "# 4. Rename new columns for clarity\n",
    "# El pivot crea columnas con nombres \"0\", \"1\", \"2\"...\n",
    "# Las renombramos a \"cluster_0\", \"cluster_1\", \"cluster_2\"...\n",
    "for i in cluster_ids:\n",
    "    materialized_df = materialized_df.withColumnRenamed(\n",
    "        str(i), f\"cluster_{i}\"\n",
    "    )\n",
    "\n",
    "print(\"DataFrame materializado con √©xito.\")\n",
    "materialized_df.select(original_cols_to_keep[-5:] + [f\"cluster_{i}\" for i in cluster_ids]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad772c14-d6f5-4ca0-97ea-660a191df887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(materialized_df.limit(19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e51e66a-4a13-4603-83c9-5cde9b8ac47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "materialized_df.createOrReplaceTempView(\"vo_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c790747-0384-40f8-9b76-2b08dd36e98e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table teams.data_science.pp_churn_features_v3_clusters\n",
    "using delta\n",
    "as select * from vo_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac91230-758d-4c9d-b3b5-cc42d8f85162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(1) from teams.data_science.pp_churn_features_v3_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb098d6f-e977-432e-bd95-928a88985eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec9bf0a-8c83-4294-92dd-f319717ad7e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 5: Visualize Cluster Profiles ---\n",
    "# (Assuming 'cluster_profiles_pandas' is our Pandas DataFrame from Step 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd # Ensure pandas is imported if not already\n",
    "\n",
    "# --- Visualization 1: Cluster Sizes (Bar Chart) ---\n",
    "\n",
    "print(\"Generating Cluster Size Bar Chart...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=cluster_profiles_pandas, \n",
    "    x='prediction', \n",
    "    y='cluster_size',\n",
    "    palette='viridis' # A nice color palette\n",
    ")\n",
    "plt.title('Distribution of Players per Cluster (Cluster Size)')\n",
    "plt.xlabel('Cluster ID (prediction)')\n",
    "plt.ylabel('Number of Players')\n",
    "plt.xticks(rotation=0)\n",
    "display(plt.gcf()) # Use display() in Databricks\n",
    "\n",
    "# --- Visualization 2: Feature Profiles (Heatmap) ---\n",
    "\n",
    "print(\"Generating Normalized Feature Profile Heatmap...\")\n",
    "\n",
    "# 1. Prepare data: Set index and drop non-feature column\n",
    "# We need to scale only the 15 features, not 'cluster_size'\n",
    "profiles_to_scale = cluster_profiles_pandas.set_index('prediction')\n",
    "features_only = profiles_to_scale.drop(columns=['cluster_size'])\n",
    "\n",
    "# 2. Scale the data (column-wise)\n",
    "# Each feature (column) will be scaled from 0 (min) to 1 (max)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features_only)\n",
    "\n",
    "# 3. Recreate DataFrame with scaled values\n",
    "scaled_profiles_df = pd.DataFrame(\n",
    "    scaled_features,\n",
    "    index=features_only.index,\n",
    "    columns=features_only.columns\n",
    ")\n",
    "\n",
    "# 4. Plot the heatmap\n",
    "plt.figure(figsize=(16, 10)) # Make it large for readability\n",
    "sns.heatmap(\n",
    "    scaled_profiles_df, \n",
    "    annot=True,     # Show the scaled values (0.0 to 1.0)\n",
    "    fmt=\".2f\",      # Format to 2 decimal places\n",
    "    cmap='coolwarm', # Blue (low) to Red (high) is very intuitive\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Normalized Feature Averages by Cluster')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Cluster ID (prediction)')\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b55206-9808-46e0-bf44-b632b8929d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 6: Cross-reference Clusters with Churn3 ---\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Calculate churn rate per cluster\n",
    "print(\"Calculating churn rate per cluster...\")\n",
    "\n",
    "# We use the 'data_with_clusters' DataFrame from Step 3\n",
    "# It should contain the original 'churn3' column\n",
    "churn_by_cluster_spark = data_with_clusters \\\n",
    "    .groupBy(\"prediction\") \\\n",
    "    .agg(\n",
    "        F.avg(\"churn3\").alias(\"churn_rate\"),\n",
    "        F.count(\"*\").alias(\"cluster_size\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"churn_rate\"))\n",
    "\n",
    "# 2. Convert to Pandas for analysis and plotting\n",
    "churn_by_cluster_pandas = churn_by_cluster_spark.toPandas()\n",
    "\n",
    "# 3. Display the churn rate table\n",
    "print(\"Churn Rate by Cluster (Highest to Lowest):\")\n",
    "display(churn_by_cluster_pandas)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nGenerando Gr√°fico de Tasa de Churn (Paleta Corregida)...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=churn_by_cluster_pandas,\n",
    "    x='prediction',\n",
    "    y='churn_rate',\n",
    "    palette='coolwarm' # \n",
    ")\n",
    "plt.title('Tasa de Churn3 por Cluster de Jugador')\n",
    "plt.xlabel('Cluster ID (prediction)')\n",
    "plt.ylabel('Tasa de Churn3 (Avg)')\n",
    "plt.axhline(y=0.03, color='black', linestyle='--', label='Churn Promedio (3%)')\n",
    "plt.legend()\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e87d903-dd60-4fd0-858c-85ee57c6b720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Materialize Table with OHE Clusters ---\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# 1. Define all original columns to keep\n",
    "# We get all columns from your DataFrame EXCEPT the ones we created for clustering\n",
    "# (Esto asume que no has borrado ninguna columna todav√≠a)\n",
    "original_cols_to_keep = [\n",
    "    c for c in data_with_clusters.columns \n",
    "    if c not in ('unscaled_features', 'features', 'prediction')\n",
    "]\n",
    "\n",
    "# 2. Define the list of cluster IDs (0 to 9)\n",
    "# Esto es para decirle al pivot qu√© columnas crear\n",
    "num_clusters = 15\n",
    "cluster_ids = list(range(num_clusters)) # [0, 1, 2, ..., 9]\n",
    "\n",
    "print(f\"Pivoting 'prediction' column into {num_clusters} new OHE columns...\")\n",
    "\n",
    "# 3. Group by original columns, pivot on 'prediction', and fill\n",
    "materialized_df = data_with_clusters \\\n",
    "    .groupBy(original_cols_to_keep) \\\n",
    "    .pivot(\"prediction\", cluster_ids) \\\n",
    "    .agg(lit(1)) \\\n",
    "    .fillna(0)\n",
    "\n",
    "# 4. Rename new columns for clarity\n",
    "# El pivot crea columnas con nombres \"0\", \"1\", \"2\"...\n",
    "# Las renombramos a \"cluster_0\", \"cluster_1\", \"cluster_2\"...\n",
    "for i in cluster_ids:\n",
    "    materialized_df = materialized_df.withColumnRenamed(\n",
    "        str(i), f\"cluster_{i}\"\n",
    "    )\n",
    "\n",
    "print(\"DataFrame materializado con √©xito.\")\n",
    "materialized_df.select(original_cols_to_keep[-5:] + [f\"cluster_{i}\" for i in cluster_ids]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e922ab-4405-4df0-acf9-d834b6a015fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "materialized_df.createOrReplaceTempView(\"vo_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10aca4f7-433a-4300-b0f5-6da52c6d3c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from teams.data_science.pp_churn_features_v3_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9003b654-3e5e-4072-90d8-d0c1ef0b0e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "create table teams.data_science.pp_churn_features_v3_clusters\n",
    "using delta\n",
    "as select * from vo_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d36055b-60dd-4088-b0e5-4c3cf06ebb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from teams.data_science.pp_churn_features_v3_clusters_15"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6833368668333247,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "kp-clustering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdca6ef-7d36-4766-8233-ce038931c504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install mlflow xgboost\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# # Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# # To disable autoreload; run %autoreload 0\n",
    "\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c9c289-852d-4992-ab5b-ce691f8eb6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, functions as F, types as T, Window\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Union, List, Tuple, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier, SparkXGBRegressor\n",
    "import mlflow\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.tuning import CrossValidatorModel, TrainValidationSplitModel\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8bdcf9-aa2f-4543-a603-6f171c63ec5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "from sampling import *\n",
    "from tracking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22db0c6-79bf-431e-9036-75491b97620f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_COL = \"churn3\"\n",
    "LABEL_COL7 = \"churn7\"\n",
    "LABEL_COL14 = \"churn14\"\n",
    "\n",
    "\n",
    "DATE_FILTER = \"2025-10-17\"\n",
    "DATE_INTERVAL = 90\n",
    "\n",
    "FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features_v2\"\n",
    "\n",
    "# These are loaded in config already\n",
    "#EXPERIMENT_NAME = \"/Users/krista@jamcity.com/PP-Churn-Model\"\n",
    "#FEATURES_TABLE_NAME = \"teams.data_science.pp_churn_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5863fec2-0e36-4266-bec2-4483014a5f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select * from {FEATURES_TABLE_NAME}\")\n",
    "\n",
    "dates = df.agg(F.min(\"date\").alias(\"min_date\"), F.max(\"date\").alias(\"max_date\")).first()\n",
    "past_need, past_need_max = 30, 3\n",
    "valid_start = F.date_add(F.lit(dates[\"min_date\"]), past_need)\n",
    "valid_end = F.date_sub(F.lit(dates[\"max_date\"]), past_need_max)\n",
    "\n",
    "df = df.filter((F.col(\"date\") >= valid_start) & (F.col(\"date\") < valid_end))\\\n",
    "    .withColumn(LABEL_COL, when(col(LABEL_COL) == False, 0).otherwise(1))\\\n",
    "    .withColumn(\"label\",col(LABEL_COL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f73231-9da4-4a96-b3e4-020ac5abd9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Muestreo estratificado manteniendo proporción de churn3\n",
    "# sample_size = 10000\n",
    "# total_count = df.count()\n",
    "# sample_fraction = sample_size / total_count\n",
    "\n",
    "# df_sample = df.sampleBy(\"churn3\", fractions={0: sample_fraction, 1: sample_fraction}, seed=42)\n",
    "\n",
    "# # Verifica las proporciones\n",
    "# df_sample.groupBy(\"churn3\").count().show()\n",
    "\n",
    "# df = df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce98161-9d7c-4854-9210-eb56eec813a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecf9857-8566-4a80-8662-b99ced78bdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get stratified train, validation, test set\n",
    "strat_train, strat_val, strat_test = stratified_sampling(df, P_TEST=0.2, P_VAL=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34d7df4-7caa-4e87-a146-09fae4078088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Undersample majority class\n",
    "strat_train_under, train_under_info = undersample_majority(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca51b17d-7a0b-4726-8188-c756d7289a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Upsample minority class\n",
    "strat_train_up, train_up_info = upsample_minority(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea40cf1-2728-4955-a288-7b119bdf8832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build Pipeline for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1efbe14-c00a-4084-9edd-0afe94c20337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PA 2025-10-24: temporary, remove later. -->\n",
    "EXPERIMENT_NAME = \"/Users/gpereyra@jamcity.com/pp-churn/churn-models/MLFlow-tracking\"\n",
    "# PA 2025-10-24: temporary, remove later. <--\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc4c5fa-394d-4046-a7af-308a36b9c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: would love to have a function that automatically sorts the columns by type\n",
    "#drop_for_features = {\"judi\",\"date\",\"churn3\"} \n",
    "#feature_cols = [c for c in df.columns if c not in drop_for_features and c not in drop_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6feb54e8-f2d3-4694-a60d-cf7044dc1bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_features = [] # add back market_idx or whatever indexed market column\n",
    "other_features = ['unique_levels_played', 'rounds_played', 'avg_attempts', 'total_attempts', 'avg_moves', 'win_rate', 'assist_success_rate', 'unassist_success_rate', 'assist_rate', 'total_boosters_used', 'total_boosters_spent', 'used_boosters_rate', 'spend_boosters_rate', 'avg_difficulty_score', 'rate_hard_levels', 'rate_superhard_levels', 'min_room_id_int', 'max_room_id_int', 'daily_win_rate_ref', 'daily_avg_boosters_used_ref', 'daily_avg_boosters_spent_ref', 'attribution_source_cd_idx', 'country_cd_idx', 'payer_type_cd_idx', 'iap_lifetime_amt', 'days_since_install', 'days_since_last_purchase', 'ad_revenue_amt', 'iap_revenue_amt', 'session_qty', 'total_session_length_qty', 'avg_session_length', 'sessions_per_round', 'avg_population_wr_on_levels_played_today', 'avg_population_assisted_rate_today', 'avg_population_attempts_today', 'wr_diff_vs_population', 'attempts_diff_vs_population', 'assist_rate_diff_vs_population', 'active_days_l7d', 'total_rounds_l7d', 'avg_rounds_l7d', 'avg_win_rate_l7d', 'avg_attempts_l7d', 'boosters_used_l7d', 'boosters_spent_l7d', 'avg_used_boosters_rate_l7d', 'active_days_l14d', 'avg_rounds_l14d', 'avg_win_rate_l14d', 'std_rounds_l14d', 'std_win_rate_l14d', 'active_days_l30d', 'avg_rounds_l30d', 'rounds_trend_weekly', 'win_rate_trend_weekly', 'boosters_usage_trend_weekly', 'rounds_ratio_7d_vs_14_7d', 'frequency_ratio_7d_vs_14d', 'levels_progressed_l7d', 'levels_progressed_l14d', 'levels_progressed_l30d', 'days_on_current_max_level', 'level_diversity_ratio',] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14131b91-a056-49b5-bee1-22e2fabd4525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_safe_works_repartition(df):\n",
    "\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    cores_per_exec = int(conf.get(\"spark.executor.cores\", \"1\"))\n",
    "    # executors = all JVMs except the driver\n",
    "    num_exec = spark._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    slots = __builtins__.max(1, cores_per_exec * __builtins__.max(1, num_exec))\n",
    "\n",
    "    safe_workers = __builtins__.max(1, __builtins__.min(slots, 32))  # cap if you like\n",
    "    df = df.repartition(safe_workers)  # match partitions to workers\n",
    "\n",
    "    return df, safe_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92016ccf-d515-4c32-a1d0-11f33a24e36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if num_workers > available slots, fitting fails\n",
    "# determine number of workers and repartition the training data\n",
    "strat_train, safe_workers = get_safe_works_repartition(strat_train)\n",
    "strat_train_up, _ = get_safe_works_repartition(strat_train_up)\n",
    "strat_train_under, _ = get_safe_works_repartition(strat_train_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7d8a2b-fac3-4a72-b816-1344b5333a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c262725c-a991-4e20-9f05-a0fd10727267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Logistic Regression Pipeline\n",
    "\n",
    "#Prepare Data\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols=other_features, outputCols=other_features).setStrategy(\"mean\")\n",
    "assembler = VectorAssembler(inputCols=other_features, outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "\n",
    "# Add classifier\n",
    "eval_metrics = [\"auc\", \"aucpr\", \"logloss\"]\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[imputer, assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0864eda1-72f9-4ef0-a5d0-5990fa8eb7a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2430f3a7-e9d3-425c-b38c-e029ee985083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Grid de búsqueda ---\n",
    "# lr_grid = (ParamGridBuilder()\n",
    "#            .addGrid(lr.regParam, [1e-4, 1e-3, 1e-2, 1e-1])\n",
    "#            .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "#            .addGrid(lr.maxIter, [25, 50, 100 , 150])\n",
    "#            .build())\n",
    "\n",
    "\n",
    "# lr_grid = (ParamGridBuilder()\n",
    "#     .addGrid(lr.regParam, [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2])  # 7 valores\n",
    "#     .addGrid(lr.elasticNetParam, [0.5, 0.7, 0.8, 0.9, 1.0])  # 5 valores\n",
    "#     .addGrid(lr.maxIter, [100, 150])  # 2 valores\n",
    "#     .addGrid(lr.threshold, [0.3, 0.35, 0.4, 0.45, 0.5])  # 5 valores\n",
    "#     .build())\n",
    "\n",
    "lr_grid = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [1e-5, 1e-3, 1e-2])\n",
    "    .addGrid(lr.elasticNetParam, [0.5, 1.0])\n",
    "    .addGrid(lr.maxIter, [100])\n",
    "    .addGrid(lr.threshold, [0.3, 0.35, 0.4, 0.45, 0.5])\n",
    "    .build())\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(metricName = \"areaUnderPR\")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr_pipeline,\n",
    "    estimatorParamMaps=lr_grid,\n",
    "    evaluator=evaluator_pr,\n",
    "    numFolds=2,\n",
    "    parallelism=2,\n",
    "    seed=42,\n",
    "    collectSubModels=True\n",
    ")\n",
    "\n",
    "\n",
    "# lr_cv = CrossValidator(\n",
    "#     estimator=lr_pipeline,\n",
    "#     estimatorParamMaps=lr_grid,\n",
    "#     evaluator= evaluator_pr,\n",
    "#     numFolds=3,\n",
    "#     parallelism=8,\n",
    "#     seed=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8646139d-1216-4b3f-93e5-4eeef1ed8053",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training on default xgb pipeline with upsampling\n",
    "run_info = run_spark_cv_with_logging_spark_only( \n",
    "    estimator = lr_cv,\n",
    "    train_df = strat_train_up,\n",
    "    test_df = strat_test,\n",
    "    val_df = strat_val,     # prefer tuning on validation\n",
    "    run_name = \"spark-ml-search-lr-cv-up\",\n",
    "    extra_tags = {'up_sampled':True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d294e78b-58d7-4df7-96b1-1e42a3e5a0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can fit your pipeline model here with MLFlow tracking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202a2816-5ac7-4a73-b60d-5d2f35f07e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25ee833-852f-49f1-9a84-62249897ee3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7cb25a-e3d6-42dd-be53-7689205e5a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example using run_spark_cv_with_logging_spark_only() function to train a cross validation pipeline\n",
    "# This will take a long time to run\n",
    "\n",
    "\"\"\"\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(xgb.reg_alpha,[1e-5, 1e-2, 0.1])\n",
    "             .addGrid(xgb.reg_lambda,[1e-5, 1e-2, 0.1])\n",
    "             .addGrid(xgb.gamma, [i/10.0 for i in range(0,2)])\n",
    "             .addGrid(xgb.n_estimators,[10,500,20])\n",
    "             #.addGrid(xgb.learning_rate,[0.01,0.1])\n",
    "             .addGrid(xgb.max_depth, range(4,50))\n",
    "             #.addGrid(xgb.min_child_weight, [3.0, 4.0])\n",
    "             #.addGrid(xgb.colsample_bytree, [i/10.0 for i in range(3,6)])\n",
    "             #.addGrid(xgb.colsample_bylevel, [i/10.0 for i in range(3,6)])\n",
    "             .build())\n",
    "\n",
    "\n",
    "#TODO: Figure out how the evaluator is handled in the run_spark_cv_with_logging_spark_only()\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    numFolds=3)\n",
    "\"\"\"\n",
    "\n",
    "#run_info = run_spark_cv_with_logging_spark_only(\n",
    "#    estimator = cv,\n",
    "#    train_df = strat_train_under,\n",
    "#    test_df = strat_test,\n",
    "#    val_df = strat_val,     # prefer tuning on validation\n",
    "#    run_name = \"spark-ml-search-xgb-under-cv\",\n",
    "#    extra_tags = {'under_sampled':True,\"cv\":True}\n",
    "\n",
    "\n",
    "#best_model =  cv.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc1339a-6b7b-40fa-906f-2693646b070e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Configurar MLflow para usar tu experimento existente\n",
    "# ----------------------------------------------------------\n",
    "mlflow.set_experiment(experiment_id=\"493970548959259\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"logreg-cv-run\") as run:\n",
    "    lr_cv = CrossValidator(\n",
    "        estimator=lr_pipeline,\n",
    "        estimatorParamMaps=lr_grid,\n",
    "        evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderPR\"),\n",
    "        numFolds=2,\n",
    "        parallelism=2\n",
    "    )\n",
    "\n",
    "    cv_model = lr_cv.fit(strat_train_up)\n",
    "    best_model = cv_model.bestModel\n",
    "\n",
    "    scored = best_model.transform(strat_val)\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\")\n",
    "    auc_roc = evaluator.evaluate(scored, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    auc_pr = evaluator.evaluate(scored, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    mlflow.log_metric(\"auc_roc\", auc_roc)\n",
    "    mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "    mlflow.spark.log_model(best_model, \"spark_model\")\n",
    "\n",
    "    print(f\"Run loggeado en MLflow con ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426ed86a-f577-4bc5-92fb-596b5d6ddc74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Configurar MLflow para usar tu experimento existente\n",
    "# ----------------------------------------------------------\n",
    "mlflow.set_experiment(experiment_id=\"493970548959259\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"logreg-cv-run\") as run:\n",
    "    lr_cv = CrossValidator(\n",
    "        estimator=lr_pipeline,\n",
    "        estimatorParamMaps=lr_grid,\n",
    "        evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderPR\"),\n",
    "        numFolds=2,\n",
    "        parallelism=2,\n",
    "        collectSubModels=True\n",
    "    )\n",
    "\n",
    "    cv_model = lr_cv.fit(strat_train_up)\n",
    "    best_model = cv_model.bestModel\n",
    "\n",
    "    scored = best_model.transform(strat_val)\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\")\n",
    "    auc_roc = evaluator.evaluate(scored, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    auc_pr = evaluator.evaluate(scored, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    mlflow.log_metric(\"auc_roc\", auc_roc)\n",
    "    mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "    mlflow.spark.log_model(best_model, \"spark_model\")\n",
    "\n",
    "    print(f\"Run loggeado en MLflow con ID: {run.info.run_id}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "new_LogReg",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
